{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcaf5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ba05b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████████▏                                                                  | 81/249 [00:00<00:01, 134.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skeeping file 1439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|█████████████████████████████████████████████████████████████████████████████████████             | 216/249 [00:01<00:00, 103.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skeeping file 3979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 249/249 [00:02<00:00, 114.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skeeping file 4659\n",
      "skeeping file 4699\n",
      "(120311, 8)\n",
      "(120311,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#reading all the files\n",
    "\n",
    "#reading the first file\n",
    "df = pd.read_csv('../src/outputs/run_history_19.csv', sep=';', index_col=0)\n",
    "df_emb = pd.read_csv('../src/outputs/run_history_emb_idx_19.csv', sep=';', index_col=0)\n",
    "np_emb_state = np.load('../src/outputs/emb_states_19.npy', allow_pickle=True)\n",
    "np_emb_next_state = np.load('../src/outputs/next_states_19.npy', allow_pickle=True)\n",
    "\n",
    "skeeped_files = []\n",
    "#adding all the others\n",
    "list1 = list(range(39, 5000, 20))\n",
    "for i in tqdm.tqdm(list1):\n",
    "    s1 = '../src/outputs/run_history_' + str(i) + '.csv'\n",
    "    df_ = pd.read_csv(s1, sep=';', index_col=0)\n",
    "    s2 = '../src/outputs/emb_states_' + str(i) + '.npy'\n",
    "    np_emb_state_ = np.load(s2, allow_pickle=True)\n",
    "    if np_emb_state_.shape[0] != df_.shape[0]:\n",
    "        print ('skeeping file %d' %i)\n",
    "        skeeped_files.append(s1)\n",
    "        skeeped_files.append(s2)\n",
    "    else:\n",
    "        df = pd.concat([df,df_], axis=0)\n",
    "        np_emb_state = np.concatenate((np_emb_state, np_emb_state_))\n",
    "        \n",
    "print(df.shape)\n",
    "print(np_emb_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3724bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120311, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71d3e706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>step</th>\n",
       "      <th>original_payload</th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>next_state</th>\n",
       "      <th>reward</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>or 1 = 1 /*</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...</td>\n",
       "      <td>7</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n0x1\\t#_4...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>\" )  or benchmark ( 10000000,MD5 ( 1  )  )  #\"</td>\n",
       "      <td>\"\\t\f",
       " )\\t\f",
       " \\t\f",
       " or\\t\f",
       " benchmark\\t\f",
       " (\\t\f",
       " 0x989680...</td>\n",
       "      <td>9</td>\n",
       "      <td>\"\\t\f",
       "\\t )\\t\f",
       "\\t \\t\f",
       "\\t or\\t\f",
       "\\t benchmark\\t\f",
       "\\t (\\t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>or pg_sleep ( __TIME__ ) --</td>\n",
       "      <td>\\n\\n \\n ||\\n \\n\\n pg_sleep\\n\\n (\\n\\n __TIME__\\...</td>\n",
       "      <td>14</td>\n",
       "      <td>\\n\\n#p7D$y2\\n\\n#p7D$y2\\n||\\n#p7D$y2\\n\\n\\n#p7D$...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>a' or 'a'  =  'a</td>\n",
       "      <td>a'  or  'a'    =    'a</td>\n",
       "      <td>12</td>\n",
       "      <td>a'\u000b",
       " \u000b",
       " or\u000b",
       " \u000b",
       " 'a'\u000b",
       " \u000b",
       " \u000b",
       " \u000b",
       " =\u000b",
       " \u000b",
       " \u000b",
       " \u000b",
       " 'a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>or 1 = 1/*</td>\n",
       "      <td>or\\n\\t\f",
       "\\n 0x1\\n\\t\f",
       "\\n =\\n\\t\f",
       "\\n 0x1/*</td>\n",
       "      <td>14</td>\n",
       "      <td>or\\n\\t\f",
       "\\n#7IM6ta\\n0x1\\n\\t\f",
       "\\n#7IM6ta\\n=\\n\\t\f",
       "\\n#...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     episode  step                                original_payload  \\\n",
       "129        5    10                                     or 1 = 1 /*   \n",
       "183        7    24  \" )  or benchmark ( 10000000,MD5 ( 1  )  )  #\"   \n",
       "224        9    11                     or pg_sleep ( __TIME__ ) --   \n",
       "287       12     3                                a' or 'a'  =  'a   \n",
       "298       13    11                                      or 1 = 1/*   \n",
       "\n",
       "                                                 state  action  \\\n",
       "129  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...       7   \n",
       "183  \"\\t\n",
       " )\\t\n",
       " \\t\n",
       " or\\t\n",
       " benchmark\\t\n",
       " (\\t\n",
       " 0x989680...       9   \n",
       "224  \\n\\n \\n ||\\n \\n\\n pg_sleep\\n\\n (\\n\\n __TIME__\\...      14   \n",
       "287                             a'  or  'a'    =    'a      12   \n",
       "298                or\\n\\t\n",
       "\\n 0x1\\n\\t\n",
       "\\n =\\n\\t\n",
       "\\n 0x1/*      14   \n",
       "\n",
       "                                            next_state  reward  win  \n",
       "129  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n0x1\\t#_4...     0.0    1  \n",
       "183  \"\\t\n",
       "\\t )\\t\n",
       "\\t \\t\n",
       "\\t or\\t\n",
       "\\t benchmark\\t\n",
       "\\t (\\t...     0.0    1  \n",
       "224  \\n\\n#p7D$y2\\n\\n#p7D$y2\\n||\\n#p7D$y2\\n\\n\\n#p7D$...     0.0    1  \n",
       "287                 a'\n",
       " \n",
       " or\n",
       " \n",
       " 'a'\n",
       " \n",
       " \n",
       " \n",
       " =\n",
       " \n",
       " \n",
       " \n",
       " 'a     0.0    1  \n",
       "298  or\\n\\t\n",
       "\\n#7IM6ta\\n0x1\\n\\t\n",
       "\\n#7IM6ta\\n=\\n\\t\n",
       "\\n#...     0.0    1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['reward'] >= 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "851ec739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>step</th>\n",
       "      <th>original_payload</th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>next_state</th>\n",
       "      <th>reward</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>or 1 = 1 /*</td>\n",
       "      <td>or 1 = 1 /*</td>\n",
       "      <td>9</td>\n",
       "      <td>\\t or\\t 1\\t =\\t 1\\t /*</td>\n",
       "      <td>-0.176471</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>or 1 = 1 /*</td>\n",
       "      <td>\\t or\\t 1\\t =\\t 1\\t /*</td>\n",
       "      <td>6</td>\n",
       "      <td>\\t  || \\t 1\\t =\\t 1\\t /*</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>or 1 = 1 /*</td>\n",
       "      <td>\\t  || \\t 1\\t =\\t 1\\t /*</td>\n",
       "      <td>14</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...</td>\n",
       "      <td>-0.049180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>or 1 = 1 /*</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...</td>\n",
       "      <td>16</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...</td>\n",
       "      <td>-0.047619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>or 1 = 1 /*</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...</td>\n",
       "      <td>18</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...</td>\n",
       "      <td>-0.047619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>or 1 = 1 /*</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...</td>\n",
       "      <td>4</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...</td>\n",
       "      <td>-0.047619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>or 1 = 1 /*</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...</td>\n",
       "      <td>12</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...</td>\n",
       "      <td>-0.047619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>or 1 = 1 /*</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...</td>\n",
       "      <td>23</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...</td>\n",
       "      <td>-0.047619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>or 1 = 1 /*</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...</td>\n",
       "      <td>19</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...</td>\n",
       "      <td>-0.047619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>or 1 = 1 /*</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...</td>\n",
       "      <td>7</td>\n",
       "      <td>\\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n0x1\\t#_4...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     episode  step original_payload  \\\n",
       "120        5     1      or 1 = 1 /*   \n",
       "121        5     2      or 1 = 1 /*   \n",
       "122        5     3      or 1 = 1 /*   \n",
       "123        5     4      or 1 = 1 /*   \n",
       "124        5     5      or 1 = 1 /*   \n",
       "125        5     6      or 1 = 1 /*   \n",
       "126        5     7      or 1 = 1 /*   \n",
       "127        5     8      or 1 = 1 /*   \n",
       "128        5     9      or 1 = 1 /*   \n",
       "129        5    10      or 1 = 1 /*   \n",
       "\n",
       "                                                 state  action  \\\n",
       "120                                        or 1 = 1 /*       9   \n",
       "121                             \\t or\\t 1\\t =\\t 1\\t /*       6   \n",
       "122                           \\t  || \\t 1\\t =\\t 1\\t /*      14   \n",
       "123  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...      16   \n",
       "124  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...      18   \n",
       "125  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...       4   \n",
       "126  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...      12   \n",
       "127  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...      23   \n",
       "128  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...      19   \n",
       "129  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7...       7   \n",
       "\n",
       "                                            next_state    reward  win  \n",
       "120                             \\t or\\t 1\\t =\\t 1\\t /* -0.176471    0  \n",
       "121                           \\t  || \\t 1\\t =\\t 1\\t /* -0.157895    0  \n",
       "122  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7... -0.049180    0  \n",
       "123  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7... -0.047619    0  \n",
       "124  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7... -0.047619    0  \n",
       "125  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7... -0.047619    0  \n",
       "126  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7... -0.047619    0  \n",
       "127  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7... -0.047619    0  \n",
       "128  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n1\\t#_4v7... -0.047619    0  \n",
       "129  \\t#_4v7;\\n#_4v7;\\n||#_4v7;\\n\\t#_4v7;\\n0x1\\t#_4...  0.000000    1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['episode'] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95cdff53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120311,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_emb_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17fdf190",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_emb_state_2 = np.array([np.float32(x) for x in np_emb_state])\n",
    "sqli_labels = [x.replace('\\n', '') for x in df['state']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcb49733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_emb_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9d5f48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 11, 22, 14, 16, 10, 6, 23, 7, 22]\n",
      "tensor([ 5, 11, 22,  ...,  0,  9, 17])\n"
     ]
    }
   ],
   "source": [
    "actions = [x for x in df['action']]\n",
    "print(actions[0:10])\n",
    "a_t = torch.Tensor(actions).to(torch.int64)\n",
    "print(a_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12ede6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_one_hot = torch.nn.functional.one_hot(a_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7413e51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120311, 26])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actions_one_hot = torch.stack(actions_one_hot)\n",
    "actions_one_hot.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbd7e588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120311, 768)\n",
      "torch.Size([120311, 768])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "stacked = np.stack(np_emb_state)\n",
    "print(stacked.shape)\n",
    "\n",
    "t_emb = torch.from_numpy(stacked)\n",
    "print(t_emb.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce478e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120311, 794])\n"
     ]
    }
   ],
   "source": [
    "t_x = torch.cat([t_emb, actions_one_hot], dim=1)\n",
    "print(t_x.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49ed469c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1094, -0.1472,  0.0592,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0161, -0.1719,  0.0004,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_x[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cca41f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5500, -0.3069, -0.3069,  ..., -0.1386, -0.1386, -0.1386])\n"
     ]
    }
   ],
   "source": [
    "t_y = df['reward']\n",
    "t_y = [np.float32(x) for x in t_y]\n",
    "t_y = torch.Tensor(t_y).to(torch.float32)\n",
    "print(t_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe531579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLiDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X, y, scale_data=True):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "          # Apply scaling if necessary\n",
    "            if scale_data:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "            self.X = torch.from_numpy(X)\n",
    "            self.y = torch.from_numpy(y)\n",
    "        if torch.is_tensor(X) and torch.is_tensor(y):\n",
    "            if scale_data:\n",
    "                self.X = torch.nn.functional.normalize(X)\n",
    "                self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5a89ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = SQLiDataset(t_x,t_y) # create your datset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6fbb77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.9135e-03, -1.1994e-02,  4.8261e-03,  9.3226e-03, -4.1403e-03,\n",
       "          6.8956e-03,  3.9776e-03, -3.4922e-03, -3.3229e-03,  1.2774e-03,\n",
       "         -1.0768e-02,  7.5708e-03, -2.6476e-03, -5.3864e-03,  1.3389e-02,\n",
       "          1.8549e-03, -4.5513e-03,  2.9522e-04,  7.6157e-05,  3.4239e-02,\n",
       "          4.0677e-03,  6.1225e-03,  1.4116e-02,  1.6052e-02,  3.0476e-03,\n",
       "         -6.2364e-03,  1.1040e-02,  8.7627e-03, -2.9484e-03,  7.3100e-03,\n",
       "         -9.3912e-04,  1.0356e-02,  2.8313e-03,  3.3876e-03,  1.8020e-02,\n",
       "          3.0558e-03,  1.0647e-02,  3.7290e-03, -1.4121e-03,  7.9860e-03,\n",
       "         -5.8115e-03, -2.3613e-02,  7.8835e-04, -6.0830e-04,  8.4837e-03,\n",
       "          2.2791e-03, -3.8470e-03,  1.7585e-02, -2.2578e-03, -2.8194e-03,\n",
       "         -7.6310e-03,  1.9595e-02, -2.4882e-03,  4.3158e-03, -1.8691e-03,\n",
       "          6.1549e-03,  2.7975e-03, -2.6485e-02,  1.1718e-02,  3.2001e-03,\n",
       "         -3.9486e-04,  3.1121e-02, -2.6385e-02,  5.9752e-03,  4.8823e-03,\n",
       "          2.8520e-03, -4.3057e-03, -7.7293e-03,  5.2727e-03, -8.1914e-03,\n",
       "         -2.4099e-03, -8.3419e-04,  1.0123e-02, -1.5816e-02, -2.0397e-03,\n",
       "         -8.5006e-03,  2.7416e-03, -4.7720e-01, -9.4589e-03,  9.7580e-03,\n",
       "          1.8973e-02, -4.1049e-03,  2.5310e-02,  4.7594e-04,  1.8186e-03,\n",
       "         -1.8411e-02,  3.6335e-03, -3.5274e-03,  1.3759e-03,  8.3373e-03,\n",
       "          1.7937e-02, -1.0056e-04,  6.8152e-03,  2.1913e-02,  3.0977e-03,\n",
       "          2.3625e-03,  6.7663e-03, -3.3727e-02, -5.1338e-03, -2.2578e-03,\n",
       "         -1.1153e-02, -3.8930e-03,  2.2954e-02,  1.9856e-02,  1.6991e-03,\n",
       "          2.7517e-03,  3.3192e-05,  2.9577e-03, -1.5340e-02, -2.7771e-03,\n",
       "          1.0740e-02,  1.4011e-02,  1.0127e-03, -1.8090e-03, -3.8512e-03,\n",
       "          8.7064e-03,  1.2635e-02,  1.0448e-02,  5.6651e-03,  2.3358e-02,\n",
       "          5.1962e-03,  1.0094e-02, -4.8950e-03,  1.1840e-02,  3.2075e-03,\n",
       "          2.5375e-02,  1.2385e-03,  2.2049e-03, -8.9449e-03, -1.3244e-03,\n",
       "          4.1654e-03, -1.3367e-02, -5.4274e-03, -2.9804e-02,  1.8447e-02,\n",
       "          6.2246e-03,  6.4171e-03,  1.5044e-03,  1.1357e-02, -3.7117e-04,\n",
       "         -2.8071e-03,  6.3871e-03,  5.2549e-03,  9.6627e-03, -1.3491e-03,\n",
       "          8.7002e-03,  2.0952e-03, -2.2172e-03, -4.5572e-03, -4.4595e-03,\n",
       "          3.9044e-03,  2.5300e-03,  2.1094e-03, -6.6108e-03,  2.5462e-03,\n",
       "          1.7027e-02, -3.1189e-03,  2.1216e-04,  4.7833e-03,  2.4084e-02,\n",
       "          8.3171e-03, -1.5455e-02,  1.3563e-02,  1.0009e-03,  1.0689e-02,\n",
       "          6.9974e-03, -1.7521e-03, -3.4852e-03, -6.9979e-03,  1.4654e-02,\n",
       "         -3.7566e-03,  4.0988e-03,  1.5058e-03, -4.0870e-03, -6.5823e-04,\n",
       "          1.2341e-02,  1.8337e-04,  3.7589e-03, -1.5884e-03, -1.0522e-03,\n",
       "          3.8524e-03,  1.1735e-03,  5.1475e-03, -8.3810e-03, -3.4360e-03,\n",
       "         -9.5560e-03, -1.2252e-03, -7.3400e-03,  3.9579e-03, -1.6047e-03,\n",
       "         -1.0695e-02,  3.5554e-03, -1.4252e-03,  1.6948e-02,  3.1929e-04,\n",
       "          6.9273e-03,  7.0931e-03,  5.2908e-03,  2.5380e-03,  4.1682e-03,\n",
       "          1.1347e-02, -2.0128e-04,  1.8393e-02,  2.9469e-03,  1.4178e-02,\n",
       "          7.4329e-04, -4.6940e-03, -2.5097e-04, -8.3229e-04, -6.1105e-03,\n",
       "          6.9229e-05,  5.1765e-03, -1.0618e-02, -2.9781e-03, -6.6404e-03,\n",
       "         -5.4440e-03, -1.3554e-03, -9.5830e-02,  4.7647e-04, -6.8845e-03,\n",
       "          4.0513e-03, -5.9538e-03, -8.6805e-03,  6.5611e-03,  9.9608e-03,\n",
       "          1.9942e-03, -6.5431e-03, -5.5355e-03,  1.2846e-03,  7.9549e-03,\n",
       "          1.0642e-02, -6.2205e-03, -1.8522e-02, -3.1670e-03,  6.7988e-03,\n",
       "         -8.4865e-03, -1.7467e-03,  2.7794e-03,  1.8977e-03,  1.9444e-03,\n",
       "         -3.3533e-02, -2.1636e-03, -1.0144e-02,  1.9501e-02, -4.1764e-03,\n",
       "          7.2629e-03,  8.3546e-03,  2.9852e-02,  2.8342e-03,  3.2834e-04,\n",
       "          6.5078e-03,  5.2400e-03, -1.2151e-03, -3.6856e-03,  3.7872e-03,\n",
       "          3.9155e-03,  2.4919e-03,  5.7147e-03,  1.1316e-02,  8.0416e-03,\n",
       "          8.4031e-03, -2.3545e-03, -4.7607e-03,  5.9941e-03, -1.3974e-03,\n",
       "         -2.4633e-02,  7.3588e-03, -4.2543e-03,  4.4495e-03, -5.5366e-03,\n",
       "         -6.3277e-03, -6.6477e-03,  6.5777e-03, -5.8100e-03,  3.8682e-03,\n",
       "         -1.5685e-03,  3.4734e-03, -2.7745e-03,  5.1542e-03,  4.2798e-03,\n",
       "          1.1679e-02, -1.9242e-03, -9.8670e-04,  7.6582e-03,  1.0071e-02,\n",
       "          5.0566e-03, -2.0150e-02, -2.2548e-03, -4.2083e-03,  5.0646e-03,\n",
       "          8.0224e-03, -8.0503e-03, -4.2941e-03,  9.7006e-03,  7.5674e-03,\n",
       "         -1.0078e-02,  5.9372e-03,  1.6812e-02,  1.1838e-02,  1.3551e-02,\n",
       "          1.3670e-03,  1.0103e-03, -9.4199e-03, -4.3027e-04,  5.1524e-04,\n",
       "          4.3851e-03, -1.5730e-04,  6.9664e-04, -9.1525e-03,  2.1983e-03,\n",
       "         -9.1234e-03, -5.8066e-03,  8.4915e-03,  1.0878e-02, -4.6350e-03,\n",
       "         -4.7588e-03,  1.5882e-02, -9.0400e-03,  2.1316e-03, -1.6046e-03,\n",
       "          9.5234e-04,  2.2769e-03,  1.4304e-04, -8.8550e-03,  3.5327e-03,\n",
       "          2.7712e-03, -1.0588e-02,  1.4640e-02, -8.3877e-04,  1.7439e-02,\n",
       "          2.5277e-02,  2.0787e-02,  5.0864e-03,  1.6325e-02,  6.9339e-03,\n",
       "          7.9156e-03, -6.5304e-03,  6.2949e-04,  1.3352e-02,  4.4347e-03,\n",
       "         -1.5809e-02,  2.0644e-02, -7.9859e-03,  4.4711e-03,  1.2758e-03,\n",
       "          1.0212e-02,  7.3582e-04, -4.4322e-03,  1.9470e-03, -8.4727e-03,\n",
       "          1.1669e-03, -4.2079e-04,  1.1398e-04, -5.3176e-03, -1.6303e-03,\n",
       "          6.3943e-03,  7.0838e-04,  1.9430e-03,  5.8832e-03, -2.1570e-02,\n",
       "         -9.2100e-03,  1.2496e-02,  6.5280e-03,  5.1370e-03,  1.3073e-02,\n",
       "         -2.1117e-02,  1.6653e-02,  2.1626e-03, -8.7161e-03,  1.2357e-02,\n",
       "          1.0567e-02,  2.4247e-03, -3.9730e-03,  6.5789e-03,  2.5396e-03,\n",
       "          5.9372e-03, -3.1358e-03,  2.0394e-03,  3.6535e-03, -1.5025e-03,\n",
       "         -3.3650e-03,  9.1142e-03,  1.7921e-03, -1.9214e-03,  9.0597e-05,\n",
       "         -2.6275e-03,  7.8318e-03, -9.8616e-03, -3.4357e-03,  1.1463e-02,\n",
       "          5.2767e-04,  2.1574e-03,  7.5611e-03,  4.3220e-03,  3.9758e-03,\n",
       "          3.0659e-03,  8.4226e-03,  3.4566e-04, -1.4794e-03,  5.8174e-03,\n",
       "         -5.1494e-03,  4.0291e-03,  1.3637e-03,  2.0790e-02, -9.5939e-03,\n",
       "          4.1805e-03,  2.9205e-03, -1.2699e-02,  6.7037e-03, -5.0863e-03,\n",
       "         -6.1879e-03, -1.8928e-02,  5.5003e-04, -9.5598e-03, -1.4948e-02,\n",
       "          2.9277e-03,  6.2401e-03,  2.2331e-02,  8.5296e-03, -3.3296e-02,\n",
       "          2.5690e-03,  1.1712e-03, -1.1904e-02,  1.7865e-03,  2.0127e-03,\n",
       "          1.3303e-03,  3.5020e-03,  2.6907e-03, -9.1277e-03,  4.1927e-03,\n",
       "         -7.9902e-04,  1.0724e-02, -8.2153e-03, -6.2540e-03, -1.6887e-03,\n",
       "         -2.8971e-03, -2.6727e-03, -9.4246e-03,  3.7355e-03, -2.4533e-04,\n",
       "          1.5061e-03, -7.3332e-03,  1.1636e-02,  6.1636e-03, -1.6619e-03,\n",
       "          4.4779e-03,  1.9613e-03,  4.9335e-03, -2.0087e-03, -6.5908e-03,\n",
       "         -1.1644e-02,  7.3792e-03, -2.1925e-02, -8.8671e-02,  9.2936e-03,\n",
       "         -7.5945e-03,  2.3117e-03,  1.8682e-03,  4.3470e-03, -1.4076e-03,\n",
       "          8.1831e-03, -9.5406e-03,  3.9518e-03,  2.0019e-03, -3.6010e-03,\n",
       "          4.4480e-03, -5.7207e-03,  2.0616e-02,  3.1261e-03,  1.3016e-02,\n",
       "          3.0038e-04, -7.1980e-03,  3.2428e-04, -1.5800e-02, -3.7177e-03,\n",
       "         -6.7514e-03,  4.8679e-03, -4.9301e-03, -5.9096e-03, -6.7514e-03,\n",
       "          2.7742e-03,  4.9176e-03, -1.5629e-03, -3.6300e-03, -4.4901e-03,\n",
       "          1.1301e-02, -5.4496e-04,  2.4808e-04, -7.0303e-04,  2.4376e-03,\n",
       "          3.0716e-03, -2.4262e-02, -1.3078e-02, -2.6923e-03,  1.4358e-02,\n",
       "          1.3631e-02, -5.6005e-02, -6.3938e-03,  3.2832e-03, -3.4932e-03,\n",
       "          7.6110e-03,  3.7319e-03,  8.1122e-03, -6.2084e-04,  2.0366e-03,\n",
       "         -1.3676e-03, -4.4422e-03,  3.3280e-04, -7.8734e-03,  1.1415e-02,\n",
       "         -5.3713e-03,  7.1413e-03,  1.1616e-02,  1.5401e-03, -3.7702e-03,\n",
       "          3.6437e-03,  1.9202e-02,  5.2823e-03,  7.7441e-03,  1.1148e-02,\n",
       "         -7.1368e-03, -2.4709e-03, -8.4923e-03,  1.2146e-03, -3.2070e-03,\n",
       "         -2.2654e-03, -1.0894e-02,  4.1321e-03,  7.2952e-04, -2.8033e-03,\n",
       "          1.8879e-02,  2.3171e-03,  8.4811e-03,  2.3548e-03,  3.2081e-03,\n",
       "          1.5402e-02, -7.3016e-03, -1.5368e-03, -1.9078e-02, -2.1998e-03,\n",
       "          3.6733e-03, -4.0608e-03,  1.3114e-02,  2.1465e-03,  1.3542e-02,\n",
       "         -9.9608e-03,  1.3676e-02,  1.4323e-03,  4.6710e-03,  1.6897e-02,\n",
       "          5.2070e-03, -2.3601e-02, -1.8834e-03, -1.0561e-02, -1.1506e-02,\n",
       "         -1.1486e-02, -5.5707e-03, -7.7518e-04,  2.7914e-03, -1.5091e-03,\n",
       "         -7.5665e-03,  1.6180e-02,  1.9195e-03, -7.9974e-04,  3.5036e-03,\n",
       "          7.4163e-03,  3.1773e-03,  5.0315e-03, -5.2172e-03, -6.0999e-03,\n",
       "         -4.9256e-02, -3.6819e-03, -1.2452e-02, -1.5876e-02,  1.2325e-03,\n",
       "          3.6338e-03,  1.0578e-02,  7.8013e-03, -4.4239e-03,  6.5448e-03,\n",
       "          1.2124e-02, -1.3477e-03,  3.6248e-03,  1.2014e-02,  8.9804e-03,\n",
       "          1.7408e-02,  1.3651e-02,  1.6439e-02,  8.1193e-01, -1.4752e-02,\n",
       "         -2.2383e-03, -6.0330e-03, -2.3335e-03, -1.7397e-03, -2.2362e-03,\n",
       "          7.7273e-03,  8.0795e-03,  4.3949e-03, -4.6583e-03,  1.4884e-02,\n",
       "         -5.6991e-03, -1.5961e-02,  1.6059e-03,  1.1183e-02,  1.9081e-02,\n",
       "         -3.4341e-03, -1.2139e-02,  1.3515e-02, -3.9217e-04,  1.0260e-02,\n",
       "          7.6322e-03,  4.6253e-02, -4.0548e-03, -7.5604e-03, -2.9087e-03,\n",
       "          1.5774e-02, -1.6016e-03, -2.8029e-03,  6.2394e-03, -1.5271e-03,\n",
       "          1.8217e-03,  7.8921e-03,  1.2157e-02, -7.2999e-03,  1.9294e-02,\n",
       "          3.2114e-03, -1.9810e-03, -7.8813e-03,  9.4721e-03,  2.9751e-03,\n",
       "         -1.5409e-03,  5.3744e-03,  2.5095e-03,  6.5751e-03,  1.4580e-02,\n",
       "          5.2523e-03,  8.6328e-04,  6.1475e-03,  1.3580e-03,  6.4393e-03,\n",
       "          1.6569e-02,  2.7556e-03, -4.2178e-03, -5.4351e-03,  5.8961e-03,\n",
       "          2.7707e-04, -1.0047e-02, -4.9607e-03, -5.3341e-03,  1.2118e-03,\n",
       "         -4.1535e-03, -2.9312e-03,  4.7213e-03,  9.1102e-03,  2.9136e-03,\n",
       "          1.2774e-02, -3.4840e-04, -1.9745e-03,  5.5618e-03, -3.6458e-03,\n",
       "         -1.2178e-02, -1.0233e-03, -1.5803e-02,  6.9496e-03, -2.0958e-02,\n",
       "         -1.1990e-02,  1.2507e-02,  1.2771e-02,  1.1486e-02, -7.5211e-04,\n",
       "         -8.5930e-03, -1.0363e-02, -2.4987e-02, -3.4967e-05,  2.5388e-04,\n",
       "         -2.0366e-03, -1.4927e-03,  4.1004e-03, -1.6641e-02, -6.1469e-03,\n",
       "         -1.5811e-03,  2.5278e-03,  1.4005e-03, -1.4071e-05,  3.1371e-03,\n",
       "          3.0643e-03,  9.9493e-03,  8.0151e-03,  5.9514e-03, -3.8993e-03,\n",
       "         -7.8779e-04,  9.6567e-03,  1.3143e-02, -3.2572e-03, -8.8030e-04,\n",
       "          7.9194e-04,  9.3011e-03,  1.6320e-02, -6.1646e-03,  6.7991e-03,\n",
       "         -9.0278e-03,  9.8895e-03,  9.2239e-04, -4.1617e-03,  3.8175e-03,\n",
       "          2.8304e-03,  1.5727e-03,  7.7301e-03,  5.3762e-03,  4.5821e-03,\n",
       "         -7.7878e-03,  5.9009e-03,  9.7233e-03,  8.1004e-03,  1.1157e-03,\n",
       "          1.2942e-04, -1.3002e-02,  3.3829e-03,  1.9328e-02,  1.3692e-03,\n",
       "          5.9338e-03,  1.6703e-02,  2.5314e-03,  5.2704e-03,  2.1287e-03,\n",
       "          8.4278e-03,  1.6026e-04, -5.6359e-04, -2.5662e-03,  8.1218e-03,\n",
       "          5.5037e-03, -1.5857e-02,  3.2740e-03, -1.0630e-03,  1.5583e-02,\n",
       "         -4.6671e-03,  7.9679e-03,  2.4326e-03, -8.6660e-03,  9.9314e-03,\n",
       "          2.3052e-02, -1.9250e-03, -7.8917e-03,  9.3445e-03, -3.3750e-03,\n",
       "          9.8013e-03,  5.4917e-03,  1.3574e-03, -1.6202e-03, -1.3392e-01,\n",
       "          1.5036e-03,  4.7827e-03, -1.3908e-02, -6.3910e-03,  3.7232e-03,\n",
       "          5.2727e-03, -9.1510e-03,  1.5769e-02,  1.4188e-02,  3.6712e-03,\n",
       "         -1.5795e-02,  2.0033e-03, -6.2800e-03, -5.9903e-03,  2.6572e-02,\n",
       "         -1.0188e-02, -1.5011e-03,  7.9603e-03,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  8.1505e-02,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]),\n",
       " tensor(-0.5500))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2d1c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(df_dataset, [0.7, 0.3], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d97d609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120311\n",
      "84218\n",
      "36093\n"
     ]
    }
   ],
   "source": [
    "print(len(df_dataset))\n",
    "print(len(train_set))\n",
    "print(len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71ca24fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = df['reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "508e4676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f13e4da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    120311.000000\n",
       "mean         -0.266329\n",
       "std           0.158993\n",
       "min          -0.950000\n",
       "25%          -0.355556\n",
       "50%          -0.251613\n",
       "75%          -0.139073\n",
       "max           0.000000\n",
       "Name: reward, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5c3531d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rew_ = df[df['reward'] >= -0.04]\n",
    "rew_ = rew_[rew_['reward'] < 0]\n",
    "len(rew_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todos los que son mayores del -0.04 ya son 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5feda756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hid1 = torch.nn.Linear(794, 128)  \n",
    "        self.hid2 = torch.nn.Linear(128, 32)\n",
    "        self.oupt = torch.nn.Linear(32, 1)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.hid1.weight)\n",
    "        torch.nn.init.zeros_(self.hid1.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.hid2.weight)\n",
    "        torch.nn.init.zeros_(self.hid2.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.oupt.weight)\n",
    "        torch.nn.init.zeros_(self.oupt.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = torch.relu(self.hid1(x))\n",
    "        z = torch.relu(self.hid2(z))\n",
    "        z = self.oupt(z)  # no activation\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d02a682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Avg. loss for epoch 0 0.0017571348\n",
      "Avg. loss for all epochs at epoch 0 0.0017571348\n",
      "Starting epoch 1\n",
      "Avg. loss for epoch 1 0.0008662019\n",
      "Avg. loss for all epochs at epoch 1 0.0013116683\n",
      "Starting epoch 2\n",
      "Avg. loss for epoch 2 0.0007263767\n",
      "Avg. loss for all epochs at epoch 2 0.0011165711\n",
      "Starting epoch 3\n",
      "Avg. loss for epoch 3 0.0006500289\n",
      "Avg. loss for all epochs at epoch 3 0.0009999355\n",
      "Starting epoch 4\n",
      "Avg. loss for epoch 4 0.0005982640\n",
      "Avg. loss for all epochs at epoch 4 0.0009196012\n",
      "Starting epoch 5\n",
      "Avg. loss for epoch 5 0.0005587655\n",
      "Avg. loss for all epochs at epoch 5 0.0008594619\n",
      "Starting epoch 6\n",
      "Avg. loss for epoch 6 0.0005279081\n",
      "Avg. loss for all epochs at epoch 6 0.0008120971\n",
      "Starting epoch 7\n",
      "Avg. loss for epoch 7 0.0005005663\n",
      "Avg. loss for all epochs at epoch 7 0.0007731558\n",
      "Starting epoch 8\n",
      "Avg. loss for epoch 8 0.0004772049\n",
      "Avg. loss for all epochs at epoch 8 0.0007402723\n",
      "Starting epoch 9\n",
      "Avg. loss for epoch 9 0.0004562637\n",
      "Avg. loss for all epochs at epoch 9 0.0007118715\n",
      "Starting epoch 10\n",
      "Avg. loss for epoch 10 0.0004373468\n",
      "Avg. loss for all epochs at epoch 10 0.0006869147\n",
      "Starting epoch 11\n",
      "Avg. loss for epoch 11 0.0004206775\n",
      "Avg. loss for all epochs at epoch 11 0.0006647283\n",
      "Starting epoch 12\n",
      "Avg. loss for epoch 12 0.0004060311\n",
      "Avg. loss for all epochs at epoch 12 0.0006448285\n",
      "Starting epoch 13\n",
      "Avg. loss for epoch 13 0.0003923968\n",
      "Avg. loss for all epochs at epoch 13 0.0006267976\n",
      "Starting epoch 14\n",
      "Avg. loss for epoch 14 0.0003799282\n",
      "Avg. loss for all epochs at epoch 14 0.0006103397\n",
      "Starting epoch 15\n",
      "Avg. loss for epoch 15 0.0003683596\n",
      "Avg. loss for all epochs at epoch 15 0.0005952159\n",
      "Starting epoch 16\n",
      "Avg. loss for epoch 16 0.0003576277\n",
      "Avg. loss for all epochs at epoch 16 0.0005812401\n",
      "Starting epoch 17\n",
      "Avg. loss for epoch 17 0.0003475894\n",
      "Avg. loss for all epochs at epoch 17 0.0005682595\n",
      "Starting epoch 18\n",
      "Avg. loss for epoch 18 0.0003388817\n",
      "Avg. loss for all epochs at epoch 18 0.0005561870\n",
      "Starting epoch 19\n",
      "Avg. loss for epoch 19 0.0003306277\n",
      "Avg. loss for all epochs at epoch 19 0.0005449091\n",
      "Starting epoch 20\n",
      "Avg. loss for epoch 20 0.0003225573\n",
      "Avg. loss for all epochs at epoch 20 0.0005343209\n",
      "Starting epoch 21\n",
      "Avg. loss for epoch 21 0.0003150467\n",
      "Avg. loss for all epochs at epoch 21 0.0005243539\n",
      "Starting epoch 22\n",
      "Avg. loss for epoch 22 0.0003083103\n",
      "Avg. loss for all epochs at epoch 22 0.0005149607\n",
      "Starting epoch 23\n",
      "Avg. loss for epoch 23 0.0003020584\n",
      "Avg. loss for all epochs at epoch 23 0.0005060898\n",
      "Starting epoch 24\n",
      "Avg. loss for epoch 24 0.0002962053\n",
      "Avg. loss for all epochs at epoch 24 0.0004976944\n",
      "Starting epoch 25\n",
      "Avg. loss for epoch 25 0.0002903162\n",
      "Avg. loss for all epochs at epoch 25 0.0004897183\n",
      "Starting epoch 26\n",
      "Avg. loss for epoch 26 0.0002850224\n",
      "Avg. loss for all epochs at epoch 26 0.0004821370\n",
      "Starting epoch 27\n",
      "Avg. loss for epoch 27 0.0002802211\n",
      "Avg. loss for all epochs at epoch 27 0.0004749257\n",
      "Starting epoch 28\n",
      "Avg. loss for epoch 28 0.0002757632\n",
      "Avg. loss for all epochs at epoch 28 0.0004680580\n",
      "Starting epoch 29\n",
      "Avg. loss for epoch 29 0.0002708600\n",
      "Avg. loss for all epochs at epoch 29 0.0004614847\n",
      "Starting epoch 30\n",
      "Avg. loss for epoch 30 0.0002668678\n",
      "Avg. loss for all epochs at epoch 30 0.0004552068\n",
      "Starting epoch 31\n",
      "Avg. loss for epoch 31 0.0002634970\n",
      "Avg. loss for all epochs at epoch 31 0.0004492158\n",
      "Starting epoch 32\n",
      "Avg. loss for epoch 32 0.0002594385\n",
      "Avg. loss for all epochs at epoch 32 0.0004434650\n",
      "Starting epoch 33\n",
      "Avg. loss for epoch 33 0.0002559423\n",
      "Avg. loss for all epochs at epoch 33 0.0004379496\n",
      "Starting epoch 34\n",
      "Avg. loss for epoch 34 0.0002527771\n",
      "Avg. loss for all epochs at epoch 34 0.0004326590\n",
      "Starting epoch 35\n",
      "Avg. loss for epoch 35 0.0002497570\n",
      "Avg. loss for all epochs at epoch 35 0.0004275784\n",
      "Starting epoch 36\n",
      "Avg. loss for epoch 36 0.0002469099\n",
      "Avg. loss for all epochs at epoch 36 0.0004226955\n",
      "Starting epoch 37\n",
      "Avg. loss for epoch 37 0.0002438023\n",
      "Avg. loss for all epochs at epoch 37 0.0004179877\n",
      "Starting epoch 38\n",
      "Avg. loss for epoch 38 0.0002411023\n",
      "Avg. loss for all epochs at epoch 38 0.0004134522\n",
      "Starting epoch 39\n",
      "Avg. loss for epoch 39 0.0002386622\n",
      "Avg. loss for all epochs at epoch 39 0.0004090825\n",
      "Starting epoch 40\n",
      "Avg. loss for epoch 40 0.0002361480\n",
      "Avg. loss for all epochs at epoch 40 0.0004048646\n",
      "Starting epoch 41\n",
      "Avg. loss for epoch 41 0.0002337930\n",
      "Avg. loss for all epochs at epoch 41 0.0004007914\n",
      "Starting epoch 42\n",
      "Avg. loss for epoch 42 0.0002311825\n",
      "Avg. loss for all epochs at epoch 42 0.0003968470\n",
      "Starting epoch 43\n",
      "Avg. loss for epoch 43 0.0002291951\n",
      "Avg. loss for all epochs at epoch 43 0.0003930368\n",
      "Starting epoch 44\n",
      "Avg. loss for epoch 44 0.0002269215\n",
      "Avg. loss for all epochs at epoch 44 0.0003893453\n",
      "Starting epoch 45\n",
      "Avg. loss for epoch 45 0.0002248393\n",
      "Avg. loss for all epochs at epoch 45 0.0003857691\n",
      "Starting epoch 46\n",
      "Avg. loss for epoch 46 0.0002231152\n",
      "Avg. loss for all epochs at epoch 46 0.0003823084\n",
      "Starting epoch 47\n",
      "Avg. loss for epoch 47 0.0002208728\n",
      "Avg. loss for all epochs at epoch 47 0.0003789451\n",
      "Starting epoch 48\n",
      "Avg. loss for epoch 48 0.0002190409\n",
      "Avg. loss for all epochs at epoch 48 0.0003756818\n",
      "Starting epoch 49\n",
      "Avg. loss for epoch 49 0.0002169954\n",
      "Avg. loss for all epochs at epoch 49 0.0003725080\n",
      "Starting epoch 50\n",
      "Avg. loss for epoch 50 0.0002153546\n",
      "Avg. loss for all epochs at epoch 50 0.0003694266\n",
      "Starting epoch 51\n",
      "Avg. loss for epoch 51 0.0002137745\n",
      "Avg. loss for all epochs at epoch 51 0.0003664333\n",
      "Starting epoch 52\n",
      "Avg. loss for epoch 52 0.0002120972\n",
      "Avg. loss for all epochs at epoch 52 0.0003635213\n",
      "Starting epoch 53\n",
      "Avg. loss for epoch 53 0.0002101409\n",
      "Avg. loss for all epochs at epoch 53 0.0003606809\n",
      "Starting epoch 54\n",
      "Avg. loss for epoch 54 0.0002087073\n",
      "Avg. loss for all epochs at epoch 54 0.0003579178\n",
      "Starting epoch 55\n",
      "Avg. loss for epoch 55 0.0002071591\n",
      "Avg. loss for all epochs at epoch 55 0.0003552256\n",
      "Starting epoch 56\n",
      "Avg. loss for epoch 56 0.0002056793\n",
      "Avg. loss for all epochs at epoch 56 0.0003526020\n",
      "Starting epoch 57\n",
      "Avg. loss for epoch 57 0.0002044731\n",
      "Avg. loss for all epochs at epoch 57 0.0003500481\n",
      "Starting epoch 58\n",
      "Avg. loss for epoch 58 0.0002029803\n",
      "Avg. loss for all epochs at epoch 58 0.0003475554\n",
      "Starting epoch 59\n",
      "Avg. loss for epoch 59 0.0002016334\n",
      "Avg. loss for all epochs at epoch 59 0.0003451234\n",
      "Starting epoch 60\n",
      "Avg. loss for epoch 60 0.0002004334\n",
      "Avg. loss for all epochs at epoch 60 0.0003427514\n",
      "Starting epoch 61\n",
      "Avg. loss for epoch 61 0.0001990217\n",
      "Avg. loss for all epochs at epoch 61 0.0003404332\n",
      "Starting epoch 62\n",
      "Avg. loss for epoch 62 0.0001975997\n",
      "Avg. loss for all epochs at epoch 62 0.0003381660\n",
      "Starting epoch 63\n",
      "Avg. loss for epoch 63 0.0001966688\n",
      "Avg. loss for all epochs at epoch 63 0.0003359551\n",
      "Starting epoch 64\n",
      "Avg. loss for epoch 64 0.0001950923\n",
      "Avg. loss for all epochs at epoch 64 0.0003337880\n",
      "Starting epoch 65\n",
      "Avg. loss for epoch 65 0.0001940211\n",
      "Avg. loss for all epochs at epoch 65 0.0003316703\n",
      "Starting epoch 66\n",
      "Avg. loss for epoch 66 0.0001928339\n",
      "Avg. loss for all epochs at epoch 66 0.0003295981\n",
      "Starting epoch 67\n",
      "Avg. loss for epoch 67 0.0001915135\n",
      "Avg. loss for all epochs at epoch 67 0.0003275674\n",
      "Starting epoch 68\n",
      "Avg. loss for epoch 68 0.0001903748\n",
      "Avg. loss for all epochs at epoch 68 0.0003255791\n",
      "Starting epoch 69\n",
      "Avg. loss for epoch 69 0.0001895293\n",
      "Avg. loss for all epochs at epoch 69 0.0003236356\n",
      "Starting epoch 70\n",
      "Avg. loss for epoch 70 0.0001884130\n",
      "Avg. loss for all epochs at epoch 70 0.0003217310\n",
      "Starting epoch 71\n",
      "Avg. loss for epoch 71 0.0001874221\n",
      "Avg. loss for all epochs at epoch 71 0.0003198656\n",
      "Starting epoch 72\n",
      "Avg. loss for epoch 72 0.0001865818\n",
      "Avg. loss for all epochs at epoch 72 0.0003180398\n",
      "Starting epoch 73\n",
      "Avg. loss for epoch 73 0.0001852742\n",
      "Avg. loss for all epochs at epoch 73 0.0003162457\n",
      "Starting epoch 74\n",
      "Avg. loss for epoch 74 0.0001844442\n",
      "Avg. loss for all epochs at epoch 74 0.0003144883\n",
      "Starting epoch 75\n",
      "Avg. loss for epoch 75 0.0001832247\n",
      "Avg. loss for all epochs at epoch 75 0.0003127612\n",
      "Starting epoch 76\n",
      "Avg. loss for epoch 76 0.0001822573\n",
      "Avg. loss for all epochs at epoch 76 0.0003110663\n",
      "Starting epoch 77\n",
      "Avg. loss for epoch 77 0.0001814325\n",
      "Avg. loss for all epochs at epoch 77 0.0003094044\n",
      "Starting epoch 78\n",
      "Avg. loss for epoch 78 0.0001806212\n",
      "Avg. loss for all epochs at epoch 78 0.0003077742\n",
      "Starting epoch 79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss for epoch 79 0.0001796318\n",
      "Avg. loss for all epochs at epoch 79 0.0003061724\n",
      "Starting epoch 80\n",
      "Avg. loss for epoch 80 0.0001786853\n",
      "Avg. loss for all epochs at epoch 80 0.0003045985\n",
      "Starting epoch 81\n",
      "Avg. loss for epoch 81 0.0001780225\n",
      "Avg. loss for all epochs at epoch 81 0.0003030549\n",
      "Starting epoch 82\n",
      "Avg. loss for epoch 82 0.0001770823\n",
      "Avg. loss for all epochs at epoch 82 0.0003015372\n",
      "Starting epoch 83\n",
      "Avg. loss for epoch 83 0.0001764122\n",
      "Avg. loss for all epochs at epoch 83 0.0003000476\n",
      "Starting epoch 84\n",
      "Avg. loss for epoch 84 0.0001755471\n",
      "Avg. loss for all epochs at epoch 84 0.0002985829\n",
      "Starting epoch 85\n",
      "Avg. loss for epoch 85 0.0001746143\n",
      "Avg. loss for all epochs at epoch 85 0.0002971414\n",
      "Starting epoch 86\n",
      "Avg. loss for epoch 86 0.0001741017\n",
      "Avg. loss for all epochs at epoch 86 0.0002957271\n",
      "Starting epoch 87\n",
      "Avg. loss for epoch 87 0.0001731776\n",
      "Avg. loss for all epochs at epoch 87 0.0002943345\n",
      "Starting epoch 88\n",
      "Avg. loss for epoch 88 0.0001720799\n",
      "Avg. loss for all epochs at epoch 88 0.0002929609\n",
      "Starting epoch 89\n",
      "Avg. loss for epoch 89 0.0001715260\n",
      "Avg. loss for all epochs at epoch 89 0.0002916116\n",
      "Starting epoch 90\n",
      "Avg. loss for epoch 90 0.0001710493\n",
      "Avg. loss for all epochs at epoch 90 0.0002902867\n",
      "Starting epoch 91\n",
      "Avg. loss for epoch 91 0.0001699684\n",
      "Avg. loss for all epochs at epoch 91 0.0002889789\n",
      "Starting epoch 92\n",
      "Avg. loss for epoch 92 0.0001693304\n",
      "Avg. loss for all epochs at epoch 92 0.0002876924\n",
      "Starting epoch 93\n",
      "Avg. loss for epoch 93 0.0001686082\n",
      "Avg. loss for all epochs at epoch 93 0.0002864255\n",
      "Starting epoch 94\n",
      "Avg. loss for epoch 94 0.0001679698\n",
      "Avg. loss for all epochs at epoch 94 0.0002851786\n",
      "Starting epoch 95\n",
      "Avg. loss for epoch 95 0.0001673912\n",
      "Avg. loss for all epochs at epoch 95 0.0002839517\n",
      "Starting epoch 96\n",
      "Avg. loss for epoch 96 0.0001665492\n",
      "Avg. loss for all epochs at epoch 96 0.0002827413\n",
      "Starting epoch 97\n",
      "Avg. loss for epoch 97 0.0001658283\n",
      "Avg. loss for all epochs at epoch 97 0.0002815483\n",
      "Starting epoch 98\n",
      "Avg. loss for epoch 98 0.0001652698\n",
      "Avg. loss for all epochs at epoch 98 0.0002803738\n",
      "Starting epoch 99\n",
      "Avg. loss for epoch 99 0.0001645856\n",
      "Avg. loss for all epochs at epoch 99 0.0002792159\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# Initialize the MLP\n",
    "mlp = SimpleNet()\n",
    "\n",
    "lr = 1e-5\n",
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Hold the best model\n",
    "best_mse = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=lr)\n",
    "  \n",
    "# Run the training loop\n",
    "for epoch in range(n_epochs): \n",
    "    mlp.train()\n",
    "    print('Starting epoch %d' %epoch)\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "      \n",
    "        \n",
    "      # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        # inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "      \n",
    "      # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "      \n",
    "      # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "      \n",
    "      # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "      \n",
    "      # Perform backward pass\n",
    "        loss.backward()\n",
    "      \n",
    "      # Perform optimization\n",
    "        optimizer.step()\n",
    "      \n",
    "      # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            current_loss = current_loss/(100*batch_size)\n",
    "            epoch_losses.append(current_loss)\n",
    "            #print('Loss after mini-batch %5d: %.10f' %\n",
    "            #    (i + 1, current_loss))\n",
    "            current_loss = 0.0\n",
    "                  \n",
    "    e_l_m =  np.mean(epoch_losses)\n",
    "    print('Avg. loss for epoch %d %.10f' %(epoch, e_l_m))\n",
    "    history.append(e_l_m)\n",
    "    print('Avg. loss for all epochs at epoch %d %.10f' %(epoch, np.mean(history)))\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2407d4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (hid1): Linear(in_features=794, out_features=128, bias=True)\n",
       "  (hid2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (oupt): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0db44573",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "testloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c781f15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss for all validation 0.0018479266\n"
     ]
    }
   ],
   "source": [
    "losses_val = []\n",
    "\n",
    "# Iterate over the DataLoader for training data\n",
    "for i, data in enumerate(testloader, 0):\n",
    "              \n",
    "    # Get and prepare inputs\n",
    "    inputs, targets = data\n",
    "    # inputs, targets = inputs.float(), targets.float()\n",
    "    targets = targets.reshape((targets.shape[0], 1))\n",
    "      \n",
    "    # Perform forward pass\n",
    "    outputs = mlp(inputs)\n",
    "      \n",
    "    # Compute loss\n",
    "    loss = loss_function(outputs, targets)\n",
    "      \n",
    "    # Print statistics\n",
    "    current_loss += loss.item()\n",
    "    if i % 100 == 0:\n",
    "        current_loss = current_loss/(100*batch_size)\n",
    "        losses_val.append(current_loss)\n",
    "        current_loss = 0.0\n",
    "                  \n",
    "l_m_val =  np.mean(losses_val)\n",
    "print('Avg. loss for all validation %.10f' %l_m_val) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b607069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs -0.0786077380\n",
      "Target 0.0000000000\n",
      "Loss 0.0061791763\n",
      "---------------------------\n",
      "Outputs -0.0555129535\n",
      "Target 0.0000000000\n",
      "Loss 0.0030816880\n",
      "---------------------------\n",
      "Outputs -0.0513866879\n",
      "Target 0.0000000000\n",
      "Loss 0.0026405917\n",
      "---------------------------\n",
      "Outputs -0.0337302946\n",
      "Target 0.0000000000\n",
      "Loss 0.0011377328\n",
      "---------------------------\n",
      "Outputs -0.0702230930\n",
      "Target 0.0000000000\n",
      "Loss 0.0049312827\n",
      "---------------------------\n",
      "Outputs -0.1896830052\n",
      "Target 0.0000000000\n",
      "Loss 0.0359796435\n",
      "---------------------------\n",
      "Outputs 0.0511640869\n",
      "Target 0.0000000000\n",
      "Loss 0.0026177638\n",
      "---------------------------\n",
      "Outputs -0.0609757639\n",
      "Target 0.0000000000\n",
      "Loss 0.0037180437\n",
      "---------------------------\n",
      "Outputs -0.1450635046\n",
      "Target 0.0000000000\n",
      "Loss 0.0210434198\n",
      "---------------------------\n",
      "Outputs -0.0985656679\n",
      "Target 0.0000000000\n",
      "Loss 0.0097151911\n",
      "---------------------------\n",
      "Outputs -0.0350415446\n",
      "Target 0.0000000000\n",
      "Loss 0.0012279098\n",
      "---------------------------\n",
      "Outputs -0.0806966871\n",
      "Target 0.0000000000\n",
      "Loss 0.0065119555\n",
      "---------------------------\n",
      "Outputs -0.0886137933\n",
      "Target 0.0000000000\n",
      "Loss 0.0078524044\n",
      "---------------------------\n",
      "Outputs -0.1429191381\n",
      "Target 0.0000000000\n",
      "Loss 0.0204258803\n",
      "---------------------------\n",
      "Outputs -0.0993165374\n",
      "Target 0.0000000000\n",
      "Loss 0.0098637743\n",
      "---------------------------\n",
      "Outputs -0.0758680701\n",
      "Target 0.0000000000\n",
      "Loss 0.0057559642\n",
      "---------------------------\n",
      "Outputs -0.1163981110\n",
      "Target 0.0000000000\n",
      "Loss 0.0135485204\n",
      "---------------------------\n",
      "Outputs -0.1096980870\n",
      "Target 0.0000000000\n",
      "Loss 0.0120336702\n",
      "---------------------------\n",
      "Outputs -0.0479900353\n",
      "Target 0.0000000000\n",
      "Loss 0.0023030434\n",
      "---------------------------\n",
      "Outputs -0.0583952405\n",
      "Target 0.0000000000\n",
      "Loss 0.0034100041\n",
      "---------------------------\n",
      "Outputs -0.0612028427\n",
      "Target 0.0000000000\n",
      "Loss 0.0037457880\n",
      "---------------------------\n",
      "Outputs 0.0019276701\n",
      "Target 0.0000000000\n",
      "Loss 0.0000037159\n",
      "---------------------------\n",
      "Outputs -0.0755124390\n",
      "Target 0.0000000000\n",
      "Loss 0.0057021286\n",
      "---------------------------\n",
      "Outputs -0.1643330455\n",
      "Target 0.0000000000\n",
      "Loss 0.0270053502\n",
      "---------------------------\n",
      "Outputs -0.0473292060\n",
      "Target 0.0000000000\n",
      "Loss 0.0022400538\n",
      "---------------------------\n",
      "Outputs -0.0744734108\n",
      "Target 0.0000000000\n",
      "Loss 0.0055462890\n",
      "---------------------------\n",
      "Outputs -0.0092110448\n",
      "Target 0.0000000000\n",
      "Loss 0.0000848433\n",
      "---------------------------\n",
      "Outputs -0.1340301782\n",
      "Target 0.0000000000\n",
      "Loss 0.0179640893\n",
      "---------------------------\n",
      "Outputs 0.0268801637\n",
      "Target 0.0000000000\n",
      "Loss 0.0007225432\n",
      "---------------------------\n",
      "Outputs -0.0240731277\n",
      "Target 0.0000000000\n",
      "Loss 0.0005795155\n",
      "---------------------------\n",
      "Outputs -0.1259061098\n",
      "Target 0.0000000000\n",
      "Loss 0.0158523489\n",
      "---------------------------\n",
      "Outputs -0.0089888535\n",
      "Target 0.0000000000\n",
      "Loss 0.0000807995\n",
      "---------------------------\n",
      "Outputs -0.1214624792\n",
      "Target 0.0000000000\n",
      "Loss 0.0147531340\n",
      "---------------------------\n",
      "Outputs -0.1450084299\n",
      "Target 0.0000000000\n",
      "Loss 0.0210274439\n",
      "---------------------------\n",
      "Outputs -0.0072467960\n",
      "Target 0.0000000000\n",
      "Loss 0.0000525161\n",
      "---------------------------\n",
      "Outputs -0.0940812826\n",
      "Target 0.0000000000\n",
      "Loss 0.0088512879\n",
      "---------------------------\n",
      "Outputs -0.0407697186\n",
      "Target 0.0000000000\n",
      "Loss 0.0016621699\n",
      "---------------------------\n",
      "Outputs -0.0933477730\n",
      "Target 0.0000000000\n",
      "Loss 0.0087138070\n",
      "---------------------------\n",
      "Outputs 0.0359363109\n",
      "Target 0.0000000000\n",
      "Loss 0.0012914184\n",
      "---------------------------\n",
      "Outputs -0.1447135508\n",
      "Target 0.0000000000\n",
      "Loss 0.0209420118\n",
      "---------------------------\n",
      "Outputs -0.0847851783\n",
      "Target 0.0000000000\n",
      "Loss 0.0071885264\n",
      "---------------------------\n",
      "Outputs -0.0466084071\n",
      "Target 0.0000000000\n",
      "Loss 0.0021723437\n",
      "---------------------------\n",
      "Outputs -0.0144961588\n",
      "Target 0.0000000000\n",
      "Loss 0.0002101386\n",
      "---------------------------\n",
      "Outputs -0.0230926685\n",
      "Target 0.0000000000\n",
      "Loss 0.0005332713\n",
      "---------------------------\n",
      "Outputs -0.1317325383\n",
      "Target 0.0000000000\n",
      "Loss 0.0173534621\n",
      "---------------------------\n",
      "Outputs -0.0439647771\n",
      "Target 0.0000000000\n",
      "Loss 0.0019329017\n",
      "---------------------------\n",
      "Outputs -0.0226336531\n",
      "Target 0.0000000000\n",
      "Loss 0.0005122822\n",
      "---------------------------\n",
      "Outputs -0.0562385358\n",
      "Target 0.0000000000\n",
      "Loss 0.0031627729\n",
      "---------------------------\n",
      "Outputs 0.0337476395\n",
      "Target 0.0000000000\n",
      "Loss 0.0011389032\n",
      "---------------------------\n",
      "Outputs -0.0184688754\n",
      "Target 0.0000000000\n",
      "Loss 0.0003410994\n",
      "---------------------------\n",
      "Outputs -0.0212823786\n",
      "Target 0.0000000000\n",
      "Loss 0.0004529396\n",
      "---------------------------\n",
      "Outputs -0.0439807065\n",
      "Target 0.0000000000\n",
      "Loss 0.0019343025\n",
      "---------------------------\n",
      "Outputs -0.0116359852\n",
      "Target 0.0000000000\n",
      "Loss 0.0001353962\n",
      "---------------------------\n",
      "Outputs -0.0982071012\n",
      "Target 0.0000000000\n",
      "Loss 0.0096446350\n",
      "---------------------------\n",
      "Outputs -0.0609254204\n",
      "Target 0.0000000000\n",
      "Loss 0.0037119070\n",
      "---------------------------\n",
      "Outputs 0.0155207254\n",
      "Target 0.0000000000\n",
      "Loss 0.0002408929\n",
      "---------------------------\n",
      "Outputs -0.0335635357\n",
      "Target 0.0000000000\n",
      "Loss 0.0011265109\n",
      "---------------------------\n",
      "Outputs -0.0823450685\n",
      "Target 0.0000000000\n",
      "Loss 0.0067807101\n",
      "---------------------------\n",
      "Outputs -0.0325862803\n",
      "Target 0.0000000000\n",
      "Loss 0.0010618657\n",
      "---------------------------\n",
      "Outputs -0.1775858402\n",
      "Target 0.0000000000\n",
      "Loss 0.0315367319\n",
      "---------------------------\n",
      "Outputs -0.0884440541\n",
      "Target 0.0000000000\n",
      "Loss 0.0078223506\n",
      "---------------------------\n",
      "Outputs -0.0353167243\n",
      "Target 0.0000000000\n",
      "Loss 0.0012472710\n",
      "---------------------------\n",
      "Outputs -0.0870695561\n",
      "Target 0.0000000000\n",
      "Loss 0.0075811078\n",
      "---------------------------\n",
      "Outputs -0.0502187945\n",
      "Target 0.0000000000\n",
      "Loss 0.0025219272\n",
      "---------------------------\n",
      "Outputs -0.1059471965\n",
      "Target 0.0000000000\n",
      "Loss 0.0112248082\n",
      "---------------------------\n",
      "Outputs -0.0408371948\n",
      "Target 0.0000000000\n",
      "Loss 0.0016676765\n",
      "---------------------------\n",
      "Outputs -0.0263550244\n",
      "Target 0.0000000000\n",
      "Loss 0.0006945873\n",
      "---------------------------\n",
      "Outputs 0.0362961069\n",
      "Target 0.0000000000\n",
      "Loss 0.0013174074\n",
      "---------------------------\n",
      "Outputs 0.0163550228\n",
      "Target 0.0000000000\n",
      "Loss 0.0002674868\n",
      "---------------------------\n",
      "Outputs -0.0092899986\n",
      "Target 0.0000000000\n",
      "Loss 0.0000863041\n",
      "---------------------------\n",
      "Outputs -0.0150011070\n",
      "Target 0.0000000000\n",
      "Loss 0.0002250332\n",
      "---------------------------\n",
      "Outputs -0.0580728985\n",
      "Target 0.0000000000\n",
      "Loss 0.0033724615\n",
      "---------------------------\n",
      "Outputs -0.0927040428\n",
      "Target 0.0000000000\n",
      "Loss 0.0085940398\n",
      "---------------------------\n",
      "Outputs -0.0552468933\n",
      "Target 0.0000000000\n",
      "Loss 0.0030522193\n",
      "---------------------------\n",
      "Outputs -0.0488301031\n",
      "Target 0.0000000000\n",
      "Loss 0.0023843790\n",
      "---------------------------\n",
      "Outputs -0.1668740064\n",
      "Target 0.0000000000\n",
      "Loss 0.0278469343\n",
      "---------------------------\n",
      "Outputs -0.0959831476\n",
      "Target 0.0000000000\n",
      "Loss 0.0092127649\n",
      "---------------------------\n",
      "Outputs -0.0448960327\n",
      "Target 0.0000000000\n",
      "Loss 0.0020156538\n",
      "---------------------------\n",
      "Outputs -0.0212825425\n",
      "Target 0.0000000000\n",
      "Loss 0.0004529466\n",
      "---------------------------\n",
      "Outputs -0.0582779013\n",
      "Target 0.0000000000\n",
      "Loss 0.0033963139\n",
      "---------------------------\n",
      "Outputs -0.1545943469\n",
      "Target 0.0000000000\n",
      "Loss 0.0238994118\n",
      "---------------------------\n",
      "Outputs -0.0709845424\n",
      "Target 0.0000000000\n",
      "Loss 0.0050388053\n",
      "---------------------------\n",
      "Outputs -0.0451301970\n",
      "Target 0.0000000000\n",
      "Loss 0.0020367347\n",
      "---------------------------\n",
      "Outputs -0.0167458765\n",
      "Target 0.0000000000\n",
      "Loss 0.0002804244\n",
      "---------------------------\n",
      "Outputs 0.0220005773\n",
      "Target 0.0000000000\n",
      "Loss 0.0004840254\n",
      "---------------------------\n",
      "Outputs -0.0637574792\n",
      "Target 0.0000000000\n",
      "Loss 0.0040650163\n",
      "---------------------------\n",
      "Outputs -0.0134959109\n",
      "Target 0.0000000000\n",
      "Loss 0.0001821396\n",
      "---------------------------\n",
      "Outputs -0.1125324070\n",
      "Target 0.0000000000\n",
      "Loss 0.0126635423\n",
      "---------------------------\n",
      "Outputs -0.0459992923\n",
      "Target 0.0000000000\n",
      "Loss 0.0021159349\n",
      "---------------------------\n",
      "Outputs -0.0431029014\n",
      "Target 0.0000000000\n",
      "Loss 0.0018578601\n",
      "---------------------------\n",
      "Outputs -0.0599400140\n",
      "Target 0.0000000000\n",
      "Loss 0.0035928052\n",
      "---------------------------\n",
      "Outputs -0.0087367035\n",
      "Target 0.0000000000\n",
      "Loss 0.0000763300\n",
      "---------------------------\n",
      "Outputs -0.0154821686\n",
      "Target 0.0000000000\n",
      "Loss 0.0002396975\n",
      "---------------------------\n",
      "Outputs -0.1006173790\n",
      "Target 0.0000000000\n",
      "Loss 0.0101238573\n",
      "---------------------------\n",
      "Outputs -0.0708947480\n",
      "Target 0.0000000000\n",
      "Loss 0.0050260653\n",
      "---------------------------\n",
      "Outputs -0.0180640407\n",
      "Target 0.0000000000\n",
      "Loss 0.0003263096\n",
      "---------------------------\n",
      "Outputs -0.0700863302\n",
      "Target 0.0000000000\n",
      "Loss 0.0049120937\n",
      "---------------------------\n",
      "Outputs -0.0328997038\n",
      "Target 0.0000000000\n",
      "Loss 0.0010823905\n",
      "---------------------------\n",
      "Outputs -0.2198329568\n",
      "Target 0.0000000000\n",
      "Loss 0.0483265296\n",
      "---------------------------\n",
      "Outputs -0.1845441461\n",
      "Target 0.0000000000\n",
      "Loss 0.0340565406\n",
      "---------------------------\n",
      "Outputs 0.0252995901\n",
      "Target 0.0000000000\n",
      "Loss 0.0006400693\n",
      "---------------------------\n",
      "Outputs -0.0707278997\n",
      "Target 0.0000000000\n",
      "Loss 0.0050024358\n",
      "---------------------------\n",
      "Outputs -0.0281410180\n",
      "Target 0.0000000000\n",
      "Loss 0.0007919169\n",
      "---------------------------\n",
      "Outputs -0.0348832719\n",
      "Target 0.0000000000\n",
      "Loss 0.0012168427\n",
      "---------------------------\n",
      "Outputs -0.0841594785\n",
      "Target 0.0000000000\n",
      "Loss 0.0070828176\n",
      "---------------------------\n",
      "Outputs 0.0231101029\n",
      "Target 0.0000000000\n",
      "Loss 0.0005340768\n",
      "---------------------------\n",
      "Outputs -0.0440233685\n",
      "Target 0.0000000000\n",
      "Loss 0.0019380570\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs -0.0813704431\n",
      "Target 0.0000000000\n",
      "Loss 0.0066211489\n",
      "---------------------------\n",
      "Outputs -0.0964433998\n",
      "Target 0.0000000000\n",
      "Loss 0.0093013290\n",
      "---------------------------\n",
      "Outputs -0.0402009375\n",
      "Target 0.0000000000\n",
      "Loss 0.0016161153\n",
      "---------------------------\n",
      "Outputs -0.0366102569\n",
      "Target 0.0000000000\n",
      "Loss 0.0013403109\n",
      "---------------------------\n",
      "Outputs -0.1845441461\n",
      "Target 0.0000000000\n",
      "Loss 0.0340565406\n",
      "---------------------------\n",
      "Outputs -0.1099269539\n",
      "Target 0.0000000000\n",
      "Loss 0.0120839356\n",
      "---------------------------\n",
      "Outputs -0.0549062602\n",
      "Target 0.0000000000\n",
      "Loss 0.0030146975\n",
      "---------------------------\n",
      "Outputs 0.0012062900\n",
      "Target 0.0000000000\n",
      "Loss 0.0000014551\n",
      "---------------------------\n",
      "Outputs -0.0934914798\n",
      "Target 0.0000000000\n",
      "Loss 0.0087406570\n",
      "---------------------------\n",
      "Outputs -0.0583140664\n",
      "Target 0.0000000000\n",
      "Loss 0.0034005304\n",
      "---------------------------\n",
      "Outputs 0.0008242801\n",
      "Target 0.0000000000\n",
      "Loss 0.0000006794\n",
      "---------------------------\n",
      "Outputs -0.0546356402\n",
      "Target 0.0000000000\n",
      "Loss 0.0029850532\n",
      "---------------------------\n",
      "Outputs -0.0502162911\n",
      "Target 0.0000000000\n",
      "Loss 0.0025216758\n",
      "---------------------------\n",
      "Outputs -0.0740572214\n",
      "Target 0.0000000000\n",
      "Loss 0.0054844720\n",
      "---------------------------\n",
      "Outputs -0.1206416488\n",
      "Target 0.0000000000\n",
      "Loss 0.0145544074\n",
      "---------------------------\n",
      "Outputs -0.0805722624\n",
      "Target 0.0000000000\n",
      "Loss 0.0064918892\n",
      "---------------------------\n",
      "Outputs -0.0948725939\n",
      "Target 0.0000000000\n",
      "Loss 0.0090008089\n",
      "---------------------------\n",
      "Outputs -0.0435008965\n",
      "Target 0.0000000000\n",
      "Loss 0.0018923280\n",
      "---------------------------\n",
      "Outputs -0.0236646943\n",
      "Target 0.0000000000\n",
      "Loss 0.0005600178\n",
      "---------------------------\n",
      "Outputs -0.1017751545\n",
      "Target 0.0000000000\n",
      "Loss 0.0103581818\n",
      "---------------------------\n",
      "Outputs -0.0755369961\n",
      "Target 0.0000000000\n",
      "Loss 0.0057058376\n",
      "---------------------------\n",
      "Outputs -0.0602011792\n",
      "Target 0.0000000000\n",
      "Loss 0.0036241820\n",
      "---------------------------\n",
      "Outputs -0.2523265183\n",
      "Target 0.0000000000\n",
      "Loss 0.0636686683\n",
      "---------------------------\n",
      "Outputs -0.0125448518\n",
      "Target 0.0000000000\n",
      "Loss 0.0001573733\n",
      "---------------------------\n",
      "Outputs -0.0715203434\n",
      "Target 0.0000000000\n",
      "Loss 0.0051151593\n",
      "---------------------------\n",
      "Outputs -0.1032861769\n",
      "Target 0.0000000000\n",
      "Loss 0.0106680347\n",
      "---------------------------\n",
      "Outputs -0.1499424130\n",
      "Target 0.0000000000\n",
      "Loss 0.0224827267\n",
      "---------------------------\n",
      "Outputs -0.0642676353\n",
      "Target 0.0000000000\n",
      "Loss 0.0041303290\n",
      "---------------------------\n",
      "Outputs -0.0225988440\n",
      "Target 0.0000000000\n",
      "Loss 0.0005107077\n",
      "---------------------------\n",
      "Outputs -0.0550832041\n",
      "Target 0.0000000000\n",
      "Loss 0.0030341593\n",
      "---------------------------\n",
      "Outputs -0.0637892634\n",
      "Target 0.0000000000\n",
      "Loss 0.0040690703\n",
      "---------------------------\n",
      "Outputs -0.1795140654\n",
      "Target 0.0000000000\n",
      "Loss 0.0322252996\n",
      "---------------------------\n",
      "Outputs 0.0069335662\n",
      "Target 0.0000000000\n",
      "Loss 0.0000480743\n",
      "---------------------------\n",
      "Outputs -0.0028570816\n",
      "Target 0.0000000000\n",
      "Loss 0.0000081629\n",
      "---------------------------\n",
      "Outputs -0.0500848256\n",
      "Target 0.0000000000\n",
      "Loss 0.0025084896\n",
      "---------------------------\n",
      "Outputs -0.0311797000\n",
      "Target 0.0000000000\n",
      "Loss 0.0009721737\n",
      "---------------------------\n",
      "Outputs -0.0348482467\n",
      "Target 0.0000000000\n",
      "Loss 0.0012144003\n",
      "---------------------------\n",
      "Outputs -0.0670784414\n",
      "Target 0.0000000000\n",
      "Loss 0.0044995174\n",
      "---------------------------\n",
      "Outputs -0.1080132425\n",
      "Target 0.0000000000\n",
      "Loss 0.0116668604\n",
      "---------------------------\n",
      "Outputs -0.7102178931\n",
      "Target 0.0000000000\n",
      "Loss 0.5044094324\n",
      "---------------------------\n",
      "Outputs -0.0532378890\n",
      "Target 0.0000000000\n",
      "Loss 0.0028342728\n",
      "---------------------------\n",
      "Outputs -0.1100260615\n",
      "Target 0.0000000000\n",
      "Loss 0.0121057341\n",
      "---------------------------\n",
      "Outputs -0.0822601467\n",
      "Target 0.0000000000\n",
      "Loss 0.0067667319\n",
      "---------------------------\n",
      "Outputs -0.0470189117\n",
      "Target 0.0000000000\n",
      "Loss 0.0022107780\n",
      "---------------------------\n",
      "Outputs -0.1306462139\n",
      "Target 0.0000000000\n",
      "Loss 0.0170684326\n",
      "---------------------------\n",
      "Outputs 0.0109211542\n",
      "Target 0.0000000000\n",
      "Loss 0.0001192716\n",
      "---------------------------\n",
      "Outputs -0.0521493889\n",
      "Target 0.0000000000\n",
      "Loss 0.0027195588\n",
      "---------------------------\n",
      "Outputs -0.1097935587\n",
      "Target 0.0000000000\n",
      "Loss 0.0120546259\n",
      "---------------------------\n",
      "Outputs -0.0042031109\n",
      "Target 0.0000000000\n",
      "Loss 0.0000176661\n",
      "---------------------------\n",
      "Outputs -0.2132475227\n",
      "Target 0.0000000000\n",
      "Loss 0.0454745069\n",
      "---------------------------\n",
      "Outputs -0.0334763639\n",
      "Target 0.0000000000\n",
      "Loss 0.0011206670\n",
      "---------------------------\n",
      "Outputs -0.1168945283\n",
      "Target 0.0000000000\n",
      "Loss 0.0136643304\n",
      "---------------------------\n",
      "Outputs -0.0824831128\n",
      "Target 0.0000000000\n",
      "Loss 0.0068034637\n",
      "---------------------------\n",
      "Outputs -0.0555767901\n",
      "Target 0.0000000000\n",
      "Loss 0.0030887795\n",
      "---------------------------\n",
      "Outputs -0.0271542333\n",
      "Target 0.0000000000\n",
      "Loss 0.0007373524\n",
      "---------------------------\n",
      "Outputs -0.0550096072\n",
      "Target 0.0000000000\n",
      "Loss 0.0030260568\n",
      "---------------------------\n",
      "Outputs -0.2189659029\n",
      "Target 0.0000000000\n",
      "Loss 0.0479460657\n",
      "---------------------------\n",
      "Outputs -0.0285819210\n",
      "Target 0.0000000000\n",
      "Loss 0.0008169262\n",
      "---------------------------\n",
      "Outputs -0.0691228807\n",
      "Target 0.0000000000\n",
      "Loss 0.0047779726\n",
      "---------------------------\n",
      "Outputs -0.0882756412\n",
      "Target 0.0000000000\n",
      "Loss 0.0077925888\n",
      "---------------------------\n",
      "Outputs -0.0194743536\n",
      "Target 0.0000000000\n",
      "Loss 0.0003792504\n",
      "---------------------------\n",
      "Outputs -0.0316743068\n",
      "Target 0.0000000000\n",
      "Loss 0.0010032617\n",
      "---------------------------\n",
      "Outputs -0.0367542915\n",
      "Target 0.0000000000\n",
      "Loss 0.0013508779\n",
      "---------------------------\n",
      "Outputs -0.0175349899\n",
      "Target 0.0000000000\n",
      "Loss 0.0003074759\n",
      "---------------------------\n",
      "Outputs 0.0231273286\n",
      "Target 0.0000000000\n",
      "Loss 0.0005348733\n",
      "---------------------------\n",
      "Outputs -0.1243710518\n",
      "Target 0.0000000000\n",
      "Loss 0.0154681588\n",
      "---------------------------\n",
      "Outputs -0.0625087023\n",
      "Target 0.0000000000\n",
      "Loss 0.0039073378\n",
      "---------------------------\n",
      "Outputs -0.0640200675\n",
      "Target 0.0000000000\n",
      "Loss 0.0040985690\n",
      "---------------------------\n",
      "Outputs 0.0004720613\n",
      "Target 0.0000000000\n",
      "Loss 0.0000002228\n",
      "---------------------------\n",
      "Outputs -0.0809034258\n",
      "Target 0.0000000000\n",
      "Loss 0.0065453644\n",
      "---------------------------\n",
      "Outputs -0.0481213070\n",
      "Target 0.0000000000\n",
      "Loss 0.0023156602\n",
      "---------------------------\n",
      "Outputs -0.0624226369\n",
      "Target 0.0000000000\n",
      "Loss 0.0038965857\n",
      "---------------------------\n",
      "Outputs -0.0374055840\n",
      "Target 0.0000000000\n",
      "Loss 0.0013991777\n",
      "---------------------------\n",
      "Outputs -0.1153378934\n",
      "Target 0.0000000000\n",
      "Loss 0.0133028300\n",
      "---------------------------\n",
      "Outputs -0.0483444668\n",
      "Target 0.0000000000\n",
      "Loss 0.0023371875\n",
      "---------------------------\n",
      "Outputs -0.0686581880\n",
      "Target 0.0000000000\n",
      "Loss 0.0047139470\n",
      "---------------------------\n",
      "Outputs -0.0607764013\n",
      "Target 0.0000000000\n",
      "Loss 0.0036937709\n",
      "---------------------------\n",
      "Outputs -0.0371449552\n",
      "Target 0.0000000000\n",
      "Loss 0.0013797476\n",
      "---------------------------\n",
      "Outputs 0.0008242801\n",
      "Target 0.0000000000\n",
      "Loss 0.0000006794\n",
      "---------------------------\n",
      "Outputs -0.0515486784\n",
      "Target 0.0000000000\n",
      "Loss 0.0026572663\n",
      "---------------------------\n",
      "Outputs -0.0178108923\n",
      "Target 0.0000000000\n",
      "Loss 0.0003172279\n",
      "---------------------------\n",
      "Outputs -0.0156348012\n",
      "Target 0.0000000000\n",
      "Loss 0.0002444470\n",
      "---------------------------\n",
      "Outputs -0.0265092216\n",
      "Target 0.0000000000\n",
      "Loss 0.0007027388\n",
      "---------------------------\n",
      "Outputs -0.0534056984\n",
      "Target 0.0000000000\n",
      "Loss 0.0028521686\n",
      "---------------------------\n",
      "Outputs -0.1320949793\n",
      "Target 0.0000000000\n",
      "Loss 0.0174490828\n",
      "---------------------------\n",
      "Outputs -0.0666314960\n",
      "Target 0.0000000000\n",
      "Loss 0.0044397563\n",
      "---------------------------\n",
      "Outputs -0.2857022583\n",
      "Target 0.0000000000\n",
      "Loss 0.0816257820\n",
      "---------------------------\n",
      "Outputs -0.0010922588\n",
      "Target 0.0000000000\n",
      "Loss 0.0000011930\n",
      "---------------------------\n",
      "Outputs -0.0790715665\n",
      "Target 0.0000000000\n",
      "Loss 0.0062523126\n",
      "---------------------------\n",
      "Outputs -0.0744618922\n",
      "Target 0.0000000000\n",
      "Loss 0.0055445735\n",
      "---------------------------\n",
      "Outputs 0.0022447109\n",
      "Target 0.0000000000\n",
      "Loss 0.0000050387\n",
      "---------------------------\n",
      "Outputs -0.0242275037\n",
      "Target 0.0000000000\n",
      "Loss 0.0005869719\n",
      "---------------------------\n",
      "Outputs -0.0246453099\n",
      "Target 0.0000000000\n",
      "Loss 0.0006073913\n",
      "---------------------------\n",
      "Outputs -0.0721115023\n",
      "Target 0.0000000000\n",
      "Loss 0.0052000689\n",
      "---------------------------\n",
      "Outputs -0.0800786465\n",
      "Target 0.0000000000\n",
      "Loss 0.0064125895\n",
      "---------------------------\n",
      "Outputs -0.0555970781\n",
      "Target 0.0000000000\n",
      "Loss 0.0030910352\n",
      "---------------------------\n",
      "Outputs 0.0125319995\n",
      "Target 0.0000000000\n",
      "Loss 0.0001570510\n",
      "---------------------------\n",
      "Outputs -0.0428168289\n",
      "Target 0.0000000000\n",
      "Loss 0.0018332809\n",
      "---------------------------\n",
      "Outputs -0.0391698368\n",
      "Target 0.0000000000\n",
      "Loss 0.0015342761\n",
      "---------------------------\n",
      "Outputs -0.1020097584\n",
      "Target 0.0000000000\n",
      "Loss 0.0104059912\n",
      "---------------------------\n",
      "Outputs 0.0229619332\n",
      "Target 0.0000000000\n",
      "Loss 0.0005272503\n",
      "---------------------------\n",
      "Outputs -0.0383550264\n",
      "Target 0.0000000000\n",
      "Loss 0.0014711081\n",
      "---------------------------\n",
      "Outputs -0.0429057963\n",
      "Target 0.0000000000\n",
      "Loss 0.0018409074\n",
      "---------------------------\n",
      "Outputs -0.0376393758\n",
      "Target 0.0000000000\n",
      "Loss 0.0014167227\n",
      "---------------------------\n",
      "Outputs -0.0546514057\n",
      "Target 0.0000000000\n",
      "Loss 0.0029867762\n",
      "---------------------------\n",
      "Outputs -0.0453284122\n",
      "Target 0.0000000000\n",
      "Loss 0.0020546650\n",
      "---------------------------\n",
      "Outputs 0.0191149451\n",
      "Target 0.0000000000\n",
      "Loss 0.0003653811\n",
      "---------------------------\n",
      "Outputs -0.0190560482\n",
      "Target 0.0000000000\n",
      "Loss 0.0003631330\n",
      "---------------------------\n",
      "Outputs 0.0421288200\n",
      "Target 0.0000000000\n",
      "Loss 0.0017748375\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs -0.0930417776\n",
      "Target 0.0000000000\n",
      "Loss 0.0086567728\n",
      "---------------------------\n",
      "Outputs -0.1446899474\n",
      "Target 0.0000000000\n",
      "Loss 0.0209351815\n",
      "---------------------------\n",
      "Outputs 0.0074344538\n",
      "Target 0.0000000000\n",
      "Loss 0.0000552711\n",
      "---------------------------\n",
      "Outputs -0.0446694084\n",
      "Target 0.0000000000\n",
      "Loss 0.0019953561\n",
      "---------------------------\n",
      "Outputs -0.0801590681\n",
      "Target 0.0000000000\n",
      "Loss 0.0064254762\n",
      "---------------------------\n",
      "Outputs -0.0262369029\n",
      "Target 0.0000000000\n",
      "Loss 0.0006883751\n",
      "---------------------------\n",
      "Outputs -0.0585258715\n",
      "Target 0.0000000000\n",
      "Loss 0.0034252775\n",
      "---------------------------\n",
      "Outputs -0.1652903557\n",
      "Target 0.0000000000\n",
      "Loss 0.0273209009\n",
      "---------------------------\n",
      "Outputs -0.0519448407\n",
      "Target 0.0000000000\n",
      "Loss 0.0026982664\n",
      "---------------------------\n",
      "Outputs -0.0728551596\n",
      "Target 0.0000000000\n",
      "Loss 0.0053078742\n",
      "---------------------------\n",
      "Outputs -0.0479389466\n",
      "Target 0.0000000000\n",
      "Loss 0.0022981425\n",
      "---------------------------\n",
      "Outputs -0.0241548680\n",
      "Target 0.0000000000\n",
      "Loss 0.0005834576\n",
      "---------------------------\n",
      "Outputs 0.0385341533\n",
      "Target 0.0000000000\n",
      "Loss 0.0014848809\n",
      "---------------------------\n",
      "Outputs -0.1617063433\n",
      "Target 0.0000000000\n",
      "Loss 0.0261489414\n",
      "---------------------------\n",
      "Outputs -0.0137027726\n",
      "Target 0.0000000000\n",
      "Loss 0.0001877660\n",
      "---------------------------\n",
      "Outputs -0.1379161626\n",
      "Target 0.0000000000\n",
      "Loss 0.0190208685\n",
      "---------------------------\n",
      "Outputs -0.0289223604\n",
      "Target 0.0000000000\n",
      "Loss 0.0008365029\n",
      "---------------------------\n",
      "Outputs -0.0357291065\n",
      "Target 0.0000000000\n",
      "Loss 0.0012765691\n",
      "---------------------------\n",
      "Outputs -0.1195754260\n",
      "Target 0.0000000000\n",
      "Loss 0.0142982826\n",
      "---------------------------\n",
      "Outputs -0.0435634740\n",
      "Target 0.0000000000\n",
      "Loss 0.0018977763\n",
      "---------------------------\n",
      "Outputs -0.0467954911\n",
      "Target 0.0000000000\n",
      "Loss 0.0021898181\n",
      "---------------------------\n",
      "Outputs -0.0925576538\n",
      "Target 0.0000000000\n",
      "Loss 0.0085669197\n",
      "---------------------------\n",
      "Outputs -0.0833047926\n",
      "Target 0.0000000000\n",
      "Loss 0.0069396887\n",
      "---------------------------\n",
      "Outputs -0.1333492398\n",
      "Target 0.0000000000\n",
      "Loss 0.0177820195\n",
      "---------------------------\n",
      "Outputs -0.0139263831\n",
      "Target 0.0000000000\n",
      "Loss 0.0001939441\n",
      "---------------------------\n",
      "Outputs -0.0095895939\n",
      "Target 0.0000000000\n",
      "Loss 0.0000919603\n",
      "---------------------------\n",
      "Outputs -0.0531538390\n",
      "Target 0.0000000000\n",
      "Loss 0.0028253307\n",
      "---------------------------\n",
      "Outputs 0.0236947276\n",
      "Target 0.0000000000\n",
      "Loss 0.0005614401\n",
      "---------------------------\n",
      "Outputs -0.1147386432\n",
      "Target 0.0000000000\n",
      "Loss 0.0131649561\n",
      "---------------------------\n",
      "Outputs -0.0489259772\n",
      "Target 0.0000000000\n",
      "Loss 0.0023937512\n",
      "---------------------------\n",
      "Outputs 0.0316864066\n",
      "Target 0.0000000000\n",
      "Loss 0.0010040284\n",
      "---------------------------\n",
      "Outputs 0.0570425503\n",
      "Target 0.0000000000\n",
      "Loss 0.0032538525\n",
      "---------------------------\n",
      "Outputs -0.0447162651\n",
      "Target 0.0000000000\n",
      "Loss 0.0019995444\n",
      "---------------------------\n",
      "Outputs -0.1106550545\n",
      "Target 0.0000000000\n",
      "Loss 0.0122445412\n",
      "---------------------------\n",
      "Outputs -0.0212587155\n",
      "Target 0.0000000000\n",
      "Loss 0.0004519330\n",
      "---------------------------\n",
      "Outputs -0.0755224228\n",
      "Target 0.0000000000\n",
      "Loss 0.0057036364\n",
      "---------------------------\n",
      "Outputs -0.0019884892\n",
      "Target 0.0000000000\n",
      "Loss 0.0000039541\n",
      "---------------------------\n",
      "Outputs -0.0119298063\n",
      "Target 0.0000000000\n",
      "Loss 0.0001423203\n",
      "---------------------------\n",
      "Outputs -0.0216813870\n",
      "Target 0.0000000000\n",
      "Loss 0.0004700825\n",
      "---------------------------\n",
      "Outputs -0.1230357587\n",
      "Target 0.0000000000\n",
      "Loss 0.0151377982\n",
      "---------------------------\n",
      "Outputs -0.0757367015\n",
      "Target 0.0000000000\n",
      "Loss 0.0057360479\n",
      "---------------------------\n",
      "Outputs -0.0545700006\n",
      "Target 0.0000000000\n",
      "Loss 0.0029778851\n",
      "---------------------------\n",
      "Outputs -0.0197569840\n",
      "Target 0.0000000000\n",
      "Loss 0.0003903384\n",
      "---------------------------\n",
      "Outputs 0.0299129747\n",
      "Target 0.0000000000\n",
      "Loss 0.0008947860\n",
      "---------------------------\n",
      "Outputs -0.0777278543\n",
      "Target 0.0000000000\n",
      "Loss 0.0060416195\n",
      "---------------------------\n",
      "Outputs 0.0104963370\n",
      "Target 0.0000000000\n",
      "Loss 0.0001101731\n",
      "---------------------------\n",
      "Outputs 0.0069377683\n",
      "Target 0.0000000000\n",
      "Loss 0.0000481326\n",
      "---------------------------\n",
      "Outputs -0.0317533053\n",
      "Target 0.0000000000\n",
      "Loss 0.0010082724\n",
      "---------------------------\n",
      "Outputs -0.2109866142\n",
      "Target 0.0000000000\n",
      "Loss 0.0445153527\n",
      "---------------------------\n",
      "Outputs -0.0365394466\n",
      "Target 0.0000000000\n",
      "Loss 0.0013351311\n",
      "---------------------------\n",
      "Outputs -0.0657294244\n",
      "Target 0.0000000000\n",
      "Loss 0.0043203570\n",
      "---------------------------\n",
      "Outputs -0.0517954268\n",
      "Target 0.0000000000\n",
      "Loss 0.0026827662\n",
      "---------------------------\n",
      "Outputs -0.1256482601\n",
      "Target 0.0000000000\n",
      "Loss 0.0157874860\n",
      "---------------------------\n",
      "Outputs -0.0230597071\n",
      "Target 0.0000000000\n",
      "Loss 0.0005317501\n",
      "---------------------------\n",
      "Outputs 0.0347746871\n",
      "Target 0.0000000000\n",
      "Loss 0.0012092788\n",
      "---------------------------\n",
      "Outputs -0.0989655107\n",
      "Target 0.0000000000\n",
      "Loss 0.0097941719\n",
      "---------------------------\n",
      "Outputs -0.0034772418\n",
      "Target 0.0000000000\n",
      "Loss 0.0000120912\n",
      "---------------------------\n",
      "Outputs -0.2210021466\n",
      "Target 0.0000000000\n",
      "Loss 0.0488419496\n",
      "---------------------------\n",
      "Outputs -0.1391181946\n",
      "Target 0.0000000000\n",
      "Loss 0.0193538722\n",
      "---------------------------\n",
      "Outputs 0.0004008748\n",
      "Target 0.0000000000\n",
      "Loss 0.0000001607\n",
      "---------------------------\n",
      "Outputs 0.0250261314\n",
      "Target 0.0000000000\n",
      "Loss 0.0006263073\n",
      "---------------------------\n",
      "Outputs -0.0032761954\n",
      "Target 0.0000000000\n",
      "Loss 0.0000107335\n",
      "---------------------------\n",
      "Outputs -0.0719481707\n",
      "Target 0.0000000000\n",
      "Loss 0.0051765391\n",
      "---------------------------\n",
      "Outputs -0.0366326012\n",
      "Target 0.0000000000\n",
      "Loss 0.0013419475\n",
      "---------------------------\n",
      "Outputs -0.0253337808\n",
      "Target 0.0000000000\n",
      "Loss 0.0006418005\n",
      "---------------------------\n",
      "Outputs -0.1009280235\n",
      "Target 0.0000000000\n",
      "Loss 0.0101864655\n",
      "---------------------------\n",
      "Outputs -0.0484858789\n",
      "Target 0.0000000000\n",
      "Loss 0.0023508805\n",
      "---------------------------\n",
      "Outputs -0.1774605066\n",
      "Target 0.0000000000\n",
      "Loss 0.0314922296\n",
      "---------------------------\n",
      "Outputs -0.0436504073\n",
      "Target 0.0000000000\n",
      "Loss 0.0019053580\n",
      "---------------------------\n",
      "Outputs -0.1616445780\n",
      "Target 0.0000000000\n",
      "Loss 0.0261289701\n",
      "---------------------------\n",
      "Outputs -0.0732591152\n",
      "Target 0.0000000000\n",
      "Loss 0.0053668981\n",
      "---------------------------\n",
      "Outputs -0.1078950912\n",
      "Target 0.0000000000\n",
      "Loss 0.0116413506\n",
      "---------------------------\n",
      "Outputs -0.0347480886\n",
      "Target 0.0000000000\n",
      "Loss 0.0012074297\n",
      "---------------------------\n",
      "Outputs -0.0213764198\n",
      "Target 0.0000000000\n",
      "Loss 0.0004569513\n",
      "---------------------------\n",
      "Outputs 0.0049739219\n",
      "Target 0.0000000000\n",
      "Loss 0.0000247399\n",
      "---------------------------\n",
      "Outputs -0.0547811501\n",
      "Target 0.0000000000\n",
      "Loss 0.0030009744\n",
      "---------------------------\n",
      "Outputs -0.2787609994\n",
      "Target 0.0000000000\n",
      "Loss 0.0777076930\n",
      "---------------------------\n",
      "Outputs -0.0023231544\n",
      "Target 0.0000000000\n",
      "Loss 0.0000053970\n",
      "---------------------------\n",
      "Outputs -0.0471040718\n",
      "Target 0.0000000000\n",
      "Loss 0.0022187936\n",
      "---------------------------\n",
      "Outputs -0.0338320099\n",
      "Target 0.0000000000\n",
      "Loss 0.0011446049\n",
      "---------------------------\n",
      "Outputs -0.0239444710\n",
      "Target 0.0000000000\n",
      "Loss 0.0005733377\n",
      "---------------------------\n",
      "Outputs -0.1687382758\n",
      "Target 0.0000000000\n",
      "Loss 0.0284726061\n",
      "---------------------------\n",
      "Outputs -0.0526051559\n",
      "Target 0.0000000000\n",
      "Loss 0.0027673023\n",
      "---------------------------\n",
      "Outputs 0.0377081297\n",
      "Target 0.0000000000\n",
      "Loss 0.0014219030\n",
      "---------------------------\n",
      "Outputs 0.0116029344\n",
      "Target 0.0000000000\n",
      "Loss 0.0001346281\n",
      "---------------------------\n",
      "Outputs -0.0134817772\n",
      "Target 0.0000000000\n",
      "Loss 0.0001817583\n",
      "---------------------------\n",
      "Outputs -0.0505056120\n",
      "Target 0.0000000000\n",
      "Loss 0.0025508169\n",
      "---------------------------\n",
      "Outputs -0.0735686421\n",
      "Target 0.0000000000\n",
      "Loss 0.0054123453\n",
      "---------------------------\n",
      "Outputs -0.0086640380\n",
      "Target 0.0000000000\n",
      "Loss 0.0000750656\n",
      "---------------------------\n",
      "Outputs -0.0346025564\n",
      "Target 0.0000000000\n",
      "Loss 0.0011973369\n",
      "---------------------------\n",
      "Outputs -0.0492071174\n",
      "Target 0.0000000000\n",
      "Loss 0.0024213404\n",
      "---------------------------\n",
      "Outputs -0.0604927726\n",
      "Target 0.0000000000\n",
      "Loss 0.0036593755\n",
      "---------------------------\n",
      "Outputs -0.0098426826\n",
      "Target 0.0000000000\n",
      "Loss 0.0000968784\n",
      "---------------------------\n",
      "Outputs -0.3204602301\n",
      "Target 0.0000000000\n",
      "Loss 0.1026947573\n",
      "---------------------------\n",
      "Outputs 0.0012870319\n",
      "Target 0.0000000000\n",
      "Loss 0.0000016565\n",
      "---------------------------\n",
      "Outputs -0.2141616046\n",
      "Target 0.0000000000\n",
      "Loss 0.0458651930\n",
      "---------------------------\n",
      "Outputs -0.0706852823\n",
      "Target 0.0000000000\n",
      "Loss 0.0049964092\n",
      "---------------------------\n",
      "Outputs 0.0029530860\n",
      "Target 0.0000000000\n",
      "Loss 0.0000087207\n",
      "---------------------------\n",
      "Outputs -0.0490307324\n",
      "Target 0.0000000000\n",
      "Loss 0.0024040127\n",
      "---------------------------\n",
      "Outputs -0.0753847957\n",
      "Target 0.0000000000\n",
      "Loss 0.0056828675\n",
      "---------------------------\n",
      "Outputs -0.0669630766\n",
      "Target 0.0000000000\n",
      "Loss 0.0044840537\n",
      "---------------------------\n",
      "Outputs -0.0440310948\n",
      "Target 0.0000000000\n",
      "Loss 0.0019387373\n",
      "---------------------------\n",
      "Outputs -0.0305468924\n",
      "Target 0.0000000000\n",
      "Loss 0.0009331126\n",
      "---------------------------\n",
      "Outputs -0.0631809682\n",
      "Target 0.0000000000\n",
      "Loss 0.0039918348\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs -0.1521320790\n",
      "Target 0.0000000000\n",
      "Loss 0.0231441688\n",
      "---------------------------\n",
      "Outputs 0.0275497846\n",
      "Target 0.0000000000\n",
      "Loss 0.0007589906\n",
      "---------------------------\n",
      "Outputs -0.0543574728\n",
      "Target 0.0000000000\n",
      "Loss 0.0029547350\n",
      "---------------------------\n",
      "Outputs -0.1524166316\n",
      "Target 0.0000000000\n",
      "Loss 0.0232308302\n",
      "---------------------------\n",
      "Outputs 0.0160601176\n",
      "Target 0.0000000000\n",
      "Loss 0.0002579274\n",
      "---------------------------\n",
      "Outputs -0.0865958482\n",
      "Target 0.0000000000\n",
      "Loss 0.0074988408\n",
      "---------------------------\n",
      "Outputs -0.0589680262\n",
      "Target 0.0000000000\n",
      "Loss 0.0034772281\n",
      "---------------------------\n",
      "Outputs -0.0439660810\n",
      "Target 0.0000000000\n",
      "Loss 0.0019330162\n",
      "---------------------------\n",
      "Outputs -0.0947232544\n",
      "Target 0.0000000000\n",
      "Loss 0.0089724949\n",
      "---------------------------\n",
      "Outputs -0.0691557527\n",
      "Target 0.0000000000\n",
      "Loss 0.0047825179\n",
      "---------------------------\n",
      "Outputs -0.0411849059\n",
      "Target 0.0000000000\n",
      "Loss 0.0016961965\n",
      "---------------------------\n",
      "Outputs -0.1158810705\n",
      "Target 0.0000000000\n",
      "Loss 0.0134284226\n",
      "---------------------------\n",
      "Outputs -0.0876840353\n",
      "Target 0.0000000000\n",
      "Loss 0.0076884902\n",
      "---------------------------\n",
      "Outputs -0.0052523799\n",
      "Target 0.0000000000\n",
      "Loss 0.0000275875\n",
      "---------------------------\n",
      "Outputs 0.0282548368\n",
      "Target 0.0000000000\n",
      "Loss 0.0007983358\n",
      "---------------------------\n",
      "Outputs -0.2114961445\n",
      "Target 0.0000000000\n",
      "Loss 0.0447306186\n",
      "---------------------------\n",
      "Outputs -0.1356228441\n",
      "Target 0.0000000000\n",
      "Loss 0.0183935557\n",
      "---------------------------\n",
      "Outputs -0.0217572041\n",
      "Target 0.0000000000\n",
      "Loss 0.0004733759\n",
      "---------------------------\n",
      "Outputs -0.0235024728\n",
      "Target 0.0000000000\n",
      "Loss 0.0005523663\n",
      "---------------------------\n",
      "Outputs -0.0689537823\n",
      "Target 0.0000000000\n",
      "Loss 0.0047546239\n",
      "---------------------------\n",
      "Outputs -0.0462295972\n",
      "Target 0.0000000000\n",
      "Loss 0.0021371758\n",
      "---------------------------\n",
      "Outputs -0.0773753971\n",
      "Target 0.0000000000\n",
      "Loss 0.0059869522\n",
      "---------------------------\n",
      "Outputs -0.1038504541\n",
      "Target 0.0000000000\n",
      "Loss 0.0107849166\n",
      "---------------------------\n",
      "Outputs -0.1342583448\n",
      "Target 0.0000000000\n",
      "Loss 0.0180253033\n",
      "---------------------------\n",
      "Outputs -0.0257500373\n",
      "Target 0.0000000000\n",
      "Loss 0.0006630644\n",
      "---------------------------\n",
      "Outputs -0.0124002360\n",
      "Target 0.0000000000\n",
      "Loss 0.0001537659\n",
      "---------------------------\n",
      "Outputs -0.0351634957\n",
      "Target 0.0000000000\n",
      "Loss 0.0012364715\n",
      "---------------------------\n",
      "Outputs -0.0998585224\n",
      "Target 0.0000000000\n",
      "Loss 0.0099717248\n",
      "---------------------------\n",
      "Outputs -0.0498211570\n",
      "Target 0.0000000000\n",
      "Loss 0.0024821477\n",
      "---------------------------\n",
      "Outputs -0.0088428743\n",
      "Target 0.0000000000\n",
      "Loss 0.0000781964\n",
      "---------------------------\n",
      "Outputs -0.0531538390\n",
      "Target 0.0000000000\n",
      "Loss 0.0028253307\n",
      "---------------------------\n",
      "Outputs -0.0471081547\n",
      "Target 0.0000000000\n",
      "Loss 0.0022191782\n",
      "---------------------------\n",
      "Outputs -0.0180446096\n",
      "Target 0.0000000000\n",
      "Loss 0.0003256079\n",
      "---------------------------\n",
      "Outputs -0.0254005827\n",
      "Target 0.0000000000\n",
      "Loss 0.0006451896\n",
      "---------------------------\n",
      "Outputs -0.0381316580\n",
      "Target 0.0000000000\n",
      "Loss 0.0014540233\n",
      "---------------------------\n",
      "Outputs -0.0413918011\n",
      "Target 0.0000000000\n",
      "Loss 0.0017132811\n",
      "---------------------------\n",
      "Outputs -0.0047285818\n",
      "Target 0.0000000000\n",
      "Loss 0.0000223595\n",
      "---------------------------\n",
      "Outputs -0.0549415983\n",
      "Target 0.0000000000\n",
      "Loss 0.0030185792\n",
      "---------------------------\n",
      "Outputs -0.0386006795\n",
      "Target 0.0000000000\n",
      "Loss 0.0014900125\n",
      "---------------------------\n",
      "Outputs -0.1415390521\n",
      "Target 0.0000000000\n",
      "Loss 0.0200333036\n",
      "---------------------------\n",
      "Outputs -0.0677733570\n",
      "Target 0.0000000000\n",
      "Loss 0.0045932280\n",
      "---------------------------\n",
      "Outputs -0.1077812910\n",
      "Target 0.0000000000\n",
      "Loss 0.0116168065\n",
      "---------------------------\n",
      "Outputs 0.0081547908\n",
      "Target 0.0000000000\n",
      "Loss 0.0000665006\n",
      "---------------------------\n",
      "Outputs -0.0393822528\n",
      "Target 0.0000000000\n",
      "Loss 0.0015509619\n",
      "---------------------------\n",
      "Outputs -0.0271542333\n",
      "Target 0.0000000000\n",
      "Loss 0.0007373524\n",
      "---------------------------\n",
      "Outputs -0.0706928223\n",
      "Target 0.0000000000\n",
      "Loss 0.0049974751\n",
      "---------------------------\n",
      "Outputs -0.0440653153\n",
      "Target 0.0000000000\n",
      "Loss 0.0019417520\n",
      "---------------------------\n",
      "Outputs -0.0577533357\n",
      "Target 0.0000000000\n",
      "Loss 0.0033354477\n",
      "---------------------------\n",
      "Outputs -0.0168889649\n",
      "Target 0.0000000000\n",
      "Loss 0.0002852371\n",
      "---------------------------\n",
      "Outputs -0.0447462201\n",
      "Target 0.0000000000\n",
      "Loss 0.0020022243\n",
      "---------------------------\n",
      "Outputs -0.0822689980\n",
      "Target 0.0000000000\n",
      "Loss 0.0067681880\n",
      "---------------------------\n",
      "Outputs -0.0866279602\n",
      "Target 0.0000000000\n",
      "Loss 0.0075044036\n",
      "---------------------------\n",
      "Outputs -0.0352012254\n",
      "Target 0.0000000000\n",
      "Loss 0.0012391263\n",
      "---------------------------\n",
      "Outputs -0.0527185313\n",
      "Target 0.0000000000\n",
      "Loss 0.0027792435\n",
      "---------------------------\n",
      "Outputs -0.0743350834\n",
      "Target 0.0000000000\n",
      "Loss 0.0055257045\n",
      "---------------------------\n",
      "Outputs -0.0669631809\n",
      "Target 0.0000000000\n",
      "Loss 0.0044840677\n",
      "---------------------------\n",
      "Outputs -0.1453181952\n",
      "Target 0.0000000000\n",
      "Loss 0.0211173780\n",
      "---------------------------\n",
      "Outputs -0.1859284788\n",
      "Target 0.0000000000\n",
      "Loss 0.0345693976\n",
      "---------------------------\n",
      "Outputs -0.0170296244\n",
      "Target 0.0000000000\n",
      "Loss 0.0002900081\n",
      "---------------------------\n",
      "Outputs -0.0434394516\n",
      "Target 0.0000000000\n",
      "Loss 0.0018869860\n",
      "---------------------------\n",
      "Outputs -0.0459746979\n",
      "Target 0.0000000000\n",
      "Loss 0.0021136729\n",
      "---------------------------\n",
      "Outputs -0.0560051091\n",
      "Target 0.0000000000\n",
      "Loss 0.0031365722\n",
      "---------------------------\n",
      "Outputs -0.1380851716\n",
      "Target 0.0000000000\n",
      "Loss 0.0190675147\n",
      "---------------------------\n",
      "Outputs -0.0486485995\n",
      "Target 0.0000000000\n",
      "Loss 0.0023666862\n",
      "---------------------------\n",
      "Outputs -0.2057708055\n",
      "Target 0.0000000000\n",
      "Loss 0.0423416235\n",
      "---------------------------\n",
      "Outputs 0.0323020555\n",
      "Target 0.0000000000\n",
      "Loss 0.0010434228\n",
      "---------------------------\n",
      "Outputs -0.0659717768\n",
      "Target 0.0000000000\n",
      "Loss 0.0043522753\n",
      "---------------------------\n",
      "Outputs -0.0029534288\n",
      "Target 0.0000000000\n",
      "Loss 0.0000087227\n",
      "---------------------------\n",
      "Outputs -0.0027051158\n",
      "Target 0.0000000000\n",
      "Loss 0.0000073177\n",
      "---------------------------\n",
      "Outputs -0.0853775591\n",
      "Target 0.0000000000\n",
      "Loss 0.0072893277\n",
      "---------------------------\n",
      "Outputs -0.1151860058\n",
      "Target 0.0000000000\n",
      "Loss 0.0132678160\n",
      "---------------------------\n",
      "Outputs -0.0771425813\n",
      "Target 0.0000000000\n",
      "Loss 0.0059509780\n",
      "---------------------------\n",
      "Outputs -0.0752112418\n",
      "Target 0.0000000000\n",
      "Loss 0.0056567308\n",
      "---------------------------\n",
      "Outputs -0.1132248491\n",
      "Target 0.0000000000\n",
      "Loss 0.0128198666\n",
      "---------------------------\n",
      "Outputs -0.0450686477\n",
      "Target 0.0000000000\n",
      "Loss 0.0020311831\n",
      "---------------------------\n",
      "Outputs 0.0111867450\n",
      "Target 0.0000000000\n",
      "Loss 0.0001251433\n",
      "---------------------------\n",
      "Outputs -0.0331457965\n",
      "Target 0.0000000000\n",
      "Loss 0.0010986439\n",
      "---------------------------\n",
      "Outputs -0.0375540070\n",
      "Target 0.0000000000\n",
      "Loss 0.0014103034\n",
      "---------------------------\n",
      "Outputs -0.0137824677\n",
      "Target 0.0000000000\n",
      "Loss 0.0001899564\n",
      "---------------------------\n",
      "Outputs -0.0853756666\n",
      "Target 0.0000000000\n",
      "Loss 0.0072890045\n",
      "---------------------------\n",
      "Outputs -0.1342958808\n",
      "Target 0.0000000000\n",
      "Loss 0.0180353839\n",
      "---------------------------\n",
      "Outputs -0.2071089894\n",
      "Target 0.0000000000\n",
      "Loss 0.0428941324\n",
      "---------------------------\n",
      "Outputs -0.0529039539\n",
      "Target 0.0000000000\n",
      "Loss 0.0027988283\n",
      "---------------------------\n",
      "Outputs -0.0455019996\n",
      "Target 0.0000000000\n",
      "Loss 0.0020704321\n",
      "---------------------------\n",
      "Outputs -0.2155429572\n",
      "Target 0.0000000000\n",
      "Loss 0.0464587659\n",
      "---------------------------\n",
      "Outputs -0.0406027324\n",
      "Target 0.0000000000\n",
      "Loss 0.0016485819\n",
      "---------------------------\n",
      "Outputs -0.0380797796\n",
      "Target 0.0000000000\n",
      "Loss 0.0014500696\n",
      "---------------------------\n",
      "Outputs -0.0834023356\n",
      "Target 0.0000000000\n",
      "Loss 0.0069559496\n",
      "---------------------------\n",
      "Outputs -0.0529511534\n",
      "Target 0.0000000000\n",
      "Loss 0.0028038246\n",
      "---------------------------\n",
      "Outputs -0.0128147341\n",
      "Target 0.0000000000\n",
      "Loss 0.0001642174\n",
      "---------------------------\n",
      "Outputs -0.0643896610\n",
      "Target 0.0000000000\n",
      "Loss 0.0041460283\n",
      "---------------------------\n",
      "Outputs 0.0154549070\n",
      "Target 0.0000000000\n",
      "Loss 0.0002388541\n",
      "---------------------------\n",
      "Outputs -0.0347269140\n",
      "Target 0.0000000000\n",
      "Loss 0.0012059585\n",
      "---------------------------\n",
      "Outputs -0.0735432655\n",
      "Target 0.0000000000\n",
      "Loss 0.0054086121\n",
      "---------------------------\n",
      "Outputs -0.0359283425\n",
      "Target 0.0000000000\n",
      "Loss 0.0012908458\n",
      "---------------------------\n",
      "Outputs -0.0709637105\n",
      "Target 0.0000000000\n",
      "Loss 0.0050358484\n",
      "---------------------------\n",
      "Outputs 0.0071754418\n",
      "Target 0.0000000000\n",
      "Loss 0.0000514870\n",
      "---------------------------\n",
      "Outputs -0.1087307334\n",
      "Target 0.0000000000\n",
      "Loss 0.0118223727\n",
      "---------------------------\n",
      "Outputs -0.0379583724\n",
      "Target 0.0000000000\n",
      "Loss 0.0014408380\n",
      "---------------------------\n",
      "Outputs -0.0604787655\n",
      "Target 0.0000000000\n",
      "Loss 0.0036576812\n",
      "---------------------------\n",
      "Outputs -0.0288959928\n",
      "Target 0.0000000000\n",
      "Loss 0.0008349784\n",
      "---------------------------\n",
      "Outputs 0.0669635385\n",
      "Target 0.0000000000\n",
      "Loss 0.0044841156\n",
      "---------------------------\n",
      "Outputs -0.0808718503\n",
      "Target 0.0000000000\n",
      "Loss 0.0065402561\n",
      "---------------------------\n",
      "Outputs -0.0482822768\n",
      "Target 0.0000000000\n",
      "Loss 0.0023311782\n",
      "---------------------------\n",
      "Outputs -0.0251149144\n",
      "Target 0.0000000000\n",
      "Loss 0.0006307589\n",
      "---------------------------\n",
      "Outputs -0.0929823369\n",
      "Target 0.0000000000\n",
      "Loss 0.0086457152\n",
      "---------------------------\n",
      "Outputs -0.0226455741\n",
      "Target 0.0000000000\n",
      "Loss 0.0005128220\n",
      "---------------------------\n",
      "Outputs -0.0230994634\n",
      "Target 0.0000000000\n",
      "Loss 0.0005335852\n",
      "---------------------------\n",
      "Outputs -0.0416008942\n",
      "Target 0.0000000000\n",
      "Loss 0.0017306344\n",
      "---------------------------\n",
      "Outputs -0.0342260413\n",
      "Target 0.0000000000\n",
      "Loss 0.0011714220\n",
      "---------------------------\n",
      "Outputs -0.0188395269\n",
      "Target 0.0000000000\n",
      "Loss 0.0003549278\n",
      "---------------------------\n",
      "Outputs -0.0882836729\n",
      "Target 0.0000000000\n",
      "Loss 0.0077940067\n",
      "---------------------------\n",
      "Outputs 0.0099242292\n",
      "Target 0.0000000000\n",
      "Loss 0.0000984903\n",
      "---------------------------\n",
      "Outputs -0.0526051559\n",
      "Target 0.0000000000\n",
      "Loss 0.0027673023\n",
      "---------------------------\n",
      "Outputs -0.0919359475\n",
      "Target 0.0000000000\n",
      "Loss 0.0084522180\n",
      "---------------------------\n",
      "Outputs -0.0842919648\n",
      "Target 0.0000000000\n",
      "Loss 0.0071051354\n",
      "---------------------------\n",
      "Outputs -0.1009280235\n",
      "Target 0.0000000000\n",
      "Loss 0.0101864655\n",
      "---------------------------\n",
      "Outputs -0.0814768821\n",
      "Target 0.0000000000\n",
      "Loss 0.0066384822\n",
      "---------------------------\n",
      "Outputs -0.0770409107\n",
      "Target 0.0000000000\n",
      "Loss 0.0059353020\n",
      "---------------------------\n",
      "Avg. Loss for targets >= -0.01: 0.0079331584\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "outputs_all = []\n",
    "targets_all = []\n",
    "for i,data in enumerate(testloader):\n",
    "    # Get and prepare inputs\n",
    "    inputs, targets = data\n",
    "    # inputs, targets = inputs.float(), targets.float()\n",
    "    targets = targets.reshape((targets.shape[0], 1))\n",
    "      \n",
    "    # Perform forward pass\n",
    "    outputs = mlp(inputs)\n",
    "    outputs_all.append(outputs[0].detach().numpy())\n",
    "    targets_all.append(targets[0].detach().numpy())\n",
    "      \n",
    "    # Compute loss\n",
    "    loss = loss_function(outputs, targets)\n",
    "    if targets >= -0.01:\n",
    "        losses.append(loss.detach().numpy())\n",
    "        print(\"Outputs %.10f\" %outputs)\n",
    "        print(\"Target %.10f\" %targets)\n",
    "        print(\"Loss %.10f\" %loss)\n",
    "        print(\"---------------------------\")\n",
    "    \n",
    "print(\"Avg. Loss for targets >= -0.01: %.10f\" %np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ad524eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(losses, bins=100, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5ae98881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9197503541279318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(r2_score(outputs_all, targets_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ad8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oppe4rl",
   "language": "python",
   "name": "oppe4rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
