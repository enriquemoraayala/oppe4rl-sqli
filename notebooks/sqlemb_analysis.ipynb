{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcaf5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba05b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '../src/outputs/run_history_emb.csv'\n",
    "# csv_path = '/home/azureuser/cloudfiles/code/outputs/run_history.csv'\n",
    "\n",
    "df = pd.read_csv('../src/outputs/run_history.csv', sep=';', index_col=0)\n",
    "df_emb = pd.read_csv('../src/outputs/run_history_emb_idx.csv', sep=';', index_col=0)\n",
    "np_emb_state = np.load('../src/outputs/emb_states.npy', allow_pickle=True)\n",
    "np_emb_next_state = np.load('../src/outputs/next_state_emb.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d3e706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>step</th>\n",
       "      <th>original_payload</th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>next_state</th>\n",
       "      <th>reward</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>select * from users where id = '1'&lt;@.. or 1 =...</td>\n",
       "      <td>select * from users where id = '1'&lt;@.. or 1 =...</td>\n",
       "      <td>22</td>\n",
       "      <td>select * from users where id = '1'&lt;@.. or 1 =...</td>\n",
       "      <td>-0.537037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>select * from users where id = '1'&lt;@.. or 1 =...</td>\n",
       "      <td>select * from users where id = '1'&lt;@.. or 1 =...</td>\n",
       "      <td>19</td>\n",
       "      <td>select * from users where id = '1'&lt;@.. or 1 =...</td>\n",
       "      <td>-0.637037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>select * from users where id = '1'&lt;@.. or 1 =...</td>\n",
       "      <td>select * from users where id = '1'&lt;@.. or 1 =...</td>\n",
       "      <td>15</td>\n",
       "      <td>select * from users where id = '1'&lt;@.. or 1 =...</td>\n",
       "      <td>-0.737037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>select * from users where id = '1'&lt;@.. or 1 =...</td>\n",
       "      <td>select * from users where id = '1'&lt;@.. or 1 =...</td>\n",
       "      <td>8</td>\n",
       "      <td>select * from users where id = '1'&lt;@.. or (SE...</td>\n",
       "      <td>-0.869444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>select * from users where id = '1'&lt;@.. or 1 =...</td>\n",
       "      <td>select * from users where id = '1'&lt;@.. or (SE...</td>\n",
       "      <td>0</td>\n",
       "      <td>/**/select/**/*/**/from/**/users/**/where/**/i...</td>\n",
       "      <td>-0.825000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode  step                                   original_payload  \\\n",
       "0        1     1   select * from users where id = '1'<@.. or 1 =...   \n",
       "1        1     2   select * from users where id = '1'<@.. or 1 =...   \n",
       "2        1     3   select * from users where id = '1'<@.. or 1 =...   \n",
       "3        1     4   select * from users where id = '1'<@.. or 1 =...   \n",
       "4        1     5   select * from users where id = '1'<@.. or 1 =...   \n",
       "\n",
       "                                               state  action  \\\n",
       "0   select * from users where id = '1'<@.. or 1 =...      22   \n",
       "1   select * from users where id = '1'<@.. or 1 =...      19   \n",
       "2   select * from users where id = '1'<@.. or 1 =...      15   \n",
       "3   select * from users where id = '1'<@.. or 1 =...       8   \n",
       "4   select * from users where id = '1'<@.. or (SE...       0   \n",
       "\n",
       "                                          next_state    reward  win  \n",
       "0   select * from users where id = '1'<@.. or 1 =... -0.537037    0  \n",
       "1   select * from users where id = '1'<@.. or 1 =... -0.637037    0  \n",
       "2   select * from users where id = '1'<@.. or 1 =... -0.737037    0  \n",
       "3   select * from users where id = '1'<@.. or (SE... -0.869444    0  \n",
       "4  /**/select/**/*/**/from/**/users/**/where/**/i... -0.825000    0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "851ec739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24908, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95cdff53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24908,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_emb_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17fdf190",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_emb_state_2 = np.array([np.float32(x) for x in np_emb_state])\n",
    "sqli_labels = [x.replace('\\n', '') for x in df['state']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcb49733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_emb_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1582090",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('metadata.txt', sqli_labels, delimiter='\\t', fmt='%s')\n",
    "np.savetxt('features.txt', np_emb_state_2, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7810f9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to get TSNE embedding with 2 dimensions\n",
    "n_components = 2\n",
    "tsne = TSNE(n_components)\n",
    "tsne_result = tsne.fit_transform(np_emb_state_2)\n",
    "tsne_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3215599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result of our TSNE with the label color coded\n",
    "# A lot of the stuff here is about making the plot look pretty and not TSNE\n",
    "tsne_result_df = pd.DataFrame({'tsne_1': tsne_result[:,0], 'tsne_2': tsne_result[:,1]})\n",
    "fig, ax = plt.subplots(1, figsize=(15, 15))\n",
    "\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', data=tsne_result_df, ax=ax,s=20)\n",
    "lim = (tsne_result.min()-5, tsne_result.max()+5)\n",
    "ax.set_xlim(lim)\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9d5f48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 19, 15, 8, 0, 7, 3, 23, 25, 18]\n",
      "tensor([22, 19, 15,  ..., 16, 20,  9])\n"
     ]
    }
   ],
   "source": [
    "actions = [x for x in df['action']]\n",
    "print(actions[0:10])\n",
    "a_t = torch.Tensor(actions).to(torch.int64)\n",
    "print(a_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12ede6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_one_hot = torch.nn.functional.one_hot(a_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7413e51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24908, 26])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actions_one_hot = torch.stack(actions_one_hot)\n",
    "actions_one_hot.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d063dc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24908,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_emb_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbd7e588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24908, 768)\n",
      "torch.Size([24908, 768])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "stacked = np.stack(np_emb_state)\n",
    "print(stacked.shape)\n",
    "\n",
    "t_emb = torch.from_numpy(stacked)\n",
    "print(t_emb.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce478e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24908, 794])\n"
     ]
    }
   ],
   "source": [
    "t_x = torch.cat([t_emb, actions_one_hot], dim=1)\n",
    "print(t_x.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49ed469c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1623, -0.1776,  0.0701,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.1623, -0.1776,  0.0701,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_x[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7cca41f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5370, -0.6370, -0.7370,  ..., -2.8822, -2.9822, -3.0796])\n"
     ]
    }
   ],
   "source": [
    "t_y = df['reward']\n",
    "t_y = [np.float32(x) for x in t_y]\n",
    "t_y = torch.Tensor(t_y).to(torch.float32)\n",
    "print(t_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe531579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLiDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X, y, scale_data=True):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "          # Apply scaling if necessary\n",
    "            if scale_data:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "            self.X = torch.from_numpy(X)\n",
    "            self.y = torch.from_numpy(y)\n",
    "        if torch.is_tensor(X) and torch.is_tensor(y):\n",
    "            if scale_data:\n",
    "                self.X = torch.nn.functional.normalize(X)\n",
    "                self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a5a89ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = SQLiDataset(t_x,t_y) # create your datset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6fbb77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.2931e-02, -1.4143e-02,  5.5855e-03,  1.6344e-02,  1.6026e-03,\n",
       "          1.2289e-02,  3.9901e-03, -5.7610e-03,  8.5404e-04, -4.1324e-03,\n",
       "         -9.7086e-03, -6.5811e-03,  4.5324e-03, -9.9580e-03,  2.2600e-02,\n",
       "         -1.9146e-03, -6.3548e-03, -5.2546e-03,  2.6897e-03,  3.6181e-02,\n",
       "          3.4488e-03,  8.7928e-03,  1.1742e-02,  6.5928e-04, -1.8527e-03,\n",
       "         -3.9596e-03,  1.0660e-02,  8.4048e-03, -5.1416e-03, -2.4669e-04,\n",
       "         -3.5284e-03,  1.1193e-02,  9.1718e-04,  2.5720e-03,  9.0391e-03,\n",
       "         -1.6371e-03,  1.5715e-02, -2.7340e-03, -9.6294e-03,  7.3590e-03,\n",
       "         -1.6091e-02, -1.5547e-02, -1.0492e-03, -8.3873e-05,  6.8688e-03,\n",
       "          9.1807e-04,  5.1240e-03,  2.2012e-02, -3.7152e-03, -3.5218e-03,\n",
       "         -7.7241e-03,  1.4926e-02,  1.4062e-03,  2.3296e-03,  2.7629e-03,\n",
       "         -2.9075e-04, -2.9533e-03, -9.2138e-03,  1.6725e-02,  1.0073e-03,\n",
       "         -1.6200e-03,  4.1777e-02, -2.7387e-02,  2.2494e-03, -6.5844e-03,\n",
       "          2.8995e-03, -6.4408e-03,  1.8248e-03, -5.3735e-04, -4.0194e-03,\n",
       "         -1.1020e-02, -5.3252e-03,  9.4560e-03, -1.0021e-02, -7.3982e-03,\n",
       "         -7.3614e-03,  1.9347e-03, -4.5680e-01, -2.0862e-03,  1.9085e-02,\n",
       "          1.7662e-02, -1.2846e-02,  3.7219e-02,  2.8182e-03,  1.1479e-03,\n",
       "         -2.4153e-02,  3.1773e-03,  5.7702e-03,  3.8999e-04,  1.4302e-02,\n",
       "          1.7305e-02, -2.3862e-03,  1.3526e-02,  2.2137e-02,  1.8308e-03,\n",
       "         -7.9128e-03,  7.6103e-03, -3.1035e-02, -8.2131e-03, -1.2822e-03,\n",
       "          8.3978e-03,  1.5163e-03,  2.1477e-02,  1.6554e-02,  4.4724e-03,\n",
       "          1.0268e-02,  7.1258e-03, -2.3761e-03, -1.0732e-02, -6.5394e-04,\n",
       "          1.4211e-02,  2.7307e-02,  5.7820e-03,  4.3122e-03, -7.4557e-03,\n",
       "          6.6099e-03,  8.9450e-03,  4.3076e-03,  5.2389e-03,  3.5383e-03,\n",
       "         -2.5471e-04,  1.3394e-02, -5.0394e-05,  5.7474e-03,  8.6091e-03,\n",
       "          2.7061e-02,  6.3930e-03,  2.9947e-03, -5.3344e-03,  2.7146e-03,\n",
       "          4.8324e-03, -2.1186e-02,  1.1404e-03, -3.1922e-02,  1.6449e-02,\n",
       "          2.3279e-03,  1.4111e-02, -2.2936e-03,  7.4219e-03, -4.8720e-03,\n",
       "         -2.5247e-03,  4.3544e-03,  1.0382e-02,  1.8119e-02,  7.4080e-03,\n",
       "          2.2422e-03, -3.3622e-03, -8.1881e-03, -1.0473e-02, -1.1642e-02,\n",
       "          7.0033e-03,  2.7959e-03, -5.5204e-04, -9.6975e-03,  2.1965e-03,\n",
       "          2.1241e-02, -6.2077e-03,  1.0785e-02,  3.5871e-03,  2.7559e-02,\n",
       "          1.2774e-02, -1.0027e-02,  1.5423e-02,  7.0150e-03,  1.1042e-02,\n",
       "          8.0314e-03, -2.5760e-03, -3.8168e-03, -1.2620e-02,  1.5723e-02,\n",
       "          2.0287e-03,  4.3522e-04,  2.4682e-03, -9.8599e-03, -8.9741e-04,\n",
       "          1.4190e-02,  8.0710e-03,  5.6295e-03, -3.7636e-03, -4.2464e-03,\n",
       "          7.9113e-03,  5.8826e-03,  3.6829e-03, -1.4106e-02, -4.9973e-03,\n",
       "         -1.1240e-02, -3.4441e-03, -1.5463e-02,  4.3100e-03,  9.6322e-03,\n",
       "         -5.1104e-03,  8.9357e-03, -4.1790e-03,  1.2243e-02, -1.6679e-03,\n",
       "          3.3925e-03,  3.8283e-03,  6.3151e-03, -2.5441e-03,  8.9484e-03,\n",
       "          6.8739e-03, -1.7973e-03,  1.3399e-02,  2.3339e-03,  7.2225e-03,\n",
       "          2.0509e-03, -4.9170e-03, -6.4325e-03,  1.1002e-03, -9.2562e-03,\n",
       "         -4.6686e-04,  1.1786e-02, -9.5276e-03, -8.9700e-03, -8.7295e-03,\n",
       "         -1.1508e-02,  4.2911e-03, -1.1335e-01,  1.1238e-03, -1.4181e-02,\n",
       "          3.0143e-03, -3.6785e-03, -1.1118e-02,  2.9015e-03,  3.1778e-04,\n",
       "          9.5394e-04, -2.9340e-03, -3.1343e-03, -3.4385e-03,  7.7222e-03,\n",
       "          6.7676e-03, -4.7977e-03, -1.0756e-02, -1.3825e-03, -1.2322e-04,\n",
       "         -1.8334e-02, -3.6852e-03, -4.1777e-03, -1.1539e-03,  1.1336e-03,\n",
       "         -2.1598e-02,  1.2668e-03, -1.7725e-02,  1.7074e-02, -5.7709e-03,\n",
       "         -1.3627e-03,  3.3454e-03,  1.7360e-02,  1.4367e-03, -1.4652e-02,\n",
       "          1.1676e-02,  1.3778e-02,  1.9966e-04, -4.5999e-04, -4.4801e-03,\n",
       "          5.4696e-03,  1.9739e-03,  2.6241e-03,  1.3719e-02,  4.9806e-03,\n",
       "          1.5055e-02,  7.3625e-03, -9.5191e-03, -1.0876e-03, -5.0751e-04,\n",
       "         -2.0784e-02,  4.2875e-03, -5.8806e-03, -3.9184e-03, -6.3953e-03,\n",
       "         -1.0703e-02, -3.6518e-03,  3.1850e-03, -1.8181e-03, -4.5782e-03,\n",
       "          4.7139e-04, -1.9748e-03, -3.1328e-03,  6.5137e-03,  6.9513e-03,\n",
       "          5.1411e-03, -8.9535e-03,  1.5860e-02,  1.5066e-03,  1.0737e-02,\n",
       "          1.3670e-02, -9.9992e-03,  4.9847e-04,  1.9705e-03,  5.3034e-03,\n",
       "          6.2379e-03, -8.2502e-03,  4.3158e-03,  1.5326e-02, -2.4895e-03,\n",
       "         -8.6034e-03,  1.0202e-02,  7.6384e-03,  6.5616e-03,  1.2248e-02,\n",
       "          4.9169e-03, -1.3995e-03, -1.7336e-02,  3.3855e-03,  2.6210e-04,\n",
       "          6.6906e-03,  2.5169e-03,  9.3117e-03, -3.2242e-03,  6.2075e-03,\n",
       "         -9.8111e-03, -1.3930e-02,  5.0487e-03,  1.3871e-02, -9.6206e-04,\n",
       "         -4.7732e-03,  1.1069e-02, -4.8731e-03,  6.3412e-03,  2.6490e-03,\n",
       "         -3.1659e-03,  3.9396e-03,  3.6608e-04, -1.1498e-03,  1.2052e-02,\n",
       "          1.4674e-03, -1.0885e-02,  1.6284e-02,  1.9499e-03,  2.2732e-02,\n",
       "          9.0800e-03,  3.7593e-02,  1.4429e-02,  1.5466e-02,  9.6924e-03,\n",
       "          7.6913e-03, -4.9955e-03,  3.8544e-03,  1.7389e-02,  9.2592e-03,\n",
       "         -1.1920e-02,  2.0179e-02, -2.1598e-03,  5.5349e-03,  9.2041e-03,\n",
       "          9.3341e-03,  2.3658e-04, -1.0541e-02,  2.2867e-04, -1.4111e-02,\n",
       "          2.8089e-03,  5.8734e-03,  7.3107e-03, -9.2302e-03,  5.8167e-05,\n",
       "         -1.5539e-05,  3.2187e-04, -3.2072e-03,  9.4124e-03, -1.4130e-02,\n",
       "          4.6818e-03,  1.9952e-02,  5.5842e-03,  5.5775e-03,  1.8433e-02,\n",
       "         -2.8394e-02,  1.2944e-02,  3.3430e-03, -3.9045e-03,  9.5126e-03,\n",
       "          6.2646e-03,  4.9448e-03, -1.6518e-03, -8.3869e-03, -3.5997e-03,\n",
       "          3.0106e-03, -7.3395e-04,  8.9920e-03,  1.3516e-02,  2.1430e-03,\n",
       "         -2.1342e-03,  1.0103e-02,  8.6068e-05, -3.3541e-03,  9.3081e-03,\n",
       "         -3.5328e-03,  5.0318e-03, -1.1148e-02, -1.5741e-02,  8.4271e-03,\n",
       "          1.1887e-03,  5.2659e-03,  2.0918e-03,  1.4987e-02,  3.6587e-03,\n",
       "          2.4265e-03,  1.5755e-02,  3.2163e-03,  7.6669e-03,  3.4107e-03,\n",
       "         -7.2191e-03, -3.5869e-03,  2.3770e-03,  1.6017e-02, -1.6387e-02,\n",
       "          2.6772e-03,  1.2461e-02, -8.3551e-03,  6.9485e-04, -9.4997e-03,\n",
       "         -9.0822e-03, -1.0024e-02,  2.8108e-03, -1.9855e-03, -1.9840e-02,\n",
       "          5.5812e-03,  9.9509e-03,  1.2748e-02, -2.8234e-03, -2.9404e-02,\n",
       "          3.6320e-03,  7.0173e-03, -1.5482e-02,  4.9563e-03,  1.2133e-02,\n",
       "         -2.8374e-03,  1.1702e-02,  9.7027e-04, -1.4113e-02,  4.0165e-03,\n",
       "         -1.7247e-03,  1.0316e-02, -7.1253e-03,  2.4240e-03,  3.7844e-04,\n",
       "          3.2477e-03,  3.1753e-04, -9.4496e-03, -2.4599e-05, -3.6069e-03,\n",
       "         -1.1556e-03, -1.0898e-02,  1.3282e-02, -1.9423e-03, -7.4289e-03,\n",
       "         -4.9010e-03,  1.1824e-02, -4.5511e-03,  1.7414e-03, -6.4044e-03,\n",
       "         -1.2244e-02,  1.1061e-02, -1.7420e-02, -1.1088e-01,  1.1602e-02,\n",
       "         -8.4184e-03,  5.2573e-03,  1.3250e-03,  4.7423e-03, -6.7491e-03,\n",
       "         -3.0630e-03, -4.5334e-03, -2.6916e-03,  2.4258e-03, -5.2479e-03,\n",
       "          6.6724e-03, -9.8198e-03,  1.5495e-02, -2.9680e-03,  8.3656e-03,\n",
       "          3.9042e-03, -2.1342e-03, -2.2821e-03, -1.1465e-02, -1.3934e-03,\n",
       "         -3.1655e-03,  4.2489e-03, -2.2638e-03, -1.0170e-02, -1.9768e-03,\n",
       "         -1.2217e-03,  1.0365e-02, -3.7323e-03, -6.4851e-03, -4.4704e-03,\n",
       "          1.3606e-02,  2.1373e-03, -1.8996e-03,  7.0255e-03,  4.5571e-03,\n",
       "          2.5664e-03, -3.3219e-02, -1.3451e-02,  8.1503e-04,  1.3151e-02,\n",
       "          9.3752e-03, -4.8294e-02, -9.6205e-03, -8.2815e-03, -2.2727e-03,\n",
       "          1.2821e-02,  4.3857e-03,  2.6239e-03,  2.4468e-03,  6.5291e-03,\n",
       "         -4.6601e-03, -3.7455e-03, -1.6723e-03, -5.4709e-03,  1.1900e-02,\n",
       "         -8.0949e-03,  7.3848e-03,  1.8335e-03,  8.3622e-03, -6.8431e-03,\n",
       "          5.4664e-03,  1.0300e-02, -8.9427e-04,  9.5014e-03,  1.5952e-02,\n",
       "         -1.1056e-02, -6.1797e-03, -3.5524e-03,  6.4665e-03,  1.8559e-03,\n",
       "         -3.4960e-04, -9.4443e-03,  6.4860e-03,  1.5641e-03, -3.4599e-03,\n",
       "          2.8691e-02,  4.4073e-03,  9.5218e-03, -2.0620e-03,  7.4297e-03,\n",
       "          8.6710e-03, -6.8360e-03,  5.1518e-03, -1.8363e-02, -8.4256e-04,\n",
       "         -1.0747e-02, -6.2617e-03,  1.4172e-02,  1.0657e-02,  1.0419e-02,\n",
       "         -4.6256e-03,  1.4383e-02,  1.7478e-03,  7.1992e-03,  9.0452e-03,\n",
       "          3.0089e-03, -2.0526e-02, -1.0363e-03, -6.6664e-03, -9.7393e-03,\n",
       "         -7.8169e-03, -4.8351e-03, -1.3292e-03,  2.6471e-03, -3.9592e-03,\n",
       "         -8.4209e-03,  2.1313e-03,  8.1229e-03,  1.8575e-03, -5.8229e-03,\n",
       "          5.3647e-03,  9.2889e-04,  1.2527e-02, -3.3444e-03, -7.7467e-04,\n",
       "         -3.5615e-02,  2.3951e-03, -1.7175e-02, -1.8532e-02,  4.6808e-03,\n",
       "          1.1841e-04,  1.2488e-02,  1.1057e-02,  3.5400e-03,  3.5686e-03,\n",
       "          1.4092e-02, -3.6221e-03,  2.8867e-03,  8.2824e-03,  7.4932e-03,\n",
       "          2.0700e-02,  6.8332e-03,  1.5058e-02,  8.1493e-01, -1.7158e-02,\n",
       "         -4.6776e-03, -5.2034e-03,  4.8757e-04,  7.7657e-04, -6.1883e-03,\n",
       "          4.5009e-03, -2.4994e-04,  1.2194e-02, -6.0803e-03,  9.7400e-03,\n",
       "         -5.9402e-03, -8.5152e-03, -2.8941e-03,  7.0503e-03,  1.2019e-02,\n",
       "          3.9230e-03, -1.2153e-02,  1.2879e-02, -1.2709e-03,  7.2163e-03,\n",
       "         -1.8690e-03,  5.3672e-02, -6.0068e-03,  3.9964e-04, -4.0604e-03,\n",
       "          1.3089e-02, -2.5578e-03, -3.4685e-03,  8.8319e-03,  8.0521e-03,\n",
       "          6.9492e-03,  9.2181e-03,  1.1852e-02, -8.6067e-03,  2.3437e-02,\n",
       "         -3.6297e-03, -2.3029e-03, -8.8742e-03,  6.1752e-03,  1.5025e-03,\n",
       "          2.3615e-03,  1.5268e-04,  5.7596e-03,  5.3685e-03,  1.6986e-02,\n",
       "          7.7942e-03,  2.2066e-03,  1.2521e-03, -8.3357e-04,  3.2105e-04,\n",
       "          1.8911e-02,  1.1194e-03, -7.4054e-04, -6.1886e-03, -2.0372e-03,\n",
       "          9.6621e-03, -5.9853e-03, -1.5269e-02,  2.6015e-03,  2.6262e-03,\n",
       "         -2.7955e-03, -2.3227e-03,  2.6571e-03,  1.2324e-02,  9.2810e-03,\n",
       "          1.4284e-02,  7.3499e-03, -2.4508e-03,  2.6987e-03, -2.3882e-03,\n",
       "         -1.2576e-02, -5.8454e-03, -2.2294e-02,  1.4569e-02, -2.9006e-02,\n",
       "         -3.8505e-03,  6.6244e-03,  1.6891e-02,  8.9799e-03,  2.0738e-04,\n",
       "         -1.0319e-02, -1.7328e-02, -1.2943e-02,  4.2387e-03,  2.1112e-03,\n",
       "         -1.7633e-04,  7.3758e-05,  8.3724e-03, -2.1094e-02, -8.5573e-03,\n",
       "          7.5917e-05,  6.7517e-03,  1.3530e-02, -1.7533e-05, -2.2734e-03,\n",
       "          2.5475e-03,  4.9505e-03,  1.9005e-03, -2.2488e-03, -2.8081e-03,\n",
       "          4.5639e-04,  1.0308e-02,  1.4022e-02,  5.6596e-03,  1.6994e-03,\n",
       "         -4.9849e-03,  2.5504e-03,  1.5016e-02, -5.0394e-03,  7.4234e-03,\n",
       "         -7.5893e-03,  1.0945e-02, -7.4952e-03, -4.4587e-03,  4.9095e-03,\n",
       "          2.3160e-03,  1.1761e-03,  8.8275e-03,  1.4622e-03, -5.1414e-04,\n",
       "         -6.1501e-03,  4.9069e-03,  9.9144e-03,  1.0939e-02,  2.4803e-03,\n",
       "          2.2115e-03, -1.4271e-02,  3.6542e-03,  8.5132e-03,  1.3164e-03,\n",
       "          2.3814e-02,  1.9693e-02,  5.9300e-03,  9.2952e-03, -8.6915e-04,\n",
       "          9.9247e-03, -1.6310e-02,  1.2592e-03, -3.6661e-03,  5.6942e-03,\n",
       "          9.1841e-03, -3.0703e-02,  5.6817e-03, -1.4531e-02,  1.2972e-02,\n",
       "          1.0006e-02,  1.0748e-02,  1.8882e-03, -1.4238e-02,  5.0236e-03,\n",
       "          2.0317e-02, -7.9171e-03, -1.1117e-02,  1.5037e-02, -6.8789e-03,\n",
       "          1.3309e-02,  5.8404e-03, -1.1654e-03, -3.8196e-05, -1.2701e-01,\n",
       "         -8.6196e-03,  2.4939e-03, -2.5190e-02, -4.0483e-03,  1.0690e-02,\n",
       "          7.9230e-03, -8.2772e-03,  1.3724e-02,  7.8246e-03,  6.7537e-03,\n",
       "         -6.3503e-03,  6.5181e-03, -4.4597e-03, -5.4972e-03,  1.4763e-02,\n",
       "         -3.7182e-03,  3.4100e-03,  5.2047e-03,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          7.9651e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00]),\n",
       " tensor(-0.5370))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2d1c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(df_dataset, [0.7, 0.3], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d97d609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24908\n",
      "17436\n",
      "7472\n"
     ]
    }
   ],
   "source": [
    "print(len(df_dataset))\n",
    "print(len(train_set))\n",
    "print(len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71ca24fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = df['reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4c7e8d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b1b11",
   "metadata": {},
   "source": [
    "Claramente tenemos los episodios que ganan pronto y los que no ganan casi nunca..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "508e4676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "35f8aaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[df['reward'] <= 100]['reward'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10eb7a5",
   "metadata": {},
   "source": [
    "Veamos si puedo ver en el espacio latente qu embeddings tienden a tener un reward mas alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b593f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqli_labels = [\"0\" if float(x) < 100 else \"1\" for x in df['reward']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e3d81a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '0', '0', '0', '0', '0', '0', '0', '0', '0']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqli_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7311f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_result_df = pd.DataFrame({'tsne_1': tsne_result[:,0], 'tsne_2': tsne_result[:,1]})\n",
    "fig, ax = plt.subplots(1, figsize=(15, 15))\n",
    "\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue=sqli_labels, data=tsne_result_df, ax=ax,s=20)\n",
    "lim = (tsne_result.min()-5, tsne_result.max()+5)\n",
    "ax.set_xlim(lim)\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5feda756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hid1 = torch.nn.Linear(794, 128)  \n",
    "        self.hid2 = torch.nn.Linear(128, 32)\n",
    "        self.oupt = torch.nn.Linear(32, 1)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.hid1.weight)\n",
    "        torch.nn.init.zeros_(self.hid1.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.hid2.weight)\n",
    "        torch.nn.init.zeros_(self.hid2.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.oupt.weight)\n",
    "        torch.nn.init.zeros_(self.oupt.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = torch.relu(self.hid1(x))\n",
    "        z = torch.relu(self.hid2(z))\n",
    "        z = self.oupt(z)  # no activation\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4d02a682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Loss after mini-batch     1: 0.004\n",
      "Loss after mini-batch   101: 31.829\n",
      "Loss after mini-batch   201: 27.317\n",
      "Loss after mini-batch   301: 20.564\n",
      "Loss after mini-batch   401: 29.545\n",
      "Loss after mini-batch   501: 40.770\n",
      "Loss after mini-batch   601: 13.773\n",
      "Loss after mini-batch   701: 29.503\n",
      "Loss after mini-batch   801: 22.771\n",
      "Loss after mini-batch   901: 27.182\n",
      "Loss after mini-batch  1001: 24.961\n",
      "Loss after mini-batch  1101: 27.214\n",
      "Loss after mini-batch  1201: 29.449\n",
      "Loss after mini-batch  1301: 13.710\n",
      "Loss after mini-batch  1401: 27.138\n",
      "Loss after mini-batch  1501: 36.081\n",
      "Loss after mini-batch  1601: 6.940\n",
      "Loss after mini-batch  1701: 24.905\n",
      "Avg. loss for epoch 0 24.092\n",
      "Avg. loss for all epochs at epoch 0 24.092\n",
      "Starting epoch 1\n",
      "Loss after mini-batch     1: 0.003\n",
      "Loss after mini-batch   101: 31.550\n",
      "Loss after mini-batch   201: 27.072\n",
      "Loss after mini-batch   301: 24.784\n",
      "Loss after mini-batch   401: 40.359\n",
      "Loss after mini-batch   501: 22.501\n",
      "Loss after mini-batch   601: 31.418\n",
      "Loss after mini-batch   701: 22.488\n",
      "Loss after mini-batch   801: 20.276\n",
      "Loss after mini-batch   901: 20.252\n",
      "Loss after mini-batch  1001: 24.641\n",
      "Loss after mini-batch  1101: 20.190\n",
      "Loss after mini-batch  1201: 22.508\n",
      "Loss after mini-batch  1301: 20.137\n",
      "Loss after mini-batch  1401: 33.402\n",
      "Loss after mini-batch  1501: 20.209\n",
      "Loss after mini-batch  1601: 20.138\n",
      "Loss after mini-batch  1701: 20.150\n",
      "Avg. loss for epoch 1 23.449\n",
      "Avg. loss for all epochs at epoch 1 23.770\n",
      "Starting epoch 2\n",
      "Loss after mini-batch     1: 2.181\n",
      "Loss after mini-batch   101: 20.097\n",
      "Loss after mini-batch   201: 13.586\n",
      "Loss after mini-batch   301: 26.669\n",
      "Loss after mini-batch   401: 31.196\n",
      "Loss after mini-batch   501: 28.971\n",
      "Loss after mini-batch   601: 13.504\n",
      "Loss after mini-batch   701: 17.934\n",
      "Loss after mini-batch   801: 24.533\n",
      "Loss after mini-batch   901: 24.357\n",
      "Loss after mini-batch  1001: 37.458\n",
      "Loss after mini-batch  1101: 20.010\n",
      "Loss after mini-batch  1201: 26.452\n",
      "Loss after mini-batch  1301: 17.955\n",
      "Loss after mini-batch  1401: 30.811\n",
      "Loss after mini-batch  1501: 26.547\n",
      "Loss after mini-batch  1601: 32.919\n",
      "Loss after mini-batch  1701: 37.281\n",
      "Avg. loss for epoch 2 24.026\n",
      "Avg. loss for all epochs at epoch 2 23.855\n",
      "Starting epoch 3\n",
      "Loss after mini-batch     1: 0.006\n",
      "Loss after mini-batch   101: 30.971\n",
      "Loss after mini-batch   201: 24.392\n",
      "Loss after mini-batch   301: 22.005\n",
      "Loss after mini-batch   401: 19.964\n",
      "Loss after mini-batch   501: 26.460\n",
      "Loss after mini-batch   601: 21.952\n",
      "Loss after mini-batch   701: 19.838\n",
      "Loss after mini-batch   801: 26.384\n",
      "Loss after mini-batch   901: 20.126\n",
      "Loss after mini-batch  1001: 19.815\n",
      "Loss after mini-batch  1101: 23.969\n",
      "Loss after mini-batch  1201: 19.679\n",
      "Loss after mini-batch  1301: 30.388\n",
      "Loss after mini-batch  1401: 23.980\n",
      "Loss after mini-batch  1501: 26.200\n",
      "Loss after mini-batch  1601: 26.059\n",
      "Loss after mini-batch  1701: 40.914\n",
      "Avg. loss for epoch 3 23.506\n",
      "Avg. loss for all epochs at epoch 3 23.768\n",
      "Starting epoch 4\n",
      "Loss after mini-batch     1: 0.010\n",
      "Loss after mini-batch   101: 38.883\n",
      "Loss after mini-batch   201: 21.555\n",
      "Loss after mini-batch   301: 21.864\n",
      "Loss after mini-batch   401: 23.987\n",
      "Loss after mini-batch   501: 32.666\n",
      "Loss after mini-batch   601: 28.129\n",
      "Loss after mini-batch   701: 19.675\n",
      "Loss after mini-batch   801: 23.642\n",
      "Loss after mini-batch   901: 21.768\n",
      "Loss after mini-batch  1001: 17.545\n",
      "Loss after mini-batch  1101: 34.473\n",
      "Loss after mini-batch  1201: 19.756\n",
      "Loss after mini-batch  1301: 27.622\n",
      "Loss after mini-batch  1401: 21.599\n",
      "Loss after mini-batch  1501: 17.382\n",
      "Loss after mini-batch  1601: 29.976\n",
      "Loss after mini-batch  1701: 21.612\n",
      "Avg. loss for epoch 4 23.452\n",
      "Avg. loss for all epochs at epoch 4 23.705\n",
      "Starting epoch 5\n",
      "Loss after mini-batch     1: 0.008\n",
      "Loss after mini-batch   101: 25.517\n",
      "Loss after mini-batch   201: 21.921\n",
      "Loss after mini-batch   301: 29.777\n",
      "Loss after mini-batch   401: 19.712\n",
      "Loss after mini-batch   501: 29.765\n",
      "Loss after mini-batch   601: 27.497\n",
      "Loss after mini-batch   701: 27.505\n",
      "Loss after mini-batch   801: 17.672\n",
      "Loss after mini-batch   901: 8.974\n",
      "Loss after mini-batch  1001: 38.466\n",
      "Loss after mini-batch  1101: 11.043\n",
      "Loss after mini-batch  1201: 33.513\n",
      "Loss after mini-batch  1301: 21.298\n",
      "Loss after mini-batch  1401: 23.595\n",
      "Loss after mini-batch  1501: 23.253\n",
      "Loss after mini-batch  1601: 31.533\n",
      "Loss after mini-batch  1701: 23.224\n",
      "Avg. loss for epoch 5 23.015\n",
      "Avg. loss for all epochs at epoch 5 23.590\n",
      "Starting epoch 6\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch   101: 13.544\n",
      "Loss after mini-batch   201: 27.385\n",
      "Loss after mini-batch   301: 16.998\n",
      "Loss after mini-batch   401: 35.334\n",
      "Loss after mini-batch   501: 14.970\n",
      "Loss after mini-batch   601: 25.428\n",
      "Loss after mini-batch   701: 26.949\n",
      "Loss after mini-batch   801: 23.581\n",
      "Loss after mini-batch   901: 25.618\n",
      "Loss after mini-batch  1001: 24.786\n",
      "Loss after mini-batch  1101: 37.761\n",
      "Loss after mini-batch  1201: 18.889\n",
      "Loss after mini-batch  1301: 19.041\n",
      "Loss after mini-batch  1401: 29.187\n",
      "Loss after mini-batch  1501: 24.931\n",
      "Loss after mini-batch  1601: 20.807\n",
      "Loss after mini-batch  1701: 19.203\n",
      "Avg. loss for epoch 6 22.467\n",
      "Avg. loss for all epochs at epoch 6 23.430\n",
      "Starting epoch 7\n",
      "Loss after mini-batch     1: 2.202\n",
      "Loss after mini-batch   101: 23.572\n",
      "Loss after mini-batch   201: 26.836\n",
      "Loss after mini-batch   301: 24.407\n",
      "Loss after mini-batch   401: 17.087\n",
      "Loss after mini-batch   501: 18.893\n",
      "Loss after mini-batch   601: 30.915\n",
      "Loss after mini-batch   701: 23.040\n",
      "Loss after mini-batch   801: 17.235\n",
      "Loss after mini-batch   901: 16.774\n",
      "Loss after mini-batch  1001: 26.476\n",
      "Loss after mini-batch  1101: 26.712\n",
      "Loss after mini-batch  1201: 25.187\n",
      "Loss after mini-batch  1301: 20.685\n",
      "Loss after mini-batch  1401: 30.335\n",
      "Loss after mini-batch  1501: 21.169\n",
      "Loss after mini-batch  1601: 23.044\n",
      "Loss after mini-batch  1701: 30.153\n",
      "Avg. loss for epoch 7 22.485\n",
      "Avg. loss for all epochs at epoch 7 23.311\n",
      "Starting epoch 8\n",
      "Loss after mini-batch     1: 0.019\n",
      "Loss after mini-batch   101: 22.766\n",
      "Loss after mini-batch   201: 24.105\n",
      "Loss after mini-batch   301: 20.519\n",
      "Loss after mini-batch   401: 38.155\n",
      "Loss after mini-batch   501: 24.515\n",
      "Loss after mini-batch   601: 18.729\n",
      "Loss after mini-batch   701: 16.684\n",
      "Loss after mini-batch   801: 18.217\n",
      "Loss after mini-batch   901: 17.044\n",
      "Loss after mini-batch  1001: 16.037\n",
      "Loss after mini-batch  1101: 25.839\n",
      "Loss after mini-batch  1201: 22.771\n",
      "Loss after mini-batch  1301: 14.484\n",
      "Loss after mini-batch  1401: 22.756\n",
      "Loss after mini-batch  1501: 30.365\n",
      "Loss after mini-batch  1601: 24.143\n",
      "Loss after mini-batch  1701: 37.348\n",
      "Avg. loss for epoch 8 21.917\n",
      "Avg. loss for all epochs at epoch 8 23.156\n",
      "Starting epoch 9\n",
      "Loss after mini-batch     1: 0.006\n",
      "Loss after mini-batch   101: 34.367\n",
      "Loss after mini-batch   201: 22.533\n",
      "Loss after mini-batch   301: 18.113\n",
      "Loss after mini-batch   401: 14.737\n",
      "Loss after mini-batch   501: 23.580\n",
      "Loss after mini-batch   601: 14.566\n",
      "Loss after mini-batch   701: 30.402\n",
      "Loss after mini-batch   801: 27.997\n",
      "Loss after mini-batch   901: 24.260\n",
      "Loss after mini-batch  1001: 13.160\n",
      "Loss after mini-batch  1101: 24.130\n",
      "Loss after mini-batch  1201: 18.493\n",
      "Loss after mini-batch  1301: 20.138\n",
      "Loss after mini-batch  1401: 25.984\n",
      "Loss after mini-batch  1501: 31.107\n",
      "Loss after mini-batch  1601: 25.871\n",
      "Loss after mini-batch  1701: 18.287\n",
      "Avg. loss for epoch 9 21.540\n",
      "Avg. loss for all epochs at epoch 9 22.995\n",
      "Starting epoch 10\n",
      "Loss after mini-batch     1: 0.020\n",
      "Loss after mini-batch   101: 23.019\n",
      "Loss after mini-batch   201: 18.472\n",
      "Loss after mini-batch   301: 28.975\n",
      "Loss after mini-batch   401: 17.376\n",
      "Loss after mini-batch   501: 32.829\n",
      "Loss after mini-batch   601: 17.997\n",
      "Loss after mini-batch   701: 21.240\n",
      "Loss after mini-batch   801: 24.427\n",
      "Loss after mini-batch   901: 23.722\n",
      "Loss after mini-batch  1001: 22.845\n",
      "Loss after mini-batch  1101: 12.184\n",
      "Loss after mini-batch  1201: 15.476\n",
      "Loss after mini-batch  1301: 20.116\n",
      "Loss after mini-batch  1401: 31.956\n",
      "Loss after mini-batch  1501: 21.010\n",
      "Loss after mini-batch  1601: 19.745\n",
      "Loss after mini-batch  1701: 28.062\n",
      "Avg. loss for epoch 10 21.082\n",
      "Avg. loss for all epochs at epoch 10 22.821\n",
      "Starting epoch 11\n",
      "Loss after mini-batch     1: 0.007\n",
      "Loss after mini-batch   101: 29.756\n",
      "Loss after mini-batch   201: 13.646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   301: 18.693\n",
      "Loss after mini-batch   401: 25.230\n",
      "Loss after mini-batch   501: 21.643\n",
      "Loss after mini-batch   601: 20.107\n",
      "Loss after mini-batch   701: 23.318\n",
      "Loss after mini-batch   801: 27.535\n",
      "Loss after mini-batch   901: 24.352\n",
      "Loss after mini-batch  1001: 23.622\n",
      "Loss after mini-batch  1101: 23.702\n",
      "Loss after mini-batch  1201: 26.617\n",
      "Loss after mini-batch  1301: 23.847\n",
      "Loss after mini-batch  1401: 23.074\n",
      "Loss after mini-batch  1501: 11.885\n",
      "Loss after mini-batch  1601: 24.891\n",
      "Loss after mini-batch  1701: 22.116\n",
      "Avg. loss for epoch 11 21.336\n",
      "Avg. loss for all epochs at epoch 11 22.697\n",
      "Starting epoch 12\n",
      "Loss after mini-batch     1: 0.027\n",
      "Loss after mini-batch   101: 25.994\n",
      "Loss after mini-batch   201: 17.484\n",
      "Loss after mini-batch   301: 35.959\n",
      "Loss after mini-batch   401: 20.409\n",
      "Loss after mini-batch   501: 32.662\n",
      "Loss after mini-batch   601: 13.650\n",
      "Loss after mini-batch   701: 26.362\n",
      "Loss after mini-batch   801: 22.685\n",
      "Loss after mini-batch   901: 16.778\n",
      "Loss after mini-batch  1001: 23.097\n",
      "Loss after mini-batch  1101: 15.939\n",
      "Loss after mini-batch  1201: 14.109\n",
      "Loss after mini-batch  1301: 30.852\n",
      "Loss after mini-batch  1401: 18.881\n",
      "Loss after mini-batch  1501: 20.489\n",
      "Loss after mini-batch  1601: 18.312\n",
      "Loss after mini-batch  1701: 18.122\n",
      "Avg. loss for epoch 12 20.656\n",
      "Avg. loss for all epochs at epoch 12 22.540\n",
      "Starting epoch 13\n",
      "Loss after mini-batch     1: 0.011\n",
      "Loss after mini-batch   101: 20.570\n",
      "Loss after mini-batch   201: 35.983\n",
      "Loss after mini-batch   301: 23.097\n",
      "Loss after mini-batch   401: 16.789\n",
      "Loss after mini-batch   501: 30.453\n",
      "Loss after mini-batch   601: 22.416\n",
      "Loss after mini-batch   701: 27.762\n",
      "Loss after mini-batch   801: 19.500\n",
      "Loss after mini-batch   901: 13.708\n",
      "Loss after mini-batch  1001: 23.246\n",
      "Loss after mini-batch  1101: 14.149\n",
      "Loss after mini-batch  1201: 23.574\n",
      "Loss after mini-batch  1301: 22.115\n",
      "Loss after mini-batch  1401: 24.325\n",
      "Loss after mini-batch  1501: 17.475\n",
      "Loss after mini-batch  1601: 11.233\n",
      "Loss after mini-batch  1701: 14.929\n",
      "Avg. loss for epoch 13 20.074\n",
      "Avg. loss for all epochs at epoch 13 22.364\n",
      "Starting epoch 14\n",
      "Loss after mini-batch     1: 1.551\n",
      "Loss after mini-batch   101: 19.366\n",
      "Loss after mini-batch   201: 19.765\n",
      "Loss after mini-batch   301: 17.550\n",
      "Loss after mini-batch   401: 19.645\n",
      "Loss after mini-batch   501: 22.736\n",
      "Loss after mini-batch   601: 12.510\n",
      "Loss after mini-batch   701: 41.678\n",
      "Loss after mini-batch   801: 23.269\n",
      "Loss after mini-batch   901: 26.072\n",
      "Loss after mini-batch  1001: 16.051\n",
      "Loss after mini-batch  1101: 23.477\n",
      "Loss after mini-batch  1201: 19.739\n",
      "Loss after mini-batch  1301: 17.843\n",
      "Loss after mini-batch  1401: 21.906\n",
      "Loss after mini-batch  1501: 22.336\n",
      "Loss after mini-batch  1601: 19.239\n",
      "Loss after mini-batch  1701: 19.329\n",
      "Avg. loss for epoch 14 20.226\n",
      "Avg. loss for all epochs at epoch 14 22.221\n",
      "Starting epoch 15\n",
      "Loss after mini-batch     1: 0.042\n",
      "Loss after mini-batch   101: 29.014\n",
      "Loss after mini-batch   201: 13.020\n",
      "Loss after mini-batch   301: 20.664\n",
      "Loss after mini-batch   401: 24.271\n",
      "Loss after mini-batch   501: 17.173\n",
      "Loss after mini-batch   601: 21.857\n",
      "Loss after mini-batch   701: 16.882\n",
      "Loss after mini-batch   801: 25.836\n",
      "Loss after mini-batch   901: 18.537\n",
      "Loss after mini-batch  1001: 25.748\n",
      "Loss after mini-batch  1101: 20.135\n",
      "Loss after mini-batch  1201: 33.466\n",
      "Loss after mini-batch  1301: 11.638\n",
      "Loss after mini-batch  1401: 18.541\n",
      "Loss after mini-batch  1501: 25.579\n",
      "Loss after mini-batch  1601: 23.118\n",
      "Loss after mini-batch  1701: 19.083\n",
      "Avg. loss for epoch 15 20.256\n",
      "Avg. loss for all epochs at epoch 15 22.099\n",
      "Starting epoch 16\n",
      "Loss after mini-batch     1: 0.003\n",
      "Loss after mini-batch   101: 18.674\n",
      "Loss after mini-batch   201: 25.629\n",
      "Loss after mini-batch   301: 25.862\n",
      "Loss after mini-batch   401: 21.854\n",
      "Loss after mini-batch   501: 27.379\n",
      "Loss after mini-batch   601: 12.148\n",
      "Loss after mini-batch   701: 15.071\n",
      "Loss after mini-batch   801: 26.345\n",
      "Loss after mini-batch   901: 15.292\n",
      "Loss after mini-batch  1001: 22.920\n",
      "Loss after mini-batch  1101: 22.199\n",
      "Loss after mini-batch  1201: 22.223\n",
      "Loss after mini-batch  1301: 24.566\n",
      "Loss after mini-batch  1401: 18.833\n",
      "Loss after mini-batch  1501: 12.742\n",
      "Loss after mini-batch  1601: 27.237\n",
      "Loss after mini-batch  1701: 15.932\n",
      "Avg. loss for epoch 16 19.717\n",
      "Avg. loss for all epochs at epoch 16 21.959\n",
      "Starting epoch 17\n",
      "Loss after mini-batch     1: 0.076\n",
      "Loss after mini-batch   101: 21.314\n",
      "Loss after mini-batch   201: 17.826\n",
      "Loss after mini-batch   301: 31.085\n",
      "Loss after mini-batch   401: 28.530\n",
      "Loss after mini-batch   501: 24.420\n",
      "Loss after mini-batch   601: 29.495\n",
      "Loss after mini-batch   701: 10.630\n",
      "Loss after mini-batch   801: 18.309\n",
      "Loss after mini-batch   901: 21.099\n",
      "Loss after mini-batch  1001: 17.359\n",
      "Loss after mini-batch  1101: 29.597\n",
      "Loss after mini-batch  1201: 8.442\n",
      "Loss after mini-batch  1301: 18.269\n",
      "Loss after mini-batch  1401: 22.617\n",
      "Loss after mini-batch  1501: 22.726\n",
      "Loss after mini-batch  1601: 20.702\n",
      "Loss after mini-batch  1701: 17.480\n",
      "Avg. loss for epoch 17 19.999\n",
      "Avg. loss for all epochs at epoch 17 21.850\n",
      "Starting epoch 18\n",
      "Loss after mini-batch     1: 0.007\n",
      "Loss after mini-batch   101: 22.886\n",
      "Loss after mini-batch   201: 21.391\n",
      "Loss after mini-batch   301: 16.665\n",
      "Loss after mini-batch   401: 20.319\n",
      "Loss after mini-batch   501: 30.096\n",
      "Loss after mini-batch   601: 20.048\n",
      "Loss after mini-batch   701: 15.864\n",
      "Loss after mini-batch   801: 25.933\n",
      "Loss after mini-batch   901: 13.037\n",
      "Loss after mini-batch  1001: 14.924\n",
      "Loss after mini-batch  1101: 12.245\n",
      "Loss after mini-batch  1201: 24.864\n",
      "Loss after mini-batch  1301: 27.544\n",
      "Loss after mini-batch  1401: 18.915\n",
      "Loss after mini-batch  1501: 20.113\n",
      "Loss after mini-batch  1601: 20.113\n",
      "Loss after mini-batch  1701: 27.151\n",
      "Avg. loss for epoch 18 19.562\n",
      "Avg. loss for all epochs at epoch 18 21.729\n",
      "Starting epoch 19\n",
      "Loss after mini-batch     1: 0.067\n",
      "Loss after mini-batch   101: 24.422\n",
      "Loss after mini-batch   201: 19.350\n",
      "Loss after mini-batch   301: 14.762\n",
      "Loss after mini-batch   401: 20.335\n",
      "Loss after mini-batch   501: 11.734\n",
      "Loss after mini-batch   601: 33.897\n",
      "Loss after mini-batch   701: 9.709\n",
      "Loss after mini-batch   801: 8.292\n",
      "Loss after mini-batch   901: 24.529\n",
      "Loss after mini-batch  1001: 26.466\n",
      "Loss after mini-batch  1101: 22.339\n",
      "Loss after mini-batch  1201: 16.763\n",
      "Loss after mini-batch  1301: 30.976\n",
      "Loss after mini-batch  1401: 19.143\n",
      "Loss after mini-batch  1501: 21.313\n",
      "Loss after mini-batch  1601: 28.527\n",
      "Loss after mini-batch  1701: 19.358\n",
      "Avg. loss for epoch 19 19.555\n",
      "Avg. loss for all epochs at epoch 19 21.621\n",
      "Starting epoch 20\n",
      "Loss after mini-batch     1: 0.031\n",
      "Loss after mini-batch   101: 12.717\n",
      "Loss after mini-batch   201: 31.295\n",
      "Loss after mini-batch   301: 27.722\n",
      "Loss after mini-batch   401: 11.436\n",
      "Loss after mini-batch   501: 12.565\n",
      "Loss after mini-batch   601: 26.976\n",
      "Loss after mini-batch   701: 20.547\n",
      "Loss after mini-batch   801: 17.951\n",
      "Loss after mini-batch   901: 16.075\n",
      "Loss after mini-batch  1001: 18.044\n",
      "Loss after mini-batch  1101: 17.950\n",
      "Loss after mini-batch  1201: 22.849\n",
      "Loss after mini-batch  1301: 20.698\n",
      "Loss after mini-batch  1401: 14.956\n",
      "Loss after mini-batch  1501: 18.678\n",
      "Loss after mini-batch  1601: 30.333\n",
      "Loss after mini-batch  1701: 21.128\n",
      "Avg. loss for epoch 20 18.997\n",
      "Avg. loss for all epochs at epoch 20 21.496\n",
      "Starting epoch 21\n",
      "Loss after mini-batch     1: 0.030\n",
      "Loss after mini-batch   101: 19.604\n",
      "Loss after mini-batch   201: 25.220\n",
      "Loss after mini-batch   301: 14.118\n",
      "Loss after mini-batch   401: 19.160\n",
      "Loss after mini-batch   501: 20.207\n",
      "Loss after mini-batch   601: 10.169\n",
      "Loss after mini-batch   701: 18.484\n",
      "Loss after mini-batch   801: 17.403\n",
      "Loss after mini-batch   901: 27.792\n",
      "Loss after mini-batch  1001: 21.777\n",
      "Loss after mini-batch  1101: 18.699\n",
      "Loss after mini-batch  1201: 18.593\n",
      "Loss after mini-batch  1301: 19.829\n",
      "Loss after mini-batch  1401: 25.128\n",
      "Loss after mini-batch  1501: 24.684\n",
      "Loss after mini-batch  1601: 31.543\n",
      "Loss after mini-batch  1701: 12.201\n",
      "Avg. loss for epoch 21 19.147\n",
      "Avg. loss for all epochs at epoch 21 21.389\n",
      "Starting epoch 22\n",
      "Loss after mini-batch     1: 0.006\n",
      "Loss after mini-batch   101: 16.891\n",
      "Loss after mini-batch   201: 24.475\n",
      "Loss after mini-batch   301: 19.492\n",
      "Loss after mini-batch   401: 12.254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   501: 27.206\n",
      "Loss after mini-batch   601: 17.345\n",
      "Loss after mini-batch   701: 17.843\n",
      "Loss after mini-batch   801: 20.163\n",
      "Loss after mini-batch   901: 16.173\n",
      "Loss after mini-batch  1001: 20.683\n",
      "Loss after mini-batch  1101: 19.498\n",
      "Loss after mini-batch  1201: 16.923\n",
      "Loss after mini-batch  1301: 12.272\n",
      "Loss after mini-batch  1401: 24.014\n",
      "Loss after mini-batch  1501: 23.087\n",
      "Loss after mini-batch  1601: 29.744\n",
      "Loss after mini-batch  1701: 24.353\n",
      "Avg. loss for epoch 22 19.023\n",
      "Avg. loss for all epochs at epoch 22 21.286\n",
      "Starting epoch 23\n",
      "Loss after mini-batch     1: 1.269\n",
      "Loss after mini-batch   101: 23.022\n",
      "Loss after mini-batch   201: 15.308\n",
      "Loss after mini-batch   301: 16.235\n",
      "Loss after mini-batch   401: 20.149\n",
      "Loss after mini-batch   501: 18.692\n",
      "Loss after mini-batch   601: 22.205\n",
      "Loss after mini-batch   701: 9.909\n",
      "Loss after mini-batch   801: 19.946\n",
      "Loss after mini-batch   901: 17.302\n",
      "Loss after mini-batch  1001: 21.015\n",
      "Loss after mini-batch  1101: 27.133\n",
      "Loss after mini-batch  1201: 19.531\n",
      "Loss after mini-batch  1301: 19.334\n",
      "Loss after mini-batch  1401: 33.573\n",
      "Loss after mini-batch  1501: 21.379\n",
      "Loss after mini-batch  1601: 15.827\n",
      "Loss after mini-batch  1701: 20.234\n",
      "Avg. loss for epoch 23 19.003\n",
      "Avg. loss for all epochs at epoch 23 21.191\n",
      "Starting epoch 24\n",
      "Loss after mini-batch     1: 0.020\n",
      "Loss after mini-batch   101: 27.971\n",
      "Loss after mini-batch   201: 13.311\n",
      "Loss after mini-batch   301: 21.473\n",
      "Loss after mini-batch   401: 22.519\n",
      "Loss after mini-batch   501: 12.353\n",
      "Loss after mini-batch   601: 23.378\n",
      "Loss after mini-batch   701: 16.101\n",
      "Loss after mini-batch   801: 24.921\n",
      "Loss after mini-batch   901: 22.378\n",
      "Loss after mini-batch  1001: 10.257\n",
      "Loss after mini-batch  1101: 18.308\n",
      "Loss after mini-batch  1201: 19.932\n",
      "Loss after mini-batch  1301: 20.804\n",
      "Loss after mini-batch  1401: 24.066\n",
      "Loss after mini-batch  1501: 22.883\n",
      "Loss after mini-batch  1601: 17.860\n",
      "Loss after mini-batch  1701: 24.423\n",
      "Avg. loss for epoch 24 19.053\n",
      "Avg. loss for all epochs at epoch 24 21.105\n",
      "Starting epoch 25\n",
      "Loss after mini-batch     1: 0.006\n",
      "Loss after mini-batch   101: 22.077\n",
      "Loss after mini-batch   201: 12.272\n",
      "Loss after mini-batch   301: 21.439\n",
      "Loss after mini-batch   401: 34.858\n",
      "Loss after mini-batch   501: 27.409\n",
      "Loss after mini-batch   601: 17.214\n",
      "Loss after mini-batch   701: 23.779\n",
      "Loss after mini-batch   801: 19.676\n",
      "Loss after mini-batch   901: 18.681\n",
      "Loss after mini-batch  1001: 14.751\n",
      "Loss after mini-batch  1101: 12.925\n",
      "Loss after mini-batch  1201: 11.891\n",
      "Loss after mini-batch  1301: 24.535\n",
      "Loss after mini-batch  1401: 27.903\n",
      "Loss after mini-batch  1501: 8.152\n",
      "Loss after mini-batch  1601: 25.421\n",
      "Loss after mini-batch  1701: 17.135\n",
      "Avg. loss for epoch 25 18.896\n",
      "Avg. loss for all epochs at epoch 25 21.020\n",
      "Starting epoch 26\n",
      "Loss after mini-batch     1: 0.118\n",
      "Loss after mini-batch   101: 17.594\n",
      "Loss after mini-batch   201: 18.556\n",
      "Loss after mini-batch   301: 22.411\n",
      "Loss after mini-batch   401: 23.891\n",
      "Loss after mini-batch   501: 29.274\n",
      "Loss after mini-batch   601: 19.664\n",
      "Loss after mini-batch   701: 14.964\n",
      "Loss after mini-batch   801: 19.494\n",
      "Loss after mini-batch   901: 14.806\n",
      "Loss after mini-batch  1001: 18.193\n",
      "Loss after mini-batch  1101: 14.183\n",
      "Loss after mini-batch  1201: 14.664\n",
      "Loss after mini-batch  1301: 30.059\n",
      "Loss after mini-batch  1401: 18.728\n",
      "Loss after mini-batch  1501: 21.468\n",
      "Loss after mini-batch  1601: 18.022\n",
      "Loss after mini-batch  1701: 21.032\n",
      "Avg. loss for epoch 26 18.729\n",
      "Avg. loss for all epochs at epoch 26 20.936\n",
      "Starting epoch 27\n",
      "Loss after mini-batch     1: 0.005\n",
      "Loss after mini-batch   101: 25.132\n",
      "Loss after mini-batch   201: 14.706\n",
      "Loss after mini-batch   301: 22.667\n",
      "Loss after mini-batch   401: 27.411\n",
      "Loss after mini-batch   501: 22.199\n",
      "Loss after mini-batch   601: 19.268\n",
      "Loss after mini-batch   701: 15.369\n",
      "Loss after mini-batch   801: 20.069\n",
      "Loss after mini-batch   901: 20.795\n",
      "Loss after mini-batch  1001: 16.092\n",
      "Loss after mini-batch  1101: 18.396\n",
      "Loss after mini-batch  1201: 19.015\n",
      "Loss after mini-batch  1301: 20.514\n",
      "Loss after mini-batch  1401: 11.281\n",
      "Loss after mini-batch  1501: 22.358\n",
      "Loss after mini-batch  1601: 20.339\n",
      "Loss after mini-batch  1701: 17.675\n",
      "Avg. loss for epoch 27 18.516\n",
      "Avg. loss for all epochs at epoch 27 20.849\n",
      "Starting epoch 28\n",
      "Loss after mini-batch     1: 0.004\n",
      "Loss after mini-batch   101: 15.246\n",
      "Loss after mini-batch   201: 14.844\n",
      "Loss after mini-batch   301: 20.033\n",
      "Loss after mini-batch   401: 18.441\n",
      "Loss after mini-batch   501: 18.530\n",
      "Loss after mini-batch   601: 21.450\n",
      "Loss after mini-batch   701: 24.744\n",
      "Loss after mini-batch   801: 15.542\n",
      "Loss after mini-batch   901: 26.843\n",
      "Loss after mini-batch  1001: 19.556\n",
      "Loss after mini-batch  1101: 22.049\n",
      "Loss after mini-batch  1201: 19.293\n",
      "Loss after mini-batch  1301: 22.526\n",
      "Loss after mini-batch  1401: 16.834\n",
      "Loss after mini-batch  1501: 17.350\n",
      "Loss after mini-batch  1601: 22.328\n",
      "Loss after mini-batch  1701: 22.125\n",
      "Avg. loss for epoch 28 18.763\n",
      "Avg. loss for all epochs at epoch 28 20.777\n",
      "Starting epoch 29\n",
      "Loss after mini-batch     1: 0.005\n",
      "Loss after mini-batch   101: 21.445\n",
      "Loss after mini-batch   201: 19.520\n",
      "Loss after mini-batch   301: 29.212\n",
      "Loss after mini-batch   401: 14.005\n",
      "Loss after mini-batch   501: 13.810\n",
      "Loss after mini-batch   601: 19.068\n",
      "Loss after mini-batch   701: 22.873\n",
      "Loss after mini-batch   801: 17.892\n",
      "Loss after mini-batch   901: 21.768\n",
      "Loss after mini-batch  1001: 15.134\n",
      "Loss after mini-batch  1101: 16.268\n",
      "Loss after mini-batch  1201: 15.209\n",
      "Loss after mini-batch  1301: 17.995\n",
      "Loss after mini-batch  1401: 18.475\n",
      "Loss after mini-batch  1501: 24.963\n",
      "Loss after mini-batch  1601: 24.108\n",
      "Loss after mini-batch  1701: 22.895\n",
      "Avg. loss for epoch 29 18.591\n",
      "Avg. loss for all epochs at epoch 29 20.704\n",
      "Starting epoch 30\n",
      "Loss after mini-batch     1: 0.218\n",
      "Loss after mini-batch   101: 22.173\n",
      "Loss after mini-batch   201: 19.908\n",
      "Loss after mini-batch   301: 15.417\n",
      "Loss after mini-batch   401: 15.913\n",
      "Loss after mini-batch   501: 14.171\n",
      "Loss after mini-batch   601: 14.521\n",
      "Loss after mini-batch   701: 19.589\n",
      "Loss after mini-batch   801: 23.022\n",
      "Loss after mini-batch   901: 12.666\n",
      "Loss after mini-batch  1001: 21.934\n",
      "Loss after mini-batch  1101: 18.931\n",
      "Loss after mini-batch  1201: 14.675\n",
      "Loss after mini-batch  1301: 25.149\n",
      "Loss after mini-batch  1401: 24.355\n",
      "Loss after mini-batch  1501: 25.704\n",
      "Loss after mini-batch  1601: 16.797\n",
      "Loss after mini-batch  1701: 25.179\n",
      "Avg. loss for epoch 30 18.351\n",
      "Avg. loss for all epochs at epoch 30 20.628\n",
      "Starting epoch 31\n",
      "Loss after mini-batch     1: 0.035\n",
      "Loss after mini-batch   101: 25.526\n",
      "Loss after mini-batch   201: 16.760\n",
      "Loss after mini-batch   301: 19.072\n",
      "Loss after mini-batch   401: 16.697\n",
      "Loss after mini-batch   501: 13.326\n",
      "Loss after mini-batch   601: 31.983\n",
      "Loss after mini-batch   701: 22.218\n",
      "Loss after mini-batch   801: 17.164\n",
      "Loss after mini-batch   901: 18.800\n",
      "Loss after mini-batch  1001: 16.600\n",
      "Loss after mini-batch  1101: 22.160\n",
      "Loss after mini-batch  1201: 20.435\n",
      "Loss after mini-batch  1301: 19.521\n",
      "Loss after mini-batch  1401: 20.516\n",
      "Loss after mini-batch  1501: 14.062\n",
      "Loss after mini-batch  1601: 17.993\n",
      "Loss after mini-batch  1701: 20.339\n",
      "Avg. loss for epoch 31 18.511\n",
      "Avg. loss for all epochs at epoch 31 20.562\n",
      "Starting epoch 32\n",
      "Loss after mini-batch     1: 0.011\n",
      "Loss after mini-batch   101: 23.700\n",
      "Loss after mini-batch   201: 19.529\n",
      "Loss after mini-batch   301: 19.251\n",
      "Loss after mini-batch   401: 24.324\n",
      "Loss after mini-batch   501: 17.417\n",
      "Loss after mini-batch   601: 19.738\n",
      "Loss after mini-batch   701: 20.962\n",
      "Loss after mini-batch   801: 19.586\n",
      "Loss after mini-batch   901: 16.779\n",
      "Loss after mini-batch  1001: 28.734\n",
      "Loss after mini-batch  1101: 24.483\n",
      "Loss after mini-batch  1201: 11.333\n",
      "Loss after mini-batch  1301: 17.309\n",
      "Loss after mini-batch  1401: 20.379\n",
      "Loss after mini-batch  1501: 16.966\n",
      "Loss after mini-batch  1601: 14.822\n",
      "Loss after mini-batch  1701: 18.934\n",
      "Avg. loss for epoch 32 18.570\n",
      "Avg. loss for all epochs at epoch 32 20.502\n",
      "Starting epoch 33\n",
      "Loss after mini-batch     1: 0.011\n",
      "Loss after mini-batch   101: 13.622\n",
      "Loss after mini-batch   201: 21.979\n",
      "Loss after mini-batch   301: 23.870\n",
      "Loss after mini-batch   401: 20.797\n",
      "Loss after mini-batch   501: 27.195\n",
      "Loss after mini-batch   601: 9.585\n",
      "Loss after mini-batch   701: 20.018\n",
      "Loss after mini-batch   801: 19.431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   901: 21.343\n",
      "Loss after mini-batch  1001: 16.377\n",
      "Loss after mini-batch  1101: 20.359\n",
      "Loss after mini-batch  1201: 26.923\n",
      "Loss after mini-batch  1301: 17.593\n",
      "Loss after mini-batch  1401: 19.687\n",
      "Loss after mini-batch  1501: 18.807\n",
      "Loss after mini-batch  1601: 15.009\n",
      "Loss after mini-batch  1701: 12.554\n",
      "Avg. loss for epoch 33 18.064\n",
      "Avg. loss for all epochs at epoch 33 20.430\n",
      "Starting epoch 34\n",
      "Loss after mini-batch     1: 0.290\n",
      "Loss after mini-batch   101: 21.765\n",
      "Loss after mini-batch   201: 14.002\n",
      "Loss after mini-batch   301: 22.851\n",
      "Loss after mini-batch   401: 17.625\n",
      "Loss after mini-batch   501: 16.345\n",
      "Loss after mini-batch   601: 17.867\n",
      "Loss after mini-batch   701: 13.248\n",
      "Loss after mini-batch   801: 35.084\n",
      "Loss after mini-batch   901: 21.907\n",
      "Loss after mini-batch  1001: 20.912\n",
      "Loss after mini-batch  1101: 20.553\n",
      "Loss after mini-batch  1201: 11.691\n",
      "Loss after mini-batch  1301: 17.341\n",
      "Loss after mini-batch  1401: 24.343\n",
      "Loss after mini-batch  1501: 13.668\n",
      "Loss after mini-batch  1601: 22.448\n",
      "Loss after mini-batch  1701: 18.863\n",
      "Avg. loss for epoch 34 18.378\n",
      "Avg. loss for all epochs at epoch 34 20.372\n",
      "Starting epoch 35\n",
      "Loss after mini-batch     1: 0.003\n",
      "Loss after mini-batch   101: 11.606\n",
      "Loss after mini-batch   201: 23.622\n",
      "Loss after mini-batch   301: 19.354\n",
      "Loss after mini-batch   401: 14.684\n",
      "Loss after mini-batch   501: 17.630\n",
      "Loss after mini-batch   601: 10.810\n",
      "Loss after mini-batch   701: 18.148\n",
      "Loss after mini-batch   801: 25.523\n",
      "Loss after mini-batch   901: 18.300\n",
      "Loss after mini-batch  1001: 16.435\n",
      "Loss after mini-batch  1101: 20.271\n",
      "Loss after mini-batch  1201: 16.115\n",
      "Loss after mini-batch  1301: 18.076\n",
      "Loss after mini-batch  1401: 22.861\n",
      "Loss after mini-batch  1501: 23.795\n",
      "Loss after mini-batch  1601: 19.100\n",
      "Loss after mini-batch  1701: 29.750\n",
      "Avg. loss for epoch 35 18.116\n",
      "Avg. loss for all epochs at epoch 35 20.309\n",
      "Starting epoch 36\n",
      "Loss after mini-batch     1: 0.012\n",
      "Loss after mini-batch   101: 9.678\n",
      "Loss after mini-batch   201: 19.090\n",
      "Loss after mini-batch   301: 26.336\n",
      "Loss after mini-batch   401: 29.385\n",
      "Loss after mini-batch   501: 22.198\n",
      "Loss after mini-batch   601: 21.167\n",
      "Loss after mini-batch   701: 21.300\n",
      "Loss after mini-batch   801: 21.364\n",
      "Loss after mini-batch   901: 13.158\n",
      "Loss after mini-batch  1001: 19.894\n",
      "Loss after mini-batch  1101: 17.125\n",
      "Loss after mini-batch  1201: 12.991\n",
      "Loss after mini-batch  1301: 27.447\n",
      "Loss after mini-batch  1401: 20.947\n",
      "Loss after mini-batch  1501: 17.399\n",
      "Loss after mini-batch  1601: 21.616\n",
      "Loss after mini-batch  1701: 10.512\n",
      "Avg. loss for epoch 36 18.423\n",
      "Avg. loss for all epochs at epoch 36 20.258\n",
      "Starting epoch 37\n",
      "Loss after mini-batch     1: 0.008\n",
      "Loss after mini-batch   101: 16.506\n",
      "Loss after mini-batch   201: 21.163\n",
      "Loss after mini-batch   301: 21.910\n",
      "Loss after mini-batch   401: 18.446\n",
      "Loss after mini-batch   501: 18.066\n",
      "Loss after mini-batch   601: 24.812\n",
      "Loss after mini-batch   701: 19.242\n",
      "Loss after mini-batch   801: 19.957\n",
      "Loss after mini-batch   901: 19.457\n",
      "Loss after mini-batch  1001: 22.839\n",
      "Loss after mini-batch  1101: 14.038\n",
      "Loss after mini-batch  1201: 20.379\n",
      "Loss after mini-batch  1301: 15.594\n",
      "Loss after mini-batch  1401: 20.471\n",
      "Loss after mini-batch  1501: 20.076\n",
      "Loss after mini-batch  1601: 15.405\n",
      "Loss after mini-batch  1701: 18.069\n",
      "Avg. loss for epoch 37 18.135\n",
      "Avg. loss for all epochs at epoch 37 20.202\n",
      "Starting epoch 38\n",
      "Loss after mini-batch     1: 0.297\n",
      "Loss after mini-batch   101: 17.697\n",
      "Loss after mini-batch   201: 25.227\n",
      "Loss after mini-batch   301: 22.666\n",
      "Loss after mini-batch   401: 20.002\n",
      "Loss after mini-batch   501: 14.483\n",
      "Loss after mini-batch   601: 14.358\n",
      "Loss after mini-batch   701: 14.985\n",
      "Loss after mini-batch   801: 25.846\n",
      "Loss after mini-batch   901: 19.679\n",
      "Loss after mini-batch  1001: 21.158\n",
      "Loss after mini-batch  1101: 16.830\n",
      "Loss after mini-batch  1201: 15.545\n",
      "Loss after mini-batch  1301: 15.777\n",
      "Loss after mini-batch  1401: 15.009\n",
      "Loss after mini-batch  1501: 15.606\n",
      "Loss after mini-batch  1601: 23.262\n",
      "Loss after mini-batch  1701: 25.243\n",
      "Avg. loss for epoch 38 17.982\n",
      "Avg. loss for all epochs at epoch 38 20.145\n",
      "Starting epoch 39\n",
      "Loss after mini-batch     1: 0.002\n",
      "Loss after mini-batch   101: 20.187\n",
      "Loss after mini-batch   201: 20.825\n",
      "Loss after mini-batch   301: 20.724\n",
      "Loss after mini-batch   401: 18.419\n",
      "Loss after mini-batch   501: 20.641\n",
      "Loss after mini-batch   601: 23.419\n",
      "Loss after mini-batch   701: 13.414\n",
      "Loss after mini-batch   801: 24.756\n",
      "Loss after mini-batch   901: 21.421\n",
      "Loss after mini-batch  1001: 27.074\n",
      "Loss after mini-batch  1101: 13.024\n",
      "Loss after mini-batch  1201: 18.789\n",
      "Loss after mini-batch  1301: 12.161\n",
      "Loss after mini-batch  1401: 16.860\n",
      "Loss after mini-batch  1501: 12.585\n",
      "Loss after mini-batch  1601: 17.262\n",
      "Loss after mini-batch  1701: 22.007\n",
      "Avg. loss for epoch 39 17.976\n",
      "Avg. loss for all epochs at epoch 39 20.091\n",
      "Starting epoch 40\n",
      "Loss after mini-batch     1: 1.136\n",
      "Loss after mini-batch   101: 17.307\n",
      "Loss after mini-batch   201: 17.994\n",
      "Loss after mini-batch   301: 17.067\n",
      "Loss after mini-batch   401: 23.084\n",
      "Loss after mini-batch   501: 10.716\n",
      "Loss after mini-batch   601: 20.004\n",
      "Loss after mini-batch   701: 15.559\n",
      "Loss after mini-batch   801: 21.836\n",
      "Loss after mini-batch   901: 19.695\n",
      "Loss after mini-batch  1001: 15.274\n",
      "Loss after mini-batch  1101: 19.047\n",
      "Loss after mini-batch  1201: 24.962\n",
      "Loss after mini-batch  1301: 16.728\n",
      "Loss after mini-batch  1401: 26.229\n",
      "Loss after mini-batch  1501: 22.811\n",
      "Loss after mini-batch  1601: 13.514\n",
      "Loss after mini-batch  1701: 19.431\n",
      "Avg. loss for epoch 40 17.911\n",
      "Avg. loss for all epochs at epoch 40 20.038\n",
      "Starting epoch 41\n",
      "Loss after mini-batch     1: 0.003\n",
      "Loss after mini-batch   101: 20.193\n",
      "Loss after mini-batch   201: 13.416\n",
      "Loss after mini-batch   301: 18.897\n",
      "Loss after mini-batch   401: 19.528\n",
      "Loss after mini-batch   501: 25.890\n",
      "Loss after mini-batch   601: 16.966\n",
      "Loss after mini-batch   701: 8.343\n",
      "Loss after mini-batch   801: 26.866\n",
      "Loss after mini-batch   901: 24.132\n",
      "Loss after mini-batch  1001: 15.455\n",
      "Loss after mini-batch  1101: 26.650\n",
      "Loss after mini-batch  1201: 23.379\n",
      "Loss after mini-batch  1301: 9.958\n",
      "Loss after mini-batch  1401: 24.382\n",
      "Loss after mini-batch  1501: 17.419\n",
      "Loss after mini-batch  1601: 18.900\n",
      "Loss after mini-batch  1701: 11.800\n",
      "Avg. loss for epoch 41 17.899\n",
      "Avg. loss for all epochs at epoch 41 19.987\n",
      "Starting epoch 42\n",
      "Loss after mini-batch     1: 0.003\n",
      "Loss after mini-batch   101: 21.847\n",
      "Loss after mini-batch   201: 22.185\n",
      "Loss after mini-batch   301: 16.333\n",
      "Loss after mini-batch   401: 17.545\n",
      "Loss after mini-batch   501: 26.199\n",
      "Loss after mini-batch   601: 22.259\n",
      "Loss after mini-batch   701: 21.365\n",
      "Loss after mini-batch   801: 20.392\n",
      "Loss after mini-batch   901: 26.733\n",
      "Loss after mini-batch  1001: 16.273\n",
      "Loss after mini-batch  1101: 15.442\n",
      "Loss after mini-batch  1201: 12.499\n",
      "Loss after mini-batch  1301: 21.319\n",
      "Loss after mini-batch  1401: 17.491\n",
      "Loss after mini-batch  1501: 9.569\n",
      "Loss after mini-batch  1601: 18.464\n",
      "Loss after mini-batch  1701: 16.725\n",
      "Avg. loss for epoch 42 17.925\n",
      "Avg. loss for all epochs at epoch 42 19.939\n",
      "Starting epoch 43\n",
      "Loss after mini-batch     1: 0.011\n",
      "Loss after mini-batch   101: 22.701\n",
      "Loss after mini-batch   201: 14.677\n",
      "Loss after mini-batch   301: 19.430\n",
      "Loss after mini-batch   401: 22.822\n",
      "Loss after mini-batch   501: 13.359\n",
      "Loss after mini-batch   601: 19.576\n",
      "Loss after mini-batch   701: 22.084\n",
      "Loss after mini-batch   801: 23.842\n",
      "Loss after mini-batch   901: 18.286\n",
      "Loss after mini-batch  1001: 13.213\n",
      "Loss after mini-batch  1101: 18.052\n",
      "Loss after mini-batch  1201: 17.543\n",
      "Loss after mini-batch  1301: 11.515\n",
      "Loss after mini-batch  1401: 25.526\n",
      "Loss after mini-batch  1501: 20.990\n",
      "Loss after mini-batch  1601: 14.298\n",
      "Loss after mini-batch  1701: 20.332\n",
      "Avg. loss for epoch 43 17.681\n",
      "Avg. loss for all epochs at epoch 43 19.888\n",
      "Starting epoch 44\n",
      "Loss after mini-batch     1: 0.007\n",
      "Loss after mini-batch   101: 21.575\n",
      "Loss after mini-batch   201: 16.238\n",
      "Loss after mini-batch   301: 20.500\n",
      "Loss after mini-batch   401: 23.058\n",
      "Loss after mini-batch   501: 16.249\n",
      "Loss after mini-batch   601: 19.834\n",
      "Loss after mini-batch   701: 8.733\n",
      "Loss after mini-batch   801: 21.879\n",
      "Loss after mini-batch   901: 14.094\n",
      "Loss after mini-batch  1001: 16.159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch  1101: 22.744\n",
      "Loss after mini-batch  1201: 16.249\n",
      "Loss after mini-batch  1301: 10.007\n",
      "Loss after mini-batch  1401: 18.318\n",
      "Loss after mini-batch  1501: 20.812\n",
      "Loss after mini-batch  1601: 23.852\n",
      "Loss after mini-batch  1701: 29.278\n",
      "Avg. loss for epoch 44 17.755\n",
      "Avg. loss for all epochs at epoch 44 19.840\n",
      "Starting epoch 45\n",
      "Loss after mini-batch     1: 0.040\n",
      "Loss after mini-batch   101: 17.783\n",
      "Loss after mini-batch   201: 20.822\n",
      "Loss after mini-batch   301: 18.234\n",
      "Loss after mini-batch   401: 18.144\n",
      "Loss after mini-batch   501: 18.928\n",
      "Loss after mini-batch   601: 12.984\n",
      "Loss after mini-batch   701: 22.566\n",
      "Loss after mini-batch   801: 11.148\n",
      "Loss after mini-batch   901: 21.163\n",
      "Loss after mini-batch  1001: 13.342\n",
      "Loss after mini-batch  1101: 14.140\n",
      "Loss after mini-batch  1201: 22.315\n",
      "Loss after mini-batch  1301: 15.736\n",
      "Loss after mini-batch  1401: 15.542\n",
      "Loss after mini-batch  1501: 26.485\n",
      "Loss after mini-batch  1601: 19.994\n",
      "Loss after mini-batch  1701: 30.411\n",
      "Avg. loss for epoch 45 17.765\n",
      "Avg. loss for all epochs at epoch 45 19.795\n",
      "Starting epoch 46\n",
      "Loss after mini-batch     1: 0.031\n",
      "Loss after mini-batch   101: 20.341\n",
      "Loss after mini-batch   201: 18.285\n",
      "Loss after mini-batch   301: 16.159\n",
      "Loss after mini-batch   401: 15.256\n",
      "Loss after mini-batch   501: 14.410\n",
      "Loss after mini-batch   601: 19.498\n",
      "Loss after mini-batch   701: 18.746\n",
      "Loss after mini-batch   801: 21.235\n",
      "Loss after mini-batch   901: 15.179\n",
      "Loss after mini-batch  1001: 23.638\n",
      "Loss after mini-batch  1101: 9.897\n",
      "Loss after mini-batch  1201: 16.351\n",
      "Loss after mini-batch  1301: 13.022\n",
      "Loss after mini-batch  1401: 23.476\n",
      "Loss after mini-batch  1501: 22.449\n",
      "Loss after mini-batch  1601: 21.780\n",
      "Loss after mini-batch  1701: 26.437\n",
      "Avg. loss for epoch 46 17.566\n",
      "Avg. loss for all epochs at epoch 46 19.748\n",
      "Starting epoch 47\n",
      "Loss after mini-batch     1: 0.157\n",
      "Loss after mini-batch   101: 19.134\n",
      "Loss after mini-batch   201: 25.680\n",
      "Loss after mini-batch   301: 21.655\n",
      "Loss after mini-batch   401: 21.822\n",
      "Loss after mini-batch   501: 11.368\n",
      "Loss after mini-batch   601: 13.807\n",
      "Loss after mini-batch   701: 20.628\n",
      "Loss after mini-batch   801: 20.248\n",
      "Loss after mini-batch   901: 16.620\n",
      "Loss after mini-batch  1001: 20.735\n",
      "Loss after mini-batch  1101: 13.312\n",
      "Loss after mini-batch  1201: 13.638\n",
      "Loss after mini-batch  1301: 23.359\n",
      "Loss after mini-batch  1401: 20.361\n",
      "Loss after mini-batch  1501: 19.206\n",
      "Loss after mini-batch  1601: 20.293\n",
      "Loss after mini-batch  1701: 11.718\n",
      "Avg. loss for epoch 47 17.430\n",
      "Avg. loss for all epochs at epoch 47 19.699\n",
      "Starting epoch 48\n",
      "Loss after mini-batch     1: 0.148\n",
      "Loss after mini-batch   101: 13.045\n",
      "Loss after mini-batch   201: 23.969\n",
      "Loss after mini-batch   301: 16.651\n",
      "Loss after mini-batch   401: 23.136\n",
      "Loss after mini-batch   501: 11.549\n",
      "Loss after mini-batch   601: 19.802\n",
      "Loss after mini-batch   701: 15.558\n",
      "Loss after mini-batch   801: 22.238\n",
      "Loss after mini-batch   901: 18.090\n",
      "Loss after mini-batch  1001: 14.877\n",
      "Loss after mini-batch  1101: 15.216\n",
      "Loss after mini-batch  1201: 22.627\n",
      "Loss after mini-batch  1301: 15.704\n",
      "Loss after mini-batch  1401: 19.348\n",
      "Loss after mini-batch  1501: 19.968\n",
      "Loss after mini-batch  1601: 18.154\n",
      "Loss after mini-batch  1701: 28.271\n",
      "Avg. loss for epoch 48 17.686\n",
      "Avg. loss for all epochs at epoch 48 19.658\n",
      "Starting epoch 49\n",
      "Loss after mini-batch     1: 0.002\n",
      "Loss after mini-batch   101: 15.901\n",
      "Loss after mini-batch   201: 22.668\n",
      "Loss after mini-batch   301: 22.271\n",
      "Loss after mini-batch   401: 20.033\n",
      "Loss after mini-batch   501: 16.244\n",
      "Loss after mini-batch   601: 14.275\n",
      "Loss after mini-batch   701: 17.281\n",
      "Loss after mini-batch   801: 17.563\n",
      "Loss after mini-batch   901: 25.281\n",
      "Loss after mini-batch  1001: 23.148\n",
      "Loss after mini-batch  1101: 21.262\n",
      "Loss after mini-batch  1201: 14.391\n",
      "Loss after mini-batch  1301: 17.592\n",
      "Loss after mini-batch  1401: 14.558\n",
      "Loss after mini-batch  1501: 20.242\n",
      "Loss after mini-batch  1601: 15.820\n",
      "Loss after mini-batch  1701: 12.287\n",
      "Avg. loss for epoch 49 17.268\n",
      "Avg. loss for all epochs at epoch 49 19.610\n",
      "Starting epoch 50\n",
      "Loss after mini-batch     1: 0.009\n",
      "Loss after mini-batch   101: 15.048\n",
      "Loss after mini-batch   201: 14.788\n",
      "Loss after mini-batch   301: 25.304\n",
      "Loss after mini-batch   401: 14.710\n",
      "Loss after mini-batch   501: 15.559\n",
      "Loss after mini-batch   601: 19.090\n",
      "Loss after mini-batch   701: 15.627\n",
      "Loss after mini-batch   801: 18.126\n",
      "Loss after mini-batch   901: 19.473\n",
      "Loss after mini-batch  1001: 12.944\n",
      "Loss after mini-batch  1101: 28.863\n",
      "Loss after mini-batch  1201: 16.595\n",
      "Loss after mini-batch  1301: 16.059\n",
      "Loss after mini-batch  1401: 17.462\n",
      "Loss after mini-batch  1501: 21.017\n",
      "Loss after mini-batch  1601: 25.603\n",
      "Loss after mini-batch  1701: 17.148\n",
      "Avg. loss for epoch 50 17.412\n",
      "Avg. loss for all epochs at epoch 50 19.567\n",
      "Starting epoch 51\n",
      "Loss after mini-batch     1: 0.003\n",
      "Loss after mini-batch   101: 15.661\n",
      "Loss after mini-batch   201: 16.782\n",
      "Loss after mini-batch   301: 18.966\n",
      "Loss after mini-batch   401: 12.502\n",
      "Loss after mini-batch   501: 18.281\n",
      "Loss after mini-batch   601: 17.873\n",
      "Loss after mini-batch   701: 21.794\n",
      "Loss after mini-batch   801: 19.451\n",
      "Loss after mini-batch   901: 20.932\n",
      "Loss after mini-batch  1001: 19.183\n",
      "Loss after mini-batch  1101: 20.285\n",
      "Loss after mini-batch  1201: 21.252\n",
      "Loss after mini-batch  1301: 8.659\n",
      "Loss after mini-batch  1401: 14.725\n",
      "Loss after mini-batch  1501: 18.596\n",
      "Loss after mini-batch  1601: 16.465\n",
      "Loss after mini-batch  1701: 23.266\n",
      "Avg. loss for epoch 51 16.926\n",
      "Avg. loss for all epochs at epoch 51 19.517\n",
      "Starting epoch 52\n",
      "Loss after mini-batch     1: 0.061\n",
      "Loss after mini-batch   101: 18.845\n",
      "Loss after mini-batch   201: 14.998\n",
      "Loss after mini-batch   301: 15.206\n",
      "Loss after mini-batch   401: 17.224\n",
      "Loss after mini-batch   501: 20.296\n",
      "Loss after mini-batch   601: 22.913\n",
      "Loss after mini-batch   701: 14.761\n",
      "Loss after mini-batch   801: 13.909\n",
      "Loss after mini-batch   901: 22.664\n",
      "Loss after mini-batch  1001: 18.596\n",
      "Loss after mini-batch  1101: 16.462\n",
      "Loss after mini-batch  1201: 21.760\n",
      "Loss after mini-batch  1301: 15.391\n",
      "Loss after mini-batch  1401: 13.346\n",
      "Loss after mini-batch  1501: 19.522\n",
      "Loss after mini-batch  1601: 25.218\n",
      "Loss after mini-batch  1701: 16.101\n",
      "Avg. loss for epoch 52 17.071\n",
      "Avg. loss for all epochs at epoch 52 19.470\n",
      "Starting epoch 53\n",
      "Loss after mini-batch     1: 0.004\n",
      "Loss after mini-batch   101: 30.330\n",
      "Loss after mini-batch   201: 15.643\n",
      "Loss after mini-batch   301: 13.603\n",
      "Loss after mini-batch   401: 16.817\n",
      "Loss after mini-batch   501: 14.401\n",
      "Loss after mini-batch   601: 18.634\n",
      "Loss after mini-batch   701: 15.554\n",
      "Loss after mini-batch   801: 17.337\n",
      "Loss after mini-batch   901: 21.551\n",
      "Loss after mini-batch  1001: 17.873\n",
      "Loss after mini-batch  1101: 19.496\n",
      "Loss after mini-batch  1201: 19.207\n",
      "Loss after mini-batch  1301: 21.686\n",
      "Loss after mini-batch  1401: 22.637\n",
      "Loss after mini-batch  1501: 8.927\n",
      "Loss after mini-batch  1601: 9.658\n",
      "Loss after mini-batch  1701: 16.998\n",
      "Avg. loss for epoch 53 16.686\n",
      "Avg. loss for all epochs at epoch 53 19.419\n",
      "Starting epoch 54\n",
      "Loss after mini-batch     1: 2.017\n",
      "Loss after mini-batch   101: 16.560\n",
      "Loss after mini-batch   201: 20.305\n",
      "Loss after mini-batch   301: 23.561\n",
      "Loss after mini-batch   401: 16.089\n",
      "Loss after mini-batch   501: 15.098\n",
      "Loss after mini-batch   601: 22.154\n",
      "Loss after mini-batch   701: 12.462\n",
      "Loss after mini-batch   801: 16.673\n",
      "Loss after mini-batch   901: 28.242\n",
      "Loss after mini-batch  1001: 18.151\n",
      "Loss after mini-batch  1101: 16.266\n",
      "Loss after mini-batch  1201: 19.078\n",
      "Loss after mini-batch  1301: 16.796\n",
      "Loss after mini-batch  1401: 21.581\n",
      "Loss after mini-batch  1501: 17.100\n",
      "Loss after mini-batch  1601: 8.654\n",
      "Loss after mini-batch  1701: 15.021\n",
      "Avg. loss for epoch 54 16.989\n",
      "Avg. loss for all epochs at epoch 54 19.375\n",
      "Starting epoch 55\n",
      "Loss after mini-batch     1: 0.933\n",
      "Loss after mini-batch   101: 18.218\n",
      "Loss after mini-batch   201: 21.979\n",
      "Loss after mini-batch   301: 14.768\n",
      "Loss after mini-batch   401: 13.887\n",
      "Loss after mini-batch   501: 8.048\n",
      "Loss after mini-batch   601: 14.265\n",
      "Loss after mini-batch   701: 18.606\n",
      "Loss after mini-batch   801: 18.592\n",
      "Loss after mini-batch   901: 23.752\n",
      "Loss after mini-batch  1001: 19.202\n",
      "Loss after mini-batch  1101: 16.979\n",
      "Loss after mini-batch  1201: 14.234\n",
      "Loss after mini-batch  1301: 10.794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch  1401: 24.388\n",
      "Loss after mini-batch  1501: 19.697\n",
      "Loss after mini-batch  1601: 18.910\n",
      "Loss after mini-batch  1701: 23.209\n",
      "Avg. loss for epoch 55 16.692\n",
      "Avg. loss for all epochs at epoch 55 19.327\n",
      "Starting epoch 56\n",
      "Loss after mini-batch     1: 0.022\n",
      "Loss after mini-batch   101: 19.222\n",
      "Loss after mini-batch   201: 18.143\n",
      "Loss after mini-batch   301: 14.755\n",
      "Loss after mini-batch   401: 19.313\n",
      "Loss after mini-batch   501: 13.631\n",
      "Loss after mini-batch   601: 17.111\n",
      "Loss after mini-batch   701: 23.658\n",
      "Loss after mini-batch   801: 24.414\n",
      "Loss after mini-batch   901: 31.735\n",
      "Loss after mini-batch  1001: 13.058\n",
      "Loss after mini-batch  1101: 23.508\n",
      "Loss after mini-batch  1201: 19.986\n",
      "Loss after mini-batch  1301: 12.315\n",
      "Loss after mini-batch  1401: 18.692\n",
      "Loss after mini-batch  1501: 16.773\n",
      "Loss after mini-batch  1601: 9.862\n",
      "Loss after mini-batch  1701: 13.068\n",
      "Avg. loss for epoch 56 17.181\n",
      "Avg. loss for all epochs at epoch 56 19.289\n",
      "Starting epoch 57\n",
      "Loss after mini-batch     1: 0.006\n",
      "Loss after mini-batch   101: 19.101\n",
      "Loss after mini-batch   201: 25.019\n",
      "Loss after mini-batch   301: 10.335\n",
      "Loss after mini-batch   401: 24.370\n",
      "Loss after mini-batch   501: 23.995\n",
      "Loss after mini-batch   601: 21.063\n",
      "Loss after mini-batch   701: 15.910\n",
      "Loss after mini-batch   801: 15.098\n",
      "Loss after mini-batch   901: 20.611\n",
      "Loss after mini-batch  1001: 11.879\n",
      "Loss after mini-batch  1101: 11.730\n",
      "Loss after mini-batch  1201: 21.244\n",
      "Loss after mini-batch  1301: 13.913\n",
      "Loss after mini-batch  1401: 18.506\n",
      "Loss after mini-batch  1501: 12.856\n",
      "Loss after mini-batch  1601: 17.811\n",
      "Loss after mini-batch  1701: 15.646\n",
      "Avg. loss for epoch 57 16.616\n",
      "Avg. loss for all epochs at epoch 57 19.243\n",
      "Starting epoch 58\n",
      "Loss after mini-batch     1: 0.004\n",
      "Loss after mini-batch   101: 15.315\n",
      "Loss after mini-batch   201: 27.351\n",
      "Loss after mini-batch   301: 16.194\n",
      "Loss after mini-batch   401: 11.709\n",
      "Loss after mini-batch   501: 13.276\n",
      "Loss after mini-batch   601: 12.646\n",
      "Loss after mini-batch   701: 15.525\n",
      "Loss after mini-batch   801: 21.093\n",
      "Loss after mini-batch   901: 18.890\n",
      "Loss after mini-batch  1001: 18.178\n",
      "Loss after mini-batch  1101: 12.904\n",
      "Loss after mini-batch  1201: 22.288\n",
      "Loss after mini-batch  1301: 15.313\n",
      "Loss after mini-batch  1401: 14.795\n",
      "Loss after mini-batch  1501: 18.335\n",
      "Loss after mini-batch  1601: 19.872\n",
      "Loss after mini-batch  1701: 26.449\n",
      "Avg. loss for epoch 58 16.674\n",
      "Avg. loss for all epochs at epoch 58 19.200\n",
      "Starting epoch 59\n",
      "Loss after mini-batch     1: 0.014\n",
      "Loss after mini-batch   101: 12.211\n",
      "Loss after mini-batch   201: 13.964\n",
      "Loss after mini-batch   301: 18.490\n",
      "Loss after mini-batch   401: 23.626\n",
      "Loss after mini-batch   501: 20.406\n",
      "Loss after mini-batch   601: 16.585\n",
      "Loss after mini-batch   701: 18.961\n",
      "Loss after mini-batch   801: 27.139\n",
      "Loss after mini-batch   901: 16.970\n",
      "Loss after mini-batch  1001: 13.821\n",
      "Loss after mini-batch  1101: 13.721\n",
      "Loss after mini-batch  1201: 17.739\n",
      "Loss after mini-batch  1301: 13.554\n",
      "Loss after mini-batch  1401: 26.909\n",
      "Loss after mini-batch  1501: 16.977\n",
      "Loss after mini-batch  1601: 12.569\n",
      "Loss after mini-batch  1701: 18.141\n",
      "Avg. loss for epoch 59 16.767\n",
      "Avg. loss for all epochs at epoch 59 19.159\n",
      "Starting epoch 60\n",
      "Loss after mini-batch     1: 0.006\n",
      "Loss after mini-batch   101: 25.650\n",
      "Loss after mini-batch   201: 19.674\n",
      "Loss after mini-batch   301: 22.232\n",
      "Loss after mini-batch   401: 19.029\n",
      "Loss after mini-batch   501: 19.275\n",
      "Loss after mini-batch   601: 10.801\n",
      "Loss after mini-batch   701: 21.009\n",
      "Loss after mini-batch   801: 21.932\n",
      "Loss after mini-batch   901: 15.972\n",
      "Loss after mini-batch  1001: 16.198\n",
      "Loss after mini-batch  1101: 12.537\n",
      "Loss after mini-batch  1201: 10.782\n",
      "Loss after mini-batch  1301: 18.255\n",
      "Loss after mini-batch  1401: 22.372\n",
      "Loss after mini-batch  1501: 16.263\n",
      "Loss after mini-batch  1601: 12.998\n",
      "Loss after mini-batch  1701: 15.018\n",
      "Avg. loss for epoch 60 16.667\n",
      "Avg. loss for all epochs at epoch 60 19.118\n",
      "Starting epoch 61\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   101: 13.927\n",
      "Loss after mini-batch   201: 12.307\n",
      "Loss after mini-batch   301: 20.976\n",
      "Loss after mini-batch   401: 11.189\n",
      "Loss after mini-batch   501: 19.477\n",
      "Loss after mini-batch   601: 22.695\n",
      "Loss after mini-batch   701: 17.877\n",
      "Loss after mini-batch   801: 12.782\n",
      "Loss after mini-batch   901: 9.096\n",
      "Loss after mini-batch  1001: 15.421\n",
      "Loss after mini-batch  1101: 22.964\n",
      "Loss after mini-batch  1201: 19.465\n",
      "Loss after mini-batch  1301: 17.283\n",
      "Loss after mini-batch  1401: 16.993\n",
      "Loss after mini-batch  1501: 23.366\n",
      "Loss after mini-batch  1601: 23.744\n",
      "Loss after mini-batch  1701: 17.155\n",
      "Avg. loss for epoch 61 16.485\n",
      "Avg. loss for all epochs at epoch 61 19.076\n",
      "Starting epoch 62\n",
      "Loss after mini-batch     1: 2.243\n",
      "Loss after mini-batch   101: 14.896\n",
      "Loss after mini-batch   201: 17.367\n",
      "Loss after mini-batch   301: 18.942\n",
      "Loss after mini-batch   401: 18.569\n",
      "Loss after mini-batch   501: 15.187\n",
      "Loss after mini-batch   601: 12.905\n",
      "Loss after mini-batch   701: 23.704\n",
      "Loss after mini-batch   801: 13.645\n",
      "Loss after mini-batch   901: 15.912\n",
      "Loss after mini-batch  1001: 12.816\n",
      "Loss after mini-batch  1101: 18.107\n",
      "Loss after mini-batch  1201: 23.581\n",
      "Loss after mini-batch  1301: 12.008\n",
      "Loss after mini-batch  1401: 20.613\n",
      "Loss after mini-batch  1501: 19.847\n",
      "Loss after mini-batch  1601: 13.503\n",
      "Loss after mini-batch  1701: 18.476\n",
      "Avg. loss for epoch 62 16.240\n",
      "Avg. loss for all epochs at epoch 62 19.031\n",
      "Starting epoch 63\n",
      "Loss after mini-batch     1: 0.012\n",
      "Loss after mini-batch   101: 14.447\n",
      "Loss after mini-batch   201: 19.442\n",
      "Loss after mini-batch   301: 16.466\n",
      "Loss after mini-batch   401: 19.116\n",
      "Loss after mini-batch   501: 16.657\n",
      "Loss after mini-batch   601: 15.752\n",
      "Loss after mini-batch   701: 24.659\n",
      "Loss after mini-batch   801: 15.196\n",
      "Loss after mini-batch   901: 23.720\n",
      "Loss after mini-batch  1001: 17.462\n",
      "Loss after mini-batch  1101: 19.131\n",
      "Loss after mini-batch  1201: 10.188\n",
      "Loss after mini-batch  1301: 16.860\n",
      "Loss after mini-batch  1401: 19.769\n",
      "Loss after mini-batch  1501: 19.727\n",
      "Loss after mini-batch  1601: 12.487\n",
      "Loss after mini-batch  1701: 15.750\n",
      "Avg. loss for epoch 63 16.491\n",
      "Avg. loss for all epochs at epoch 63 18.991\n",
      "Starting epoch 64\n",
      "Loss after mini-batch     1: 0.194\n",
      "Loss after mini-batch   101: 10.220\n",
      "Loss after mini-batch   201: 20.251\n",
      "Loss after mini-batch   301: 20.553\n",
      "Loss after mini-batch   401: 12.472\n",
      "Loss after mini-batch   501: 18.163\n",
      "Loss after mini-batch   601: 18.081\n",
      "Loss after mini-batch   701: 19.616\n",
      "Loss after mini-batch   801: 25.041\n",
      "Loss after mini-batch   901: 21.191\n",
      "Loss after mini-batch  1001: 13.746\n",
      "Loss after mini-batch  1101: 15.769\n",
      "Loss after mini-batch  1201: 17.567\n",
      "Loss after mini-batch  1301: 19.378\n",
      "Loss after mini-batch  1401: 13.090\n",
      "Loss after mini-batch  1501: 15.586\n",
      "Loss after mini-batch  1601: 12.775\n",
      "Loss after mini-batch  1701: 16.324\n",
      "Avg. loss for epoch 64 16.112\n",
      "Avg. loss for all epochs at epoch 64 18.947\n",
      "Starting epoch 65\n",
      "Loss after mini-batch     1: 0.035\n",
      "Loss after mini-batch   101: 21.214\n",
      "Loss after mini-batch   201: 14.708\n",
      "Loss after mini-batch   301: 17.319\n",
      "Loss after mini-batch   401: 17.347\n",
      "Loss after mini-batch   501: 14.763\n",
      "Loss after mini-batch   601: 13.333\n",
      "Loss after mini-batch   701: 17.590\n",
      "Loss after mini-batch   801: 19.049\n",
      "Loss after mini-batch   901: 11.861\n",
      "Loss after mini-batch  1001: 17.027\n",
      "Loss after mini-batch  1101: 25.627\n",
      "Loss after mini-batch  1201: 17.945\n",
      "Loss after mini-batch  1301: 15.786\n",
      "Loss after mini-batch  1401: 15.747\n",
      "Loss after mini-batch  1501: 18.290\n",
      "Loss after mini-batch  1601: 19.781\n",
      "Loss after mini-batch  1701: 13.479\n",
      "Avg. loss for epoch 65 16.161\n",
      "Avg. loss for all epochs at epoch 65 18.904\n",
      "Starting epoch 66\n",
      "Loss after mini-batch     1: 0.108\n",
      "Loss after mini-batch   101: 14.613\n",
      "Loss after mini-batch   201: 14.325\n",
      "Loss after mini-batch   301: 12.942\n",
      "Loss after mini-batch   401: 17.042\n",
      "Loss after mini-batch   501: 20.833\n",
      "Loss after mini-batch   601: 11.671\n",
      "Loss after mini-batch   701: 18.891\n",
      "Loss after mini-batch   801: 19.814\n",
      "Loss after mini-batch   901: 11.994\n",
      "Loss after mini-batch  1001: 25.377\n",
      "Loss after mini-batch  1101: 17.472\n",
      "Loss after mini-batch  1201: 21.513\n",
      "Loss after mini-batch  1301: 13.538\n",
      "Loss after mini-batch  1401: 14.330\n",
      "Loss after mini-batch  1501: 18.740\n",
      "Loss after mini-batch  1601: 22.182\n",
      "Loss after mini-batch  1701: 17.418\n",
      "Avg. loss for epoch 66 16.267\n",
      "Avg. loss for all epochs at epoch 66 18.865\n",
      "Starting epoch 67\n",
      "Loss after mini-batch     1: 0.134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   101: 18.765\n",
      "Loss after mini-batch   201: 10.924\n",
      "Loss after mini-batch   301: 21.522\n",
      "Loss after mini-batch   401: 11.077\n",
      "Loss after mini-batch   501: 12.452\n",
      "Loss after mini-batch   601: 16.351\n",
      "Loss after mini-batch   701: 19.149\n",
      "Loss after mini-batch   801: 16.061\n",
      "Loss after mini-batch   901: 7.333\n",
      "Loss after mini-batch  1001: 25.345\n",
      "Loss after mini-batch  1101: 22.478\n",
      "Loss after mini-batch  1201: 18.328\n",
      "Loss after mini-batch  1301: 18.678\n",
      "Loss after mini-batch  1401: 13.901\n",
      "Loss after mini-batch  1501: 17.977\n",
      "Loss after mini-batch  1601: 19.763\n",
      "Loss after mini-batch  1701: 23.167\n",
      "Avg. loss for epoch 67 16.300\n",
      "Avg. loss for all epochs at epoch 67 18.827\n",
      "Starting epoch 68\n",
      "Loss after mini-batch     1: 0.014\n",
      "Loss after mini-batch   101: 12.847\n",
      "Loss after mini-batch   201: 14.177\n",
      "Loss after mini-batch   301: 17.632\n",
      "Loss after mini-batch   401: 22.948\n",
      "Loss after mini-batch   501: 16.361\n",
      "Loss after mini-batch   601: 19.059\n",
      "Loss after mini-batch   701: 18.519\n",
      "Loss after mini-batch   801: 16.989\n",
      "Loss after mini-batch   901: 19.760\n",
      "Loss after mini-batch  1001: 16.346\n",
      "Loss after mini-batch  1101: 24.632\n",
      "Loss after mini-batch  1201: 13.470\n",
      "Loss after mini-batch  1301: 17.396\n",
      "Loss after mini-batch  1401: 16.381\n",
      "Loss after mini-batch  1501: 14.410\n",
      "Loss after mini-batch  1601: 12.667\n",
      "Loss after mini-batch  1701: 17.097\n",
      "Avg. loss for epoch 68 16.150\n",
      "Avg. loss for all epochs at epoch 68 18.789\n",
      "Starting epoch 69\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   101: 22.112\n",
      "Loss after mini-batch   201: 16.581\n",
      "Loss after mini-batch   301: 20.023\n",
      "Loss after mini-batch   401: 21.959\n",
      "Loss after mini-batch   501: 15.723\n",
      "Loss after mini-batch   601: 15.423\n",
      "Loss after mini-batch   701: 17.770\n",
      "Loss after mini-batch   801: 14.853\n",
      "Loss after mini-batch   901: 16.232\n",
      "Loss after mini-batch  1001: 14.002\n",
      "Loss after mini-batch  1101: 15.343\n",
      "Loss after mini-batch  1201: 24.849\n",
      "Loss after mini-batch  1301: 12.703\n",
      "Loss after mini-batch  1401: 15.758\n",
      "Loss after mini-batch  1501: 14.101\n",
      "Loss after mini-batch  1601: 15.709\n",
      "Loss after mini-batch  1701: 17.054\n",
      "Avg. loss for epoch 69 16.123\n",
      "Avg. loss for all epochs at epoch 69 18.751\n",
      "Starting epoch 70\n",
      "Loss after mini-batch     1: 1.975\n",
      "Loss after mini-batch   101: 29.172\n",
      "Loss after mini-batch   201: 19.804\n",
      "Loss after mini-batch   301: 12.339\n",
      "Loss after mini-batch   401: 23.531\n",
      "Loss after mini-batch   501: 9.538\n",
      "Loss after mini-batch   601: 12.845\n",
      "Loss after mini-batch   701: 15.859\n",
      "Loss after mini-batch   801: 16.795\n",
      "Loss after mini-batch   901: 16.068\n",
      "Loss after mini-batch  1001: 20.067\n",
      "Loss after mini-batch  1101: 13.073\n",
      "Loss after mini-batch  1201: 12.708\n",
      "Loss after mini-batch  1301: 21.402\n",
      "Loss after mini-batch  1401: 17.345\n",
      "Loss after mini-batch  1501: 13.632\n",
      "Loss after mini-batch  1601: 18.725\n",
      "Loss after mini-batch  1701: 14.194\n",
      "Avg. loss for epoch 70 16.059\n",
      "Avg. loss for all epochs at epoch 70 18.713\n",
      "Starting epoch 71\n",
      "Loss after mini-batch     1: 0.063\n",
      "Loss after mini-batch   101: 12.039\n",
      "Loss after mini-batch   201: 15.371\n",
      "Loss after mini-batch   301: 14.207\n",
      "Loss after mini-batch   401: 20.099\n",
      "Loss after mini-batch   501: 18.790\n",
      "Loss after mini-batch   601: 17.869\n",
      "Loss after mini-batch   701: 17.062\n",
      "Loss after mini-batch   801: 11.808\n",
      "Loss after mini-batch   901: 18.765\n",
      "Loss after mini-batch  1001: 19.529\n",
      "Loss after mini-batch  1101: 20.521\n",
      "Loss after mini-batch  1201: 12.977\n",
      "Loss after mini-batch  1301: 21.649\n",
      "Loss after mini-batch  1401: 11.728\n",
      "Loss after mini-batch  1501: 17.784\n",
      "Loss after mini-batch  1601: 16.078\n",
      "Loss after mini-batch  1701: 22.233\n",
      "Avg. loss for epoch 71 16.032\n",
      "Avg. loss for all epochs at epoch 71 18.675\n",
      "Starting epoch 72\n",
      "Loss after mini-batch     1: 0.023\n",
      "Loss after mini-batch   101: 20.860\n",
      "Loss after mini-batch   201: 21.110\n",
      "Loss after mini-batch   301: 18.128\n",
      "Loss after mini-batch   401: 26.257\n",
      "Loss after mini-batch   501: 17.251\n",
      "Loss after mini-batch   601: 22.959\n",
      "Loss after mini-batch   701: 16.144\n",
      "Loss after mini-batch   801: 11.292\n",
      "Loss after mini-batch   901: 13.366\n",
      "Loss after mini-batch  1001: 19.365\n",
      "Loss after mini-batch  1101: 18.623\n",
      "Loss after mini-batch  1201: 17.035\n",
      "Loss after mini-batch  1301: 17.914\n",
      "Loss after mini-batch  1401: 9.572\n",
      "Loss after mini-batch  1501: 12.511\n",
      "Loss after mini-batch  1601: 11.529\n",
      "Loss after mini-batch  1701: 15.266\n",
      "Avg. loss for epoch 72 16.067\n",
      "Avg. loss for all epochs at epoch 72 18.640\n",
      "Starting epoch 73\n",
      "Loss after mini-batch     1: 0.002\n",
      "Loss after mini-batch   101: 10.810\n",
      "Loss after mini-batch   201: 21.777\n",
      "Loss after mini-batch   301: 18.782\n",
      "Loss after mini-batch   401: 15.340\n",
      "Loss after mini-batch   501: 22.858\n",
      "Loss after mini-batch   601: 14.266\n",
      "Loss after mini-batch   701: 17.623\n",
      "Loss after mini-batch   801: 17.471\n",
      "Loss after mini-batch   901: 10.012\n",
      "Loss after mini-batch  1001: 19.416\n",
      "Loss after mini-batch  1101: 16.403\n",
      "Loss after mini-batch  1201: 21.742\n",
      "Loss after mini-batch  1301: 12.228\n",
      "Loss after mini-batch  1401: 23.107\n",
      "Loss after mini-batch  1501: 13.844\n",
      "Loss after mini-batch  1601: 13.741\n",
      "Loss after mini-batch  1701: 16.903\n",
      "Avg. loss for epoch 73 15.907\n",
      "Avg. loss for all epochs at epoch 73 18.603\n",
      "Starting epoch 74\n",
      "Loss after mini-batch     1: 0.004\n",
      "Loss after mini-batch   101: 14.277\n",
      "Loss after mini-batch   201: 25.419\n",
      "Loss after mini-batch   301: 14.842\n",
      "Loss after mini-batch   401: 15.519\n",
      "Loss after mini-batch   501: 15.867\n",
      "Loss after mini-batch   601: 16.587\n",
      "Loss after mini-batch   701: 15.723\n",
      "Loss after mini-batch   801: 17.649\n",
      "Loss after mini-batch   901: 10.271\n",
      "Loss after mini-batch  1001: 14.056\n",
      "Loss after mini-batch  1101: 13.464\n",
      "Loss after mini-batch  1201: 11.998\n",
      "Loss after mini-batch  1301: 21.483\n",
      "Loss after mini-batch  1401: 24.220\n",
      "Loss after mini-batch  1501: 13.396\n",
      "Loss after mini-batch  1601: 19.337\n",
      "Loss after mini-batch  1701: 20.167\n",
      "Avg. loss for epoch 74 15.793\n",
      "Avg. loss for all epochs at epoch 74 18.565\n",
      "Starting epoch 75\n",
      "Loss after mini-batch     1: 0.011\n",
      "Loss after mini-batch   101: 10.632\n",
      "Loss after mini-batch   201: 11.544\n",
      "Loss after mini-batch   301: 24.034\n",
      "Loss after mini-batch   401: 18.150\n",
      "Loss after mini-batch   501: 10.950\n",
      "Loss after mini-batch   601: 16.704\n",
      "Loss after mini-batch   701: 16.445\n",
      "Loss after mini-batch   801: 24.604\n",
      "Loss after mini-batch   901: 12.349\n",
      "Loss after mini-batch  1001: 12.067\n",
      "Loss after mini-batch  1101: 14.041\n",
      "Loss after mini-batch  1201: 13.671\n",
      "Loss after mini-batch  1301: 22.300\n",
      "Loss after mini-batch  1401: 26.000\n",
      "Loss after mini-batch  1501: 16.520\n",
      "Loss after mini-batch  1601: 16.592\n",
      "Loss after mini-batch  1701: 12.437\n",
      "Avg. loss for epoch 75 15.503\n",
      "Avg. loss for all epochs at epoch 75 18.525\n",
      "Starting epoch 76\n",
      "Loss after mini-batch     1: 0.020\n",
      "Loss after mini-batch   101: 16.533\n",
      "Loss after mini-batch   201: 18.188\n",
      "Loss after mini-batch   301: 11.989\n",
      "Loss after mini-batch   401: 21.892\n",
      "Loss after mini-batch   501: 10.071\n",
      "Loss after mini-batch   601: 11.581\n",
      "Loss after mini-batch   701: 13.575\n",
      "Loss after mini-batch   801: 19.904\n",
      "Loss after mini-batch   901: 16.620\n",
      "Loss after mini-batch  1001: 15.843\n",
      "Loss after mini-batch  1101: 12.799\n",
      "Loss after mini-batch  1201: 18.494\n",
      "Loss after mini-batch  1301: 18.522\n",
      "Loss after mini-batch  1401: 17.968\n",
      "Loss after mini-batch  1501: 18.886\n",
      "Loss after mini-batch  1601: 15.106\n",
      "Loss after mini-batch  1701: 19.227\n",
      "Avg. loss for epoch 76 15.401\n",
      "Avg. loss for all epochs at epoch 76 18.484\n",
      "Starting epoch 77\n",
      "Loss after mini-batch     1: 0.004\n",
      "Loss after mini-batch   101: 20.443\n",
      "Loss after mini-batch   201: 15.577\n",
      "Loss after mini-batch   301: 18.796\n",
      "Loss after mini-batch   401: 12.757\n",
      "Loss after mini-batch   501: 15.493\n",
      "Loss after mini-batch   601: 11.657\n",
      "Loss after mini-batch   701: 13.896\n",
      "Loss after mini-batch   801: 14.615\n",
      "Loss after mini-batch   901: 18.294\n",
      "Loss after mini-batch  1001: 16.554\n",
      "Loss after mini-batch  1101: 8.837\n",
      "Loss after mini-batch  1201: 13.834\n",
      "Loss after mini-batch  1301: 21.438\n",
      "Loss after mini-batch  1401: 27.701\n",
      "Loss after mini-batch  1501: 12.538\n",
      "Loss after mini-batch  1601: 20.971\n",
      "Loss after mini-batch  1701: 17.030\n",
      "Avg. loss for epoch 77 15.580\n",
      "Avg. loss for all epochs at epoch 77 18.447\n",
      "Starting epoch 78\n",
      "Loss after mini-batch     1: 0.006\n",
      "Loss after mini-batch   101: 14.304\n",
      "Loss after mini-batch   201: 16.295\n",
      "Loss after mini-batch   301: 17.198\n",
      "Loss after mini-batch   401: 22.326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   501: 19.195\n",
      "Loss after mini-batch   601: 6.958\n",
      "Loss after mini-batch   701: 23.030\n",
      "Loss after mini-batch   801: 17.669\n",
      "Loss after mini-batch   901: 11.742\n",
      "Loss after mini-batch  1001: 14.734\n",
      "Loss after mini-batch  1101: 11.906\n",
      "Loss after mini-batch  1201: 10.191\n",
      "Loss after mini-batch  1301: 12.210\n",
      "Loss after mini-batch  1401: 15.504\n",
      "Loss after mini-batch  1501: 16.411\n",
      "Loss after mini-batch  1601: 19.814\n",
      "Loss after mini-batch  1701: 20.891\n",
      "Avg. loss for epoch 78 15.021\n",
      "Avg. loss for all epochs at epoch 78 18.404\n",
      "Starting epoch 79\n",
      "Loss after mini-batch     1: 0.003\n",
      "Loss after mini-batch   101: 7.190\n",
      "Loss after mini-batch   201: 16.324\n",
      "Loss after mini-batch   301: 14.416\n",
      "Loss after mini-batch   401: 7.808\n",
      "Loss after mini-batch   501: 14.425\n",
      "Loss after mini-batch   601: 19.434\n",
      "Loss after mini-batch   701: 16.446\n",
      "Loss after mini-batch   801: 14.529\n",
      "Loss after mini-batch   901: 24.110\n",
      "Loss after mini-batch  1001: 20.701\n",
      "Loss after mini-batch  1101: 14.285\n",
      "Loss after mini-batch  1201: 22.510\n",
      "Loss after mini-batch  1301: 11.299\n",
      "Loss after mini-batch  1401: 20.593\n",
      "Loss after mini-batch  1501: 15.445\n",
      "Loss after mini-batch  1601: 19.840\n",
      "Loss after mini-batch  1701: 17.792\n",
      "Avg. loss for epoch 79 15.397\n",
      "Avg. loss for all epochs at epoch 79 18.366\n",
      "Starting epoch 80\n",
      "Loss after mini-batch     1: 0.004\n",
      "Loss after mini-batch   101: 18.482\n",
      "Loss after mini-batch   201: 17.688\n",
      "Loss after mini-batch   301: 20.171\n",
      "Loss after mini-batch   401: 16.027\n",
      "Loss after mini-batch   501: 11.685\n",
      "Loss after mini-batch   601: 10.673\n",
      "Loss after mini-batch   701: 18.850\n",
      "Loss after mini-batch   801: 12.215\n",
      "Loss after mini-batch   901: 15.209\n",
      "Loss after mini-batch  1001: 21.233\n",
      "Loss after mini-batch  1101: 16.662\n",
      "Loss after mini-batch  1201: 11.574\n",
      "Loss after mini-batch  1301: 10.842\n",
      "Loss after mini-batch  1401: 22.594\n",
      "Loss after mini-batch  1501: 20.480\n",
      "Loss after mini-batch  1601: 14.797\n",
      "Loss after mini-batch  1701: 15.854\n",
      "Avg. loss for epoch 80 15.280\n",
      "Avg. loss for all epochs at epoch 80 18.328\n",
      "Starting epoch 81\n",
      "Loss after mini-batch     1: 0.003\n",
      "Loss after mini-batch   101: 15.953\n",
      "Loss after mini-batch   201: 13.909\n",
      "Loss after mini-batch   301: 23.129\n",
      "Loss after mini-batch   401: 12.954\n",
      "Loss after mini-batch   501: 17.268\n",
      "Loss after mini-batch   601: 17.339\n",
      "Loss after mini-batch   701: 25.755\n",
      "Loss after mini-batch   801: 8.206\n",
      "Loss after mini-batch   901: 18.646\n",
      "Loss after mini-batch  1001: 15.395\n",
      "Loss after mini-batch  1101: 12.512\n",
      "Loss after mini-batch  1201: 19.195\n",
      "Loss after mini-batch  1301: 15.193\n",
      "Loss after mini-batch  1401: 10.774\n",
      "Loss after mini-batch  1501: 15.763\n",
      "Loss after mini-batch  1601: 17.606\n",
      "Loss after mini-batch  1701: 15.191\n",
      "Avg. loss for epoch 81 15.266\n",
      "Avg. loss for all epochs at epoch 81 18.291\n",
      "Starting epoch 82\n",
      "Loss after mini-batch     1: 0.935\n",
      "Loss after mini-batch   101: 17.169\n",
      "Loss after mini-batch   201: 16.143\n",
      "Loss after mini-batch   301: 20.546\n",
      "Loss after mini-batch   401: 20.347\n",
      "Loss after mini-batch   501: 11.740\n",
      "Loss after mini-batch   601: 18.476\n",
      "Loss after mini-batch   701: 16.071\n",
      "Loss after mini-batch   801: 21.220\n",
      "Loss after mini-batch   901: 17.959\n",
      "Loss after mini-batch  1001: 10.166\n",
      "Loss after mini-batch  1101: 13.252\n",
      "Loss after mini-batch  1201: 10.508\n",
      "Loss after mini-batch  1301: 12.664\n",
      "Loss after mini-batch  1401: 15.231\n",
      "Loss after mini-batch  1501: 22.420\n",
      "Loss after mini-batch  1601: 13.275\n",
      "Loss after mini-batch  1701: 16.088\n",
      "Avg. loss for epoch 82 15.234\n",
      "Avg. loss for all epochs at epoch 82 18.254\n",
      "Starting epoch 83\n",
      "Loss after mini-batch     1: 0.009\n",
      "Loss after mini-batch   101: 18.782\n",
      "Loss after mini-batch   201: 21.412\n",
      "Loss after mini-batch   301: 11.738\n",
      "Loss after mini-batch   401: 16.081\n",
      "Loss after mini-batch   501: 15.547\n",
      "Loss after mini-batch   601: 14.856\n",
      "Loss after mini-batch   701: 18.686\n",
      "Loss after mini-batch   801: 13.033\n",
      "Loss after mini-batch   901: 13.735\n",
      "Loss after mini-batch  1001: 14.860\n",
      "Loss after mini-batch  1101: 15.971\n",
      "Loss after mini-batch  1201: 13.982\n",
      "Loss after mini-batch  1301: 19.101\n",
      "Loss after mini-batch  1401: 15.404\n",
      "Loss after mini-batch  1501: 15.743\n",
      "Loss after mini-batch  1601: 12.819\n",
      "Loss after mini-batch  1701: 19.710\n",
      "Avg. loss for epoch 83 15.082\n",
      "Avg. loss for all epochs at epoch 83 18.216\n",
      "Starting epoch 84\n",
      "Loss after mini-batch     1: 0.013\n",
      "Loss after mini-batch   101: 15.981\n",
      "Loss after mini-batch   201: 18.097\n",
      "Loss after mini-batch   301: 15.243\n",
      "Loss after mini-batch   401: 18.570\n",
      "Loss after mini-batch   501: 14.005\n",
      "Loss after mini-batch   601: 17.270\n",
      "Loss after mini-batch   701: 9.433\n",
      "Loss after mini-batch   801: 18.135\n",
      "Loss after mini-batch   901: 13.949\n",
      "Loss after mini-batch  1001: 15.357\n",
      "Loss after mini-batch  1101: 9.232\n",
      "Loss after mini-batch  1201: 16.538\n",
      "Loss after mini-batch  1301: 16.436\n",
      "Loss after mini-batch  1401: 12.650\n",
      "Loss after mini-batch  1501: 18.812\n",
      "Loss after mini-batch  1601: 20.949\n",
      "Loss after mini-batch  1701: 18.283\n",
      "Avg. loss for epoch 84 14.942\n",
      "Avg. loss for all epochs at epoch 84 18.178\n",
      "Starting epoch 85\n",
      "Loss after mini-batch     1: 0.657\n",
      "Loss after mini-batch   101: 24.215\n",
      "Loss after mini-batch   201: 16.128\n",
      "Loss after mini-batch   301: 23.492\n",
      "Loss after mini-batch   401: 12.723\n",
      "Loss after mini-batch   501: 17.033\n",
      "Loss after mini-batch   601: 11.163\n",
      "Loss after mini-batch   701: 12.707\n",
      "Loss after mini-batch   801: 11.776\n",
      "Loss after mini-batch   901: 15.956\n",
      "Loss after mini-batch  1001: 14.389\n",
      "Loss after mini-batch  1101: 24.702\n",
      "Loss after mini-batch  1201: 7.698\n",
      "Loss after mini-batch  1301: 15.614\n",
      "Loss after mini-batch  1401: 17.362\n",
      "Loss after mini-batch  1501: 22.698\n",
      "Loss after mini-batch  1601: 10.241\n",
      "Loss after mini-batch  1701: 9.103\n",
      "Avg. loss for epoch 85 14.870\n",
      "Avg. loss for all epochs at epoch 85 18.139\n",
      "Starting epoch 86\n",
      "Loss after mini-batch     1: 0.002\n",
      "Loss after mini-batch   101: 12.559\n",
      "Loss after mini-batch   201: 25.113\n",
      "Loss after mini-batch   301: 12.461\n",
      "Loss after mini-batch   401: 18.216\n",
      "Loss after mini-batch   501: 23.103\n",
      "Loss after mini-batch   601: 17.945\n",
      "Loss after mini-batch   701: 17.742\n",
      "Loss after mini-batch   801: 11.407\n",
      "Loss after mini-batch   901: 9.561\n",
      "Loss after mini-batch  1001: 17.006\n",
      "Loss after mini-batch  1101: 15.304\n",
      "Loss after mini-batch  1201: 16.320\n",
      "Loss after mini-batch  1301: 20.507\n",
      "Loss after mini-batch  1401: 8.705\n",
      "Loss after mini-batch  1501: 14.420\n",
      "Loss after mini-batch  1601: 13.279\n",
      "Loss after mini-batch  1701: 13.242\n",
      "Avg. loss for epoch 86 14.827\n",
      "Avg. loss for all epochs at epoch 86 18.101\n",
      "Starting epoch 87\n",
      "Loss after mini-batch     1: 0.023\n",
      "Loss after mini-batch   101: 17.950\n",
      "Loss after mini-batch   201: 16.238\n",
      "Loss after mini-batch   301: 12.725\n",
      "Loss after mini-batch   401: 9.524\n",
      "Loss after mini-batch   501: 22.616\n",
      "Loss after mini-batch   601: 12.975\n",
      "Loss after mini-batch   701: 15.614\n",
      "Loss after mini-batch   801: 10.071\n",
      "Loss after mini-batch   901: 17.648\n",
      "Loss after mini-batch  1001: 20.773\n",
      "Loss after mini-batch  1101: 17.155\n",
      "Loss after mini-batch  1201: 12.819\n",
      "Loss after mini-batch  1301: 20.817\n",
      "Loss after mini-batch  1401: 12.616\n",
      "Loss after mini-batch  1501: 16.621\n",
      "Loss after mini-batch  1601: 11.922\n",
      "Loss after mini-batch  1701: 20.808\n",
      "Avg. loss for epoch 87 14.940\n",
      "Avg. loss for all epochs at epoch 87 18.065\n",
      "Starting epoch 88\n",
      "Loss after mini-batch     1: 0.034\n",
      "Loss after mini-batch   101: 13.740\n",
      "Loss after mini-batch   201: 15.483\n",
      "Loss after mini-batch   301: 10.333\n",
      "Loss after mini-batch   401: 19.827\n",
      "Loss after mini-batch   501: 22.777\n",
      "Loss after mini-batch   601: 17.564\n",
      "Loss after mini-batch   701: 11.869\n",
      "Loss after mini-batch   801: 15.870\n",
      "Loss after mini-batch   901: 21.147\n",
      "Loss after mini-batch  1001: 12.936\n",
      "Loss after mini-batch  1101: 17.824\n",
      "Loss after mini-batch  1201: 15.287\n",
      "Loss after mini-batch  1301: 15.504\n",
      "Loss after mini-batch  1401: 12.726\n",
      "Loss after mini-batch  1501: 12.509\n",
      "Loss after mini-batch  1601: 12.992\n",
      "Loss after mini-batch  1701: 21.106\n",
      "Avg. loss for epoch 88 14.974\n",
      "Avg. loss for all epochs at epoch 88 18.030\n",
      "Starting epoch 89\n",
      "Loss after mini-batch     1: 0.005\n",
      "Loss after mini-batch   101: 11.281\n",
      "Loss after mini-batch   201: 15.356\n",
      "Loss after mini-batch   301: 11.686\n",
      "Loss after mini-batch   401: 15.811\n",
      "Loss after mini-batch   501: 21.828\n",
      "Loss after mini-batch   601: 18.365\n",
      "Loss after mini-batch   701: 11.075\n",
      "Loss after mini-batch   801: 17.929\n",
      "Loss after mini-batch   901: 13.840\n",
      "Loss after mini-batch  1001: 10.167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch  1101: 15.247\n",
      "Loss after mini-batch  1201: 13.867\n",
      "Loss after mini-batch  1301: 16.232\n",
      "Loss after mini-batch  1401: 16.366\n",
      "Loss after mini-batch  1501: 20.950\n",
      "Loss after mini-batch  1601: 16.735\n",
      "Loss after mini-batch  1701: 15.504\n",
      "Avg. loss for epoch 89 14.569\n",
      "Avg. loss for all epochs at epoch 89 17.992\n",
      "Starting epoch 90\n",
      "Loss after mini-batch     1: 0.017\n",
      "Loss after mini-batch   101: 13.011\n",
      "Loss after mini-batch   201: 12.351\n",
      "Loss after mini-batch   301: 15.929\n",
      "Loss after mini-batch   401: 17.995\n",
      "Loss after mini-batch   501: 17.338\n",
      "Loss after mini-batch   601: 13.942\n",
      "Loss after mini-batch   701: 21.172\n",
      "Loss after mini-batch   801: 15.603\n",
      "Loss after mini-batch   901: 8.543\n",
      "Loss after mini-batch  1001: 11.584\n",
      "Loss after mini-batch  1101: 15.032\n",
      "Loss after mini-batch  1201: 14.065\n",
      "Loss after mini-batch  1301: 14.695\n",
      "Loss after mini-batch  1401: 18.688\n",
      "Loss after mini-batch  1501: 12.604\n",
      "Loss after mini-batch  1601: 18.130\n",
      "Loss after mini-batch  1701: 20.982\n",
      "Avg. loss for epoch 90 14.538\n",
      "Avg. loss for all epochs at epoch 90 17.954\n",
      "Starting epoch 91\n",
      "Loss after mini-batch     1: 0.031\n",
      "Loss after mini-batch   101: 10.558\n",
      "Loss after mini-batch   201: 23.162\n",
      "Loss after mini-batch   301: 14.984\n",
      "Loss after mini-batch   401: 15.009\n",
      "Loss after mini-batch   501: 14.458\n",
      "Loss after mini-batch   601: 9.815\n",
      "Loss after mini-batch   701: 16.189\n",
      "Loss after mini-batch   801: 12.387\n",
      "Loss after mini-batch   901: 11.617\n",
      "Loss after mini-batch  1001: 13.433\n",
      "Loss after mini-batch  1101: 15.435\n",
      "Loss after mini-batch  1201: 13.528\n",
      "Loss after mini-batch  1301: 15.669\n",
      "Loss after mini-batch  1401: 21.421\n",
      "Loss after mini-batch  1501: 14.008\n",
      "Loss after mini-batch  1601: 19.749\n",
      "Loss after mini-batch  1701: 14.987\n",
      "Avg. loss for epoch 91 14.247\n",
      "Avg. loss for all epochs at epoch 91 17.914\n",
      "Starting epoch 92\n",
      "Loss after mini-batch     1: 0.019\n",
      "Loss after mini-batch   101: 12.652\n",
      "Loss after mini-batch   201: 12.882\n",
      "Loss after mini-batch   301: 21.482\n",
      "Loss after mini-batch   401: 13.355\n",
      "Loss after mini-batch   501: 18.349\n",
      "Loss after mini-batch   601: 19.742\n",
      "Loss after mini-batch   701: 16.068\n",
      "Loss after mini-batch   801: 15.212\n",
      "Loss after mini-batch   901: 16.313\n",
      "Loss after mini-batch  1001: 19.744\n",
      "Loss after mini-batch  1101: 16.801\n",
      "Loss after mini-batch  1201: 9.285\n",
      "Loss after mini-batch  1301: 14.954\n",
      "Loss after mini-batch  1401: 16.408\n",
      "Loss after mini-batch  1501: 9.997\n",
      "Loss after mini-batch  1601: 10.969\n",
      "Loss after mini-batch  1701: 17.315\n",
      "Avg. loss for epoch 92 14.530\n",
      "Avg. loss for all epochs at epoch 92 17.877\n",
      "Starting epoch 93\n",
      "Loss after mini-batch     1: 0.056\n",
      "Loss after mini-batch   101: 18.418\n",
      "Loss after mini-batch   201: 18.889\n",
      "Loss after mini-batch   301: 8.712\n",
      "Loss after mini-batch   401: 16.397\n",
      "Loss after mini-batch   501: 17.130\n",
      "Loss after mini-batch   601: 17.853\n",
      "Loss after mini-batch   701: 11.549\n",
      "Loss after mini-batch   801: 12.142\n",
      "Loss after mini-batch   901: 10.214\n",
      "Loss after mini-batch  1001: 14.872\n",
      "Loss after mini-batch  1101: 12.320\n",
      "Loss after mini-batch  1201: 14.900\n",
      "Loss after mini-batch  1301: 14.795\n",
      "Loss after mini-batch  1401: 22.247\n",
      "Loss after mini-batch  1501: 16.029\n",
      "Loss after mini-batch  1601: 14.262\n",
      "Loss after mini-batch  1701: 16.220\n",
      "Avg. loss for epoch 93 14.278\n",
      "Avg. loss for all epochs at epoch 93 17.839\n",
      "Starting epoch 94\n",
      "Loss after mini-batch     1: 0.012\n",
      "Loss after mini-batch   101: 12.592\n",
      "Loss after mini-batch   201: 23.918\n",
      "Loss after mini-batch   301: 14.781\n",
      "Loss after mini-batch   401: 13.846\n",
      "Loss after mini-batch   501: 24.927\n",
      "Loss after mini-batch   601: 12.226\n",
      "Loss after mini-batch   701: 11.443\n",
      "Loss after mini-batch   801: 15.031\n",
      "Loss after mini-batch   901: 15.261\n",
      "Loss after mini-batch  1001: 15.974\n",
      "Loss after mini-batch  1101: 13.569\n",
      "Loss after mini-batch  1201: 14.063\n",
      "Loss after mini-batch  1301: 11.416\n",
      "Loss after mini-batch  1401: 14.034\n",
      "Loss after mini-batch  1501: 14.190\n",
      "Loss after mini-batch  1601: 14.354\n",
      "Loss after mini-batch  1701: 16.847\n",
      "Avg. loss for epoch 94 14.360\n",
      "Avg. loss for all epochs at epoch 94 17.802\n",
      "Starting epoch 95\n",
      "Loss after mini-batch     1: 0.010\n",
      "Loss after mini-batch   101: 12.789\n",
      "Loss after mini-batch   201: 15.458\n",
      "Loss after mini-batch   301: 18.430\n",
      "Loss after mini-batch   401: 12.886\n",
      "Loss after mini-batch   501: 19.808\n",
      "Loss after mini-batch   601: 12.450\n",
      "Loss after mini-batch   701: 10.940\n",
      "Loss after mini-batch   801: 10.978\n",
      "Loss after mini-batch   901: 17.721\n",
      "Loss after mini-batch  1001: 14.559\n",
      "Loss after mini-batch  1101: 12.438\n",
      "Loss after mini-batch  1201: 9.850\n",
      "Loss after mini-batch  1301: 19.725\n",
      "Loss after mini-batch  1401: 15.347\n",
      "Loss after mini-batch  1501: 16.624\n",
      "Loss after mini-batch  1601: 16.985\n",
      "Loss after mini-batch  1701: 14.540\n",
      "Avg. loss for epoch 95 13.974\n",
      "Avg. loss for all epochs at epoch 95 17.763\n",
      "Starting epoch 96\n",
      "Loss after mini-batch     1: 0.022\n",
      "Loss after mini-batch   101: 13.765\n",
      "Loss after mini-batch   201: 13.440\n",
      "Loss after mini-batch   301: 11.332\n",
      "Loss after mini-batch   401: 19.146\n",
      "Loss after mini-batch   501: 21.557\n",
      "Loss after mini-batch   601: 13.248\n",
      "Loss after mini-batch   701: 15.975\n",
      "Loss after mini-batch   801: 20.128\n",
      "Loss after mini-batch   901: 14.307\n",
      "Loss after mini-batch  1001: 8.493\n",
      "Loss after mini-batch  1101: 12.328\n",
      "Loss after mini-batch  1201: 9.525\n",
      "Loss after mini-batch  1301: 12.024\n",
      "Loss after mini-batch  1401: 16.936\n",
      "Loss after mini-batch  1501: 16.580\n",
      "Loss after mini-batch  1601: 22.321\n",
      "Loss after mini-batch  1701: 14.354\n",
      "Avg. loss for epoch 96 14.193\n",
      "Avg. loss for all epochs at epoch 96 17.726\n",
      "Starting epoch 97\n",
      "Loss after mini-batch     1: 0.081\n",
      "Loss after mini-batch   101: 17.331\n",
      "Loss after mini-batch   201: 11.509\n",
      "Loss after mini-batch   301: 13.569\n",
      "Loss after mini-batch   401: 12.399\n",
      "Loss after mini-batch   501: 9.917\n",
      "Loss after mini-batch   601: 15.348\n",
      "Loss after mini-batch   701: 19.718\n",
      "Loss after mini-batch   801: 14.614\n",
      "Loss after mini-batch   901: 22.735\n",
      "Loss after mini-batch  1001: 17.890\n",
      "Loss after mini-batch  1101: 17.861\n",
      "Loss after mini-batch  1201: 17.993\n",
      "Loss after mini-batch  1301: 13.093\n",
      "Loss after mini-batch  1401: 11.858\n",
      "Loss after mini-batch  1501: 17.276\n",
      "Loss after mini-batch  1601: 12.381\n",
      "Loss after mini-batch  1701: 8.914\n",
      "Avg. loss for epoch 97 14.138\n",
      "Avg. loss for all epochs at epoch 97 17.689\n",
      "Starting epoch 98\n",
      "Loss after mini-batch     1: 0.237\n",
      "Loss after mini-batch   101: 13.912\n",
      "Loss after mini-batch   201: 8.803\n",
      "Loss after mini-batch   301: 15.731\n",
      "Loss after mini-batch   401: 16.353\n",
      "Loss after mini-batch   501: 14.005\n",
      "Loss after mini-batch   601: 11.874\n",
      "Loss after mini-batch   701: 20.799\n",
      "Loss after mini-batch   801: 9.985\n",
      "Loss after mini-batch   901: 17.790\n",
      "Loss after mini-batch  1001: 14.526\n",
      "Loss after mini-batch  1101: 21.078\n",
      "Loss after mini-batch  1201: 13.637\n",
      "Loss after mini-batch  1301: 18.194\n",
      "Loss after mini-batch  1401: 16.875\n",
      "Loss after mini-batch  1501: 17.683\n",
      "Loss after mini-batch  1601: 8.516\n",
      "Loss after mini-batch  1701: 10.847\n",
      "Avg. loss for epoch 98 13.936\n",
      "Avg. loss for all epochs at epoch 98 17.651\n",
      "Starting epoch 99\n",
      "Loss after mini-batch     1: 0.009\n",
      "Loss after mini-batch   101: 12.425\n",
      "Loss after mini-batch   201: 12.191\n",
      "Loss after mini-batch   301: 13.137\n",
      "Loss after mini-batch   401: 9.885\n",
      "Loss after mini-batch   501: 15.140\n",
      "Loss after mini-batch   601: 9.954\n",
      "Loss after mini-batch   701: 19.414\n",
      "Loss after mini-batch   801: 15.637\n",
      "Loss after mini-batch   901: 12.380\n",
      "Loss after mini-batch  1001: 16.238\n",
      "Loss after mini-batch  1101: 22.132\n",
      "Loss after mini-batch  1201: 15.046\n",
      "Loss after mini-batch  1301: 13.209\n",
      "Loss after mini-batch  1401: 13.917\n",
      "Loss after mini-batch  1501: 18.926\n",
      "Loss after mini-batch  1601: 17.959\n",
      "Loss after mini-batch  1701: 13.728\n",
      "Avg. loss for epoch 99 13.963\n",
      "Avg. loss for all epochs at epoch 99 17.614\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# Initialize the MLP\n",
    "mlp = SimpleNet()\n",
    "\n",
    "lr = 1e-4\n",
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Hold the best model\n",
    "best_mse = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=lr)\n",
    "  \n",
    "# Run the training loop\n",
    "for epoch in range(n_epochs): \n",
    "    mlp.train()\n",
    "    print('Starting epoch %d' %epoch)\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "      \n",
    "        \n",
    "      # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        # inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "      \n",
    "      # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "      \n",
    "      # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "      \n",
    "      # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "      \n",
    "      # Perform backward pass\n",
    "        loss.backward()\n",
    "      \n",
    "      # Perform optimization\n",
    "        optimizer.step()\n",
    "      \n",
    "      # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            current_loss = current_loss/(100*batch_size)\n",
    "            epoch_losses.append(current_loss)\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                (i + 1, current_loss))\n",
    "            current_loss = 0.0\n",
    "                  \n",
    "    e_l_m =  np.mean(epoch_losses)\n",
    "    print('Avg. loss for epoch %d %.3f' %(epoch, e_l_m))\n",
    "    history.append(e_l_m)\n",
    "    print('Avg. loss for all epochs at epoch %d %.3f' %(epoch, np.mean(history)))\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2407d4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oppe4rl",
   "language": "python",
   "name": "oppe4rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
